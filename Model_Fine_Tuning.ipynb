{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning Llama 3.2 1B Instruct 4-bit for Conflict Event Extraction \n",
    "\n",
    "The final stage shows how to fine-tune a **4-bit quantised Llama 3.2 1B Instruct** model using **LoRA**. The aim is to train it to extract structured data from ACLED conflict event texts. Each unstructured narrative is turned into a JSON object with **14 fields**, including the event date, location, actors, casualties, and other key details about the conflict.\n",
    "\n",
    "### Workflow Overview  \n",
    "1. Prepare and validate the ACLED dataset  \n",
    "2. Configure LoRA adapters for the model  \n",
    "3. Fine-tune the model with optimised training settings  \n",
    "4. Evaluate performance using structured accuracy metrics and standard machine learning metrics (F1, Recall, etc.).  \n",
    "\n",
    "This pipeline supports applications in automated conflict monitoring, research, and near real-time situational analysis.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Dependencies\n",
    "\n",
    "These imports support all stages of the pipeline, including data preparation, quality scoring, model fine-tuning, and structured evaluation.\n",
    "\n",
    "### Core Python Utilities  \n",
    "\n",
    "- `os`, `Path`: File and directory operations  \n",
    "- `re`, `json`: Pattern matching and JSON parsing  \n",
    "- `random`, `subprocess`: Random sampling and shell commands  \n",
    "- `datetime`, `timedelta`: Time-based filtering and logging  \n",
    "- `dataclass`, `typing`: Structured configuration and type safety  \n",
    "- `defaultdict` (from `collections`): Easy counting and nested stats  \n",
    "- `contextlib`: Graceful error handling with `suppress`\n",
    "\n",
    "### Data Handling and Evaluation\n",
    "\n",
    "- `pandas`, `numpy`: Data manipulation and vectorised operations  \n",
    "- `precision_score`, `recall_score`, `f1_score` (from `sklearn.metrics`): Standard evaluation metrics\n",
    "\n",
    "### MLX Framework (Apple Silicon)  \n",
    "\n",
    "- `mlx.core`: MLX’s core tensor computation engine  \n",
    "- `mlx_lm`: Tools for loading, running, and generating text with MLX-compatible language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import subprocess\n",
    "import inspect\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import contextlib \n",
    "from contextlib import contextmanager, suppress\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "import mlx.core as mx\n",
    "from mlx_lm import load, generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Configuration and Environment Setup\n",
    "\n",
    "This cell defines the core configuration parameters for fine-tuning the Llama 3.2 1B model on the ACLED dataset using Low-Rank Adaptation (LoRA), and prepares the training environment accordingly.\n",
    "\n",
    "#### Configuration Parameters\n",
    "\n",
    "A `@dataclass` is used to define the training setup:\n",
    "\n",
    "- **Model path**: `\"llama_models/Llama-3.2-1B-Instruct-4bit\"` — the base instruction-tuned model checkpoint.\n",
    "- **Dataset path**: `\"ACLED_data_export/ACLED_finetuning_dataset.jsonl\"` — the preprocessed dataset formatted for instruction–response fine-tuning.\n",
    "- **Output directory**: `\"ACLED_llama_fine_tuned\"` — location for all outputs, including adapter checkpoints and logs.\n",
    "\n",
    "Key hyperparameters:\n",
    "\n",
    "- `NUM_EPOCHS = 3`: Number of training epochs.\n",
    "- `LEARNING_RATE = 2e-4`: Initial learning rate.\n",
    "- `BATCH_SIZE = 4`: Training batch size.\n",
    "- `MAX_SEQ_LEN = 1024`: Maximum sequence length for each input sample.\n",
    "- `ITERATIONS = 6000`: Total number of training iterations.\n",
    "- `EVAL_FREQUENCY = 500`: Evaluate and checkpoint every 500 steps.\n",
    "- `SEED = 42`: Random seed for reproducibility.\n",
    "\n",
    "#### Environment Setup\n",
    "\n",
    "- **Random Seed Initialisation**:\n",
    "  Ensures reproducible results across Python (`random`), NumPy (`np`), and MLX (`mx`).\n",
    "\n",
    "- **Output Directories**:\n",
    "  Automatically creates the output structure:\n",
    "  - `data/` for dataset fragments (if needed),\n",
    "  - `adapters_optimised/` for saving LoRA checkpoints.\n",
    "\n",
    "- **Tokenizer Parallelism**:\n",
    "  Disables tokeniser multiprocessing via `TOKENIZERS_PARALLELISM=false` to avoid potential conflicts during training.\n",
    "\n",
    "#### MLX Hardware Check\n",
    "\n",
    "The cell also prints:\n",
    "\n",
    "- MLX version.\n",
    "- Default device and its type (CPU or GPU).\n",
    "- Whether Metal Performance Shaders (MPS) are available.\n",
    "- Active memory usage in gigabytes.\n",
    "\n",
    "If GPU acceleration is available via Metal, it is confirmed; otherwise, a warning is issued to highlight that training may be significantly slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACLED Conflict Event Extraction — Llama 3.2 1B LoRA Fine-tuning\n",
      "========================================================================\n",
      "\n",
      "MLX version: 0.28.0\n",
      "Default device: Device(gpu, 0)\n",
      "Device type: DeviceType.gpu\n",
      "Is Metal available: True\n",
      "Active memory: 0.00 GB\n",
      "MPS (Metal Performance Shaders) GPU acceleration is enabled.\n",
      "\n",
      "Training Configuration:\n",
      "        MODEL_PATH: llama_models/Llama-3.2-1B-Instruct-4bit\n",
      "         DATA_PATH: ACLED_data_export/ACLED_finetuning_dataset.jsonl\n",
      "        OUTPUT_DIR: ACLED_llama_fine_tuned\n",
      "     LEARNING_RATE: 0.0002\n",
      "        BATCH_SIZE: 4\n",
      "       MAX_SEQ_LEN: 1024\n",
      "        ITERATIONS: 7000\n",
      "              SEED: 42\n",
      "    EVAL_FREQUENCY: 500\n"
     ]
    }
   ],
   "source": [
    "print(\"ACLED Conflict Event Extraction — Llama 3.2 1B LoRA Fine-tuning\")\n",
    "print(\"=\" * 72)\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    MODEL_PATH: str = \"llama_models/Llama-3.2-1B-Instruct-4bit\"\n",
    "    DATA_PATH: str  = \"ACLED_data_export/ACLED_finetuning_dataset.jsonl\"\n",
    "    OUTPUT_DIR: str = \"ACLED_llama_fine_tuned\"\n",
    "\n",
    "    LEARNING_RATE: float = 2e-4  \n",
    "    BATCH_SIZE: int      = 4\n",
    "    MAX_SEQ_LEN: int     = 1024  \n",
    "    ITERATIONS: int      = 7000\n",
    "    SEED: int            = 42\n",
    "    EVAL_FREQUENCY: int  = 500\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Random seeds for reproducibility\n",
    "random.seed(config.SEED)\n",
    "np.random.seed(config.SEED)\n",
    "mx.random.seed(config.SEED)\n",
    "\n",
    "out_dir      = Path(config.OUTPUT_DIR)\n",
    "data_dir     = out_dir / \"data\"\n",
    "adapters_dir = out_dir / \"adapters_optimised\"\n",
    "for p in (out_dir, data_dir, adapters_dir):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Prevent tokenizer multiprocessing issues\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# MLX version and device check\n",
    "print(f\"\\nMLX version: {mx.__version__}\")\n",
    "with suppress(Exception):\n",
    "    device = mx.default_device()\n",
    "    print(f\"Default device: {device}\")\n",
    "    print(f\"Device type: {device.type}\")\n",
    "    print(f\"Is Metal available: {mx.metal.is_available()}\")\n",
    "    print(f\"Active memory: {mx.get_active_memory() / (1024**3):.2f} GB\")\n",
    "\n",
    "    if getattr(device.type, \"name\", str(device.type)).lower() == \"gpu\":\n",
    "        print(\"MPS (Metal Performance Shaders) GPU acceleration is enabled.\")\n",
    "    else:\n",
    "        print(\"WARNING: Not using GPU acceleration — training may be slow.\")\n",
    "\n",
    "print(\"\\nTraining Configuration:\")\n",
    "for key, value in config.__dict__.items():\n",
    "    print(f\"{key:>18}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation\n",
    "\n",
    "#### The cell below performs the following:\n",
    "\n",
    "- Reads and validates the fine-tuning dataset from the specified JSONL file.\n",
    "- Filters out malformed lines and entries missing the `instruction` or `output` fields.\n",
    "- Converts each entry into Llama-compatible format using `[INST] ... [/INST]` syntax.\n",
    "- Shuffles and splits the dataset into training (80%) and validation (20%) sets using a fixed seed.\n",
    "- Saves the resulting files as `train.jsonl` and `valid.jsonl` for use during fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset for fine-tuning\n",
      "Total usable: 25,000\n",
      "Malformed/missing keys: 0\n",
      "Train: 20,000 -> ACLED_llama_fine_tuned/data/train.jsonl\n",
      "Valid: 5,000 -> ACLED_llama_fine_tuned/data/valid.jsonl\n",
      "Preview: [INST] Extract relevant information from this conflict event report:  On 23 February 2025, Russian forces shelled Ukrainian positions near Kupiansk, Kharkiv. According to Russian sources, up to 200 Uk\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing dataset for fine-tuning\")\n",
    "\n",
    "src_path = Path(config.DATA_PATH)\n",
    "assert src_path.is_file(), f\"Data file not found: {src_path}\"\n",
    "\n",
    "# Format instruction-output pairs into Llama-compatible input\n",
    "def to_llama_text(instr: str, out: str) -> str:\n",
    "    instr = instr.strip()\n",
    "    out = out.strip()\n",
    "    return f\"[INST] {instr} [/INST] {out}\"\n",
    "\n",
    "formatted = []\n",
    "bad = 0\n",
    "\n",
    "# Read dataset line by line and parse valid JSON entries\n",
    "with src_path.open(\"r\") as f:\n",
    "    for i, line in enumerate(f, 1):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            obj = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            bad += 1\n",
    "            continue\n",
    "        if \"instruction\" not in obj or \"output\" not in obj:\n",
    "            bad += 1\n",
    "            continue\n",
    "        text = to_llama_text(obj[\"instruction\"], obj[\"output\"])\n",
    "        formatted.append({\"text\": text})\n",
    "\n",
    "# Ensure dataset is not empty after filtering\n",
    "assert len(formatted) > 0, \"Dataset is empty after strict parse\"\n",
    "\n",
    "rng = random.Random(config.SEED)\n",
    "rng.shuffle(formatted)\n",
    "split_idx = int(0.8 * len(formatted))\n",
    "train_data = formatted[:split_idx]\n",
    "valid_data = formatted[split_idx:]\n",
    "\n",
    "train_path = data_dir / \"train.jsonl\"\n",
    "valid_path = data_dir / \"valid.jsonl\"\n",
    "with train_path.open(\"w\") as f:\n",
    "    for s in train_data:\n",
    "        f.write(json.dumps(s, ensure_ascii=False) + \"\\n\")\n",
    "with valid_path.open(\"w\") as f:\n",
    "    for s in valid_data:\n",
    "        f.write(json.dumps(s, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Total usable: {len(formatted):,}\")\n",
    "print(f\"Malformed/missing keys: {bad:,}\")\n",
    "print(f\"Train: {len(train_data):,} -> {train_path}\")\n",
    "print(f\"Valid: {len(valid_data):,} -> {valid_path}\")\n",
    "if train_data:\n",
    "    print(\"Preview:\", train_data[0][\"text\"][:200].replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA Configuration\n",
    "\n",
    "A YAML file defines the LoRA hyperparameters used for training. It is saved to the output directory and passed to the MLX-LM training command to ensure reproducibility and explicit documentation.\n",
    "\n",
    "Defined values:\n",
    "\n",
    "- `lora_r`: 16 (rank of the low-rank adaptation)\n",
    "- `lora_alpha`: 32 (scaling factor)\n",
    "- `lora_dropout`: 0.1 (dropout rate applied to adapter layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing LoRA configuration\n",
      "Wrote ACLED_llama_fine_tuned/lora_optimised.yaml\n"
     ]
    }
   ],
   "source": [
    "print(\"Writing LoRA configuration\")\n",
    "\n",
    "lora_yaml_path = out_dir / \"lora_optimised.yaml\"\n",
    "lora_yaml_path.write_text(\n",
    "    \"lora_r: 16\\n\"\n",
    "    \"lora_alpha: 32\\n\"\n",
    "    \"lora_dropout: 0.1\\n\"\n",
    ")\n",
    "print(f\"Wrote {lora_yaml_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning Execution with Checkpoint Support and ETA Tracking\n",
    "\n",
    "The main training loop runs the `mlx_lm` CLI with LoRA on the 4bit quantised Llama 3.2 1B model. It supports resumable training, live loss reporting, and time estimation, with all parameters controlled via the central `Config` class.\n",
    "\n",
    "#### Adaptive Resume Logic and Checkpoint Management\n",
    "\n",
    "Before training begins, the script checks for previously saved adapter checkpoints:\n",
    "\n",
    "- Uses file pattern matching to identify the last completed training step.\n",
    "- If a checkpoint is found, it is promoted to `adapters.safetensors` for resumption.\n",
    "- If not, training begins from iteration 0.\n",
    "\n",
    "This ensures safe recovery from interruptions and avoids redundant computation.\n",
    "\n",
    "#### Command-Line Interface and Training Parameters\n",
    "\n",
    "The training command is constructed as a subprocess call with the following options:\n",
    "\n",
    "- `--model`: Path to the 4-bit quantised Llama 3.2 model\n",
    "- `--data`: Directory containing formatted `train.jsonl` and `valid.jsonl` files\n",
    "- `--batch-size`, `--iters`, `--learning-rate`, `--max-seq-length`: Controlled by config\n",
    "- `--adapter-path`: Directory to save LoRA adapter checkpoints\n",
    "- `--grad-checkpoint`: Enables memory-efficient training\n",
    "- `-c`: Points to the YAML file specifying LoRA hyperparameters\n",
    "\n",
    "These arguments ensure reproducibility and make the setup easily extendable for further experimentation.\n",
    "\n",
    "#### Live Log Parsing and ETA Monitoring\n",
    "\n",
    "The subprocess output is parsed in real time using regular expressions to extract:\n",
    "\n",
    "- **Step Number** (`Iter N`)\n",
    "- **Training Loss** (`Train loss …`)\n",
    "- **Validation Loss** (`Val loss …`)\n",
    "\n",
    "Once iteration progress begins, a smoothed estimate of time per step is calculated and used to print:\n",
    "\n",
    "- Percentage of training completed\n",
    "- Current local and global step counts\n",
    "- Current learning rate\n",
    "- Most recent training and validation losses\n",
    "- Estimated time remaining (ETA)\n",
    "\n",
    "The display updates live, providing detailed feedback without requiring external logging tools.\n",
    "\n",
    "#### Final Output and Summary\n",
    "\n",
    "After completion or interruption, the script prints:\n",
    "\n",
    "- The final checkpointed step\n",
    "- Total training time in hours, minutes, and seconds\n",
    "- Final observed training and validation losses (if available)\n",
    "\n",
    "This loop is designed for local fine-tuning on macOS with Apple Silicon and Metal, providing a lightweight, resumable, and transparent training experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training (constant LR)\n",
      "  target iters = 7000 | lr = 0.0002 | batch = 4\n",
      "No checkpoints found. Starting at 0.\n",
      "Command: python -m mlx_lm lora --model llama_models/Llama-3.2-1B-Instruct-4bit --train --data ACLED_llama_fine_tuned/data --batch-size 4 --iters 7000 --learning-rate 0.0002 --steps-per-report 100 --steps-per-eval 500 --save-every 500 --adapter-path ACLED_llama_fine_tuned/adapters_optimised --max-seq-length 1024 -c ACLED_llama_fine_tuned/lora_optimised.yaml --grad-checkpoint --seed 42\n",
      "Progress: 0.0% | Local: 0/7000 | Global: 0->7000 | ETA: warming up…\n",
      "Loading configuration file ACLED_llama_fine_tuned/lora_optimised.yaml\n",
      "Loading pretrained model\n",
      "Loading datasets\n",
      "Training\n",
      "Trainable parameters: 0.069% (0.852M/1235.814M)\n",
      "Starting training..., iters: 7000\n",
      "Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "Calculating loss...:   4%|▍         | 1/25 [00:00<00:13,  1.81it/s]\n",
      "Calculating loss...:   8%|▊         | 2/25 [00:01<00:15,  1.48it/s]\n",
      "Calculating loss...:  12%|█▏        | 3/25 [00:01<00:12,  1.72it/s]\n",
      "Calculating loss...:  16%|█▌        | 4/25 [00:02<00:13,  1.53it/s]\n",
      "Calculating loss...:  20%|██        | 5/25 [00:03<00:12,  1.65it/s]\n",
      "Calculating loss...:  24%|██▍       | 6/25 [00:03<00:11,  1.72it/s]\n",
      "Calculating loss...:  28%|██▊       | 7/25 [00:04<00:10,  1.72it/s]\n",
      "Calculating loss...:  32%|███▏      | 8/25 [00:04<00:10,  1.57it/s]\n",
      "Calculating loss...:  36%|███▌      | 9/25 [00:05<00:11,  1.40it/s]\n",
      "Calculating loss...:  40%|████      | 10/25 [00:06<00:09,  1.52it/s]\n",
      "Calculating loss...:  44%|████▍     | 11/25 [00:06<00:08,  1.58it/s]\n",
      "Calculating loss...:  48%|████▊     | 12/25 [00:07<00:08,  1.62it/s]\n",
      "Calculating loss...:  52%|█████▏    | 13/25 [00:07<00:06,  1.75it/s]\n",
      "Calculating loss...:  56%|█████▌    | 14/25 [00:08<00:06,  1.74it/s]\n",
      "Calculating loss...:  60%|██████    | 15/25 [00:09<00:05,  1.84it/s]\n",
      "Calculating loss...:  64%|██████▍   | 16/25 [00:09<00:04,  1.86it/s]\n",
      "Calculating loss...:  68%|██████▊   | 17/25 [00:10<00:04,  1.65it/s]\n",
      "Calculating loss...:  72%|███████▏  | 18/25 [00:10<00:04,  1.72it/s]\n",
      "Calculating loss...:  76%|███████▌  | 19/25 [00:11<00:03,  1.72it/s]\n",
      "Calculating loss...:  80%|████████  | 20/25 [00:11<00:02,  1.77it/s]\n",
      "Calculating loss...:  84%|████████▍ | 21/25 [00:12<00:02,  1.60it/s]\n",
      "Calculating loss...:  88%|████████▊ | 22/25 [00:13<00:01,  1.63it/s]\n",
      "Calculating loss...:  92%|█████████▏| 23/25 [00:13<00:01,  1.65it/s]\n",
      "Calculating loss...:  96%|█████████▌| 24/25 [00:14<00:00,  1.49it/s]\n",
      "Calculating loss...: 100%|██████████| 25/25 [00:15<00:00,  1.43it/s]\n",
      "Calculating loss...: 100%|██████████| 25/25 [00:15<00:00,  1.62it/s]\n",
      "Iter 1: Val loss 2.810, Val took 15.491s\n",
      "Run (0.0% total) | Local 1/7000 | Global 1/7000 | LR 2.0e-04 | Val 2.810 | ETA estimating…Iter 100: Train loss 0.810, Learning Rate 2.000e-04, It/sec 0.565, Tokens/sec 601.881, Trained Tokens 106603, Peak mem 3.712 GB\n",
      "Run (1.4% total) | Local 100/7000 | Global 100/7000 | LR 2.0e-04 | Train 0.810 | Val 2.810 | ETA 03:25:55Iter 200: Train loss 0.538, Learning Rate 2.000e-04, It/sec 0.539, Tokens/sec 578.568, Trained Tokens 213852, Peak mem 3.719 GB\n",
      "Run (2.9% total) | Local 200/7000 | Global 200/7000 | LR 2.0e-04 | Train 0.538 | Val 2.810 | ETA 03:25:07Iter 300: Train loss 0.474, Learning Rate 2.000e-04, It/sec 0.523, Tokens/sec 563.920, Trained Tokens 321779, Peak mem 3.719 GB\n",
      "Run (4.3% total) | Local 300/7000 | Global 300/7000 | LR 2.0e-04 | Train 0.474 | Val 2.810 | ETA 03:25:38Iter 400: Train loss 0.454, Learning Rate 2.000e-04, It/sec 0.497, Tokens/sec 543.794, Trained Tokens 431261, Peak mem 3.719 GB\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.454 | Val 2.810 | ETA 03:28:16Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.454 | Val 2.810 | ETA 03:28:16Calculating loss...:   4%|▍         | 1/25 [00:00<00:19,  1.24it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.454 | Val 2.810 | ETA 03:28:16Calculating loss...:   8%|▊         | 2/25 [00:01<00:16,  1.43it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.454 | Val 2.810 | ETA 03:28:16Calculating loss...:  12%|█▏        | 3/25 [00:02<00:17,  1.27it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.454 | Val 2.810 | ETA 03:28:16Calculating loss...:  16%|█▌        | 4/25 [00:02<00:14,  1.48it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.454 | Val 2.810 | ETA 03:28:16Calculating loss...:  20%|██        | 5/25 [00:03<00:14,  1.39it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.454 | Val 2.810 | ETA 03:28:16Calculating loss...:  24%|██▍       | 6/25 [00:04<00:12,  1.51it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.454 | Val 2.810 | ETA 03:28:16Calculating loss...:  28%|██▊       | 7/25 [00:05<00:13,  1.34it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.454 | Val 2.810 | ETA 03:28:16Calculating loss...:  32%|███▏      | 8/25 [00:05<00:11,  1.46it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.454 | Val 2.810 | ETA 03:28:16Calculating loss...:  36%|███▌      | 9/25 [00:06<00:10,  1.51it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.454 | Val 2.810 | ETA 03:28:16Calculating loss...:  40%|████      | 10/25 [00:07<00:10,  1.38it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.454 | Val 2.810 | ETA 03:28:16Calculating loss...:  44%|████▍     | 11/25 [00:07<00:09,  1.47it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.454 | Val 2.810 | ETA 03:28:16Calculating loss...:  48%|████▊     | 12/25 [00:08<00:09,  1.36it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.454 | Val 2.810 | ETA 03:28:16Calculating loss...:  52%|█████▏    | 13/25 [00:09<00:08,  1.47it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.454 | Val 2.810 | ETA 03:28:16Calculating loss...:  56%|█████▌    | 14/25 [00:09<00:07,  1.40it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.454 | Val 2.810 | ETA 03:28:16Calculating loss...:  60%|██████    | 15/25 [00:10<00:06,  1.50it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.454 | Val 2.810 | ETA 03:28:16Calculating loss...:  64%|██████▍   | 16/25 [00:11<00:05,  1.54it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.454 | Val 2.810 | ETA 03:28:16Calculating loss...:  68%|██████▊   | 17/25 [00:11<00:04,  1.61it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.454 | Val 2.810 | ETA 03:28:16Calculating loss...:  72%|███████▏  | 18/25 [00:12<00:04,  1.66it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.454 | Val 2.810 | ETA 03:28:16Calculating loss...:  76%|███████▌  | 19/25 [00:13<00:04,  1.39it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.454 | Val 2.810 | ETA 03:28:16Calculating loss...:  80%|████████  | 20/25 [00:13<00:03,  1.48it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.454 | Val 2.810 | ETA 03:28:16Calculating loss...:  84%|████████▍ | 21/25 [00:14<00:02,  1.41it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.454 | Val 2.810 | ETA 03:28:16Calculating loss...:  88%|████████▊ | 22/25 [00:15<00:02,  1.47it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.454 | Val 2.810 | ETA 03:28:16Calculating loss...:  92%|█████████▏| 23/25 [00:15<00:01,  1.51it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.454 | Val 2.810 | ETA 03:28:16Calculating loss...:  96%|█████████▌| 24/25 [00:16<00:00,  1.39it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.454 | Val 2.810 | ETA 03:28:16Calculating loss...: 100%|██████████| 25/25 [00:17<00:00,  1.45it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.454 | Val 2.810 | ETA 03:28:16Calculating loss...: 100%|██████████| 25/25 [00:17<00:00,  1.45it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.454 | Val 2.810 | ETA 03:28:16Iter 500: Val loss 0.382, Val took 17.262s\n",
      "Run (7.1% total) | Local 500/7000 | Global 500/7000 | LR 2.0e-04 | Train 0.454 | Val 0.382 | ETA 03:32:01Iter 500: Train loss 0.431, Learning Rate 2.000e-04, It/sec 0.513, Tokens/sec 548.329, Trained Tokens 538140, Peak mem 3.719 GB\n",
      "Run (7.1% total) | Local 500/7000 | Global 500/7000 | LR 2.0e-04 | Train 0.431 | Val 0.382 | ETA 03:32:01Iter 500: Saved adapter weights to ACLED_llama_fine_tuned/adapters_optimised/adapters.safetensors and ACLED_llama_fine_tuned/adapters_optimised/0000500_adapters.safetensors.\n",
      "Run (7.1% total) | Local 500/7000 | Global 500/7000 | LR 2.0e-04 | Train 0.431 | Val 0.382 | ETA 03:32:01Iter 600: Train loss 0.421, Learning Rate 2.000e-04, It/sec 0.532, Tokens/sec 559.984, Trained Tokens 643470, Peak mem 3.719 GB\n",
      "Run (8.6% total) | Local 600/7000 | Global 600/7000 | LR 2.0e-04 | Train 0.421 | Val 0.382 | ETA 03:26:21Iter 700: Train loss 0.413, Learning Rate 2.000e-04, It/sec 0.512, Tokens/sec 546.073, Trained Tokens 750215, Peak mem 3.719 GB\n",
      "Run (10.0% total) | Local 700/7000 | Global 700/7000 | LR 2.0e-04 | Train 0.413 | Val 0.382 | ETA 03:23:48Iter 800: Train loss 0.400, Learning Rate 2.000e-04, It/sec 0.509, Tokens/sec 550.985, Trained Tokens 858505, Peak mem 3.719 GB\n",
      "Run (11.4% total) | Local 800/7000 | Global 800/7000 | LR 2.0e-04 | Train 0.400 | Val 0.382 | ETA 03:21:22Iter 900: Train loss 0.369, Learning Rate 2.000e-04, It/sec 0.516, Tokens/sec 551.845, Trained Tokens 965441, Peak mem 3.723 GB\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.369 | Val 0.382 | ETA 03:17:50Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.369 | Val 0.382 | ETA 03:17:50Calculating loss...:   4%|▍         | 1/25 [00:05<02:22,  5.93s/it]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.369 | Val 0.382 | ETA 03:17:50Calculating loss...:   8%|▊         | 2/25 [00:06<01:03,  2.76s/it]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.369 | Val 0.382 | ETA 03:17:50Calculating loss...:  12%|█▏        | 3/25 [00:07<00:39,  1.78s/it]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.369 | Val 0.382 | ETA 03:17:50Calculating loss...:  16%|█▌        | 4/25 [00:07<00:27,  1.32s/it]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.369 | Val 0.382 | ETA 03:17:50Calculating loss...:  20%|██        | 5/25 [00:08<00:21,  1.06s/it]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.369 | Val 0.382 | ETA 03:17:50Calculating loss...:  24%|██▍       | 6/25 [00:08<00:16,  1.12it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.369 | Val 0.382 | ETA 03:17:50Calculating loss...:  28%|██▊       | 7/25 [00:09<00:14,  1.24it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.369 | Val 0.382 | ETA 03:17:50Calculating loss...:  32%|███▏      | 8/25 [00:10<00:13,  1.24it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.369 | Val 0.382 | ETA 03:17:50Calculating loss...:  36%|███▌      | 9/25 [00:11<00:13,  1.17it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.369 | Val 0.382 | ETA 03:17:50Calculating loss...:  40%|████      | 10/25 [00:11<00:11,  1.34it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.369 | Val 0.382 | ETA 03:17:50Calculating loss...:  44%|████▍     | 11/25 [00:12<00:10,  1.36it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.369 | Val 0.382 | ETA 03:17:50Calculating loss...:  48%|████▊     | 12/25 [00:13<00:09,  1.41it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.369 | Val 0.382 | ETA 03:17:50Calculating loss...:  52%|█████▏    | 13/25 [00:13<00:08,  1.37it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.369 | Val 0.382 | ETA 03:17:50Calculating loss...:  56%|█████▌    | 14/25 [00:14<00:08,  1.31it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.369 | Val 0.382 | ETA 03:17:50Calculating loss...:  60%|██████    | 15/25 [00:15<00:08,  1.18it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.369 | Val 0.382 | ETA 03:17:50Calculating loss...:  64%|██████▍   | 16/25 [00:22<00:22,  2.47s/it]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.369 | Val 0.382 | ETA 03:17:50Calculating loss...:  68%|██████▊   | 17/25 [00:26<00:23,  2.99s/it]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.369 | Val 0.382 | ETA 03:17:50Calculating loss...:  72%|███████▏  | 18/25 [00:29<00:21,  3.14s/it]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.369 | Val 0.382 | ETA 03:17:50Calculating loss...:  76%|███████▌  | 19/25 [00:32<00:18,  3.06s/it]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.369 | Val 0.382 | ETA 03:17:50Calculating loss...:  80%|████████  | 20/25 [00:35<00:15,  3.01s/it]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.369 | Val 0.382 | ETA 03:17:50Calculating loss...:  84%|████████▍ | 21/25 [00:37<00:11,  2.83s/it]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.369 | Val 0.382 | ETA 03:17:50Calculating loss...:  88%|████████▊ | 22/25 [00:40<00:08,  2.74s/it]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.369 | Val 0.382 | ETA 03:17:50Calculating loss...:  92%|█████████▏| 23/25 [01:07<00:19,  9.98s/it]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.369 | Val 0.382 | ETA 03:17:50Calculating loss...:  96%|█████████▌| 24/25 [01:10<00:07,  7.82s/it]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.369 | Val 0.382 | ETA 03:17:50Calculating loss...: 100%|██████████| 25/25 [01:10<00:00,  5.69s/it]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.369 | Val 0.382 | ETA 03:17:50Calculating loss...: 100%|██████████| 25/25 [01:10<00:00,  2.83s/it]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.369 | Val 0.382 | ETA 03:17:50Iter 1000: Val loss 0.399, Val took 46.575s\n",
      "Run (14.3% total) | Local 1000/7000 | Global 1000/7000 | LR 2.0e-04 | Train 0.369 | Val 0.399 | ETA 03:39:02Iter 1000: Train loss 0.379, Learning Rate 2.000e-04, It/sec 0.487, Tokens/sec 530.341, Trained Tokens 1074299, Peak mem 3.723 GB\n",
      "Run (14.3% total) | Local 1000/7000 | Global 1000/7000 | LR 2.0e-04 | Train 0.379 | Val 0.399 | ETA 03:39:02Iter 1000: Saved adapter weights to ACLED_llama_fine_tuned/adapters_optimised/adapters.safetensors and ACLED_llama_fine_tuned/adapters_optimised/0001000_adapters.safetensors.\n",
      "Run (14.3% total) | Local 1000/7000 | Global 1000/7000 | LR 2.0e-04 | Train 0.379 | Val 0.399 | ETA 03:39:02Iter 1100: Train loss 0.379, Learning Rate 2.000e-04, It/sec 0.408, Tokens/sec 440.332, Trained Tokens 1182352, Peak mem 3.723 GB\n",
      "Run (15.7% total) | Local 1100/7000 | Global 1100/7000 | LR 2.0e-04 | Train 0.379 | Val 0.399 | ETA 03:43:12Iter 1200: Train loss 0.363, Learning Rate 2.000e-04, It/sec 0.460, Tokens/sec 493.674, Trained Tokens 1289697, Peak mem 3.723 GB\n",
      "Run (17.1% total) | Local 1200/7000 | Global 1200/7000 | LR 2.0e-04 | Train 0.363 | Val 0.399 | ETA 03:36:41Iter 1300: Train loss 0.374, Learning Rate 2.000e-04, It/sec 0.470, Tokens/sec 501.811, Trained Tokens 1396463, Peak mem 3.723 GB\n",
      "Run (18.6% total) | Local 1300/7000 | Global 1300/7000 | LR 2.0e-04 | Train 0.374 | Val 0.399 | ETA 03:29:44Iter 1400: Train loss 0.361, Learning Rate 2.000e-04, It/sec 0.522, Tokens/sec 545.039, Trained Tokens 1500849, Peak mem 3.723 GB\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.361 | Val 0.399 | ETA 03:17:54Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.361 | Val 0.399 | ETA 03:17:54Calculating loss...:   4%|▍         | 1/25 [00:00<00:19,  1.23it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.361 | Val 0.399 | ETA 03:17:54Calculating loss...:   8%|▊         | 2/25 [00:01<00:16,  1.42it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.361 | Val 0.399 | ETA 03:17:54Calculating loss...:  12%|█▏        | 3/25 [00:02<00:14,  1.57it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.361 | Val 0.399 | ETA 03:17:54Calculating loss...:  16%|█▌        | 4/25 [00:02<00:13,  1.52it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.361 | Val 0.399 | ETA 03:17:54Calculating loss...:  20%|██        | 5/25 [00:03<00:12,  1.60it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.361 | Val 0.399 | ETA 03:17:54Calculating loss...:  24%|██▍       | 6/25 [00:03<00:11,  1.66it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.361 | Val 0.399 | ETA 03:17:54Calculating loss...:  28%|██▊       | 7/25 [00:04<00:10,  1.70it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.361 | Val 0.399 | ETA 03:17:54Calculating loss...:  32%|███▏      | 8/25 [00:05<00:11,  1.51it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.361 | Val 0.399 | ETA 03:17:54Calculating loss...:  36%|███▌      | 9/25 [00:05<00:10,  1.59it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.361 | Val 0.399 | ETA 03:17:54Calculating loss...:  40%|████      | 10/25 [00:06<00:09,  1.64it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.361 | Val 0.399 | ETA 03:17:54Calculating loss...:  44%|████▍     | 11/25 [00:07<00:09,  1.45it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.361 | Val 0.399 | ETA 03:17:54Calculating loss...:  48%|████▊     | 12/25 [00:08<00:09,  1.34it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.361 | Val 0.399 | ETA 03:17:54Calculating loss...:  52%|█████▏    | 13/25 [00:08<00:08,  1.45it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.361 | Val 0.399 | ETA 03:17:54Calculating loss...:  56%|█████▌    | 14/25 [00:09<00:06,  1.58it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.361 | Val 0.399 | ETA 03:17:54Calculating loss...:  60%|██████    | 15/25 [00:09<00:06,  1.59it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.361 | Val 0.399 | ETA 03:17:54Calculating loss...:  64%|██████▍   | 16/25 [00:10<00:06,  1.50it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.361 | Val 0.399 | ETA 03:17:54Calculating loss...:  68%|██████▊   | 17/25 [00:11<00:05,  1.41it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.361 | Val 0.399 | ETA 03:17:54Calculating loss...:  72%|███████▏  | 18/25 [00:12<00:04,  1.42it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.361 | Val 0.399 | ETA 03:17:54Calculating loss...:  76%|███████▌  | 19/25 [00:12<00:04,  1.36it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.361 | Val 0.399 | ETA 03:17:54Calculating loss...:  80%|████████  | 20/25 [00:13<00:04,  1.22it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.361 | Val 0.399 | ETA 03:17:54Calculating loss...:  84%|████████▍ | 21/25 [00:14<00:03,  1.31it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.361 | Val 0.399 | ETA 03:17:54Calculating loss...:  88%|████████▊ | 22/25 [00:15<00:02,  1.43it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.361 | Val 0.399 | ETA 03:17:54Calculating loss...:  92%|█████████▏| 23/25 [00:15<00:01,  1.54it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.361 | Val 0.399 | ETA 03:17:54Calculating loss...:  96%|█████████▌| 24/25 [00:16<00:00,  1.43it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.361 | Val 0.399 | ETA 03:17:54Calculating loss...: 100%|██████████| 25/25 [00:17<00:00,  1.36it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.361 | Val 0.399 | ETA 03:17:54Calculating loss...: 100%|██████████| 25/25 [00:17<00:00,  1.46it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.361 | Val 0.399 | ETA 03:17:54Iter 1500: Val loss 0.347, Val took 17.176s\n",
      "Run (21.4% total) | Local 1500/7000 | Global 1500/7000 | LR 2.0e-04 | Train 0.361 | Val 0.347 | ETA 03:14:08Iter 1500: Train loss 0.337, Learning Rate 2.000e-04, It/sec 0.511, Tokens/sec 542.882, Trained Tokens 1607088, Peak mem 3.723 GB\n",
      "Run (21.4% total) | Local 1500/7000 | Global 1500/7000 | LR 2.0e-04 | Train 0.337 | Val 0.347 | ETA 03:14:08Iter 1500: Saved adapter weights to ACLED_llama_fine_tuned/adapters_optimised/adapters.safetensors and ACLED_llama_fine_tuned/adapters_optimised/0001500_adapters.safetensors.\n",
      "Run (21.4% total) | Local 1500/7000 | Global 1500/7000 | LR 2.0e-04 | Train 0.337 | Val 0.347 | ETA 03:14:08Iter 1600: Train loss 0.374, Learning Rate 2.000e-04, It/sec 0.502, Tokens/sec 543.066, Trained Tokens 1715276, Peak mem 3.723 GB\n",
      "Run (22.9% total) | Local 1600/7000 | Global 1600/7000 | LR 2.0e-04 | Train 0.374 | Val 0.347 | ETA 03:07:15Iter 1700: Train loss 0.342, Learning Rate 2.000e-04, It/sec 0.503, Tokens/sec 539.572, Trained Tokens 1822501, Peak mem 3.723 GB\n",
      "Run (24.3% total) | Local 1700/7000 | Global 1700/7000 | LR 2.0e-04 | Train 0.342 | Val 0.347 | ETA 03:01:21Iter 1800: Train loss 0.372, Learning Rate 2.000e-04, It/sec 0.495, Tokens/sec 539.103, Trained Tokens 1931366, Peak mem 3.723 GB\n",
      "Run (25.7% total) | Local 1800/7000 | Global 1800/7000 | LR 2.0e-04 | Train 0.372 | Val 0.347 | ETA 02:57:05Iter 1900: Train loss 0.332, Learning Rate 2.000e-04, It/sec 0.501, Tokens/sec 546.492, Trained Tokens 2040428, Peak mem 3.723 GB\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.332 | Val 0.347 | ETA 02:52:29Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.332 | Val 0.347 | ETA 02:52:29Calculating loss...:   4%|▍         | 1/25 [00:00<00:15,  1.57it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.332 | Val 0.347 | ETA 02:52:29Calculating loss...:   8%|▊         | 2/25 [00:01<00:14,  1.58it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.332 | Val 0.347 | ETA 02:52:29Calculating loss...:  12%|█▏        | 3/25 [00:02<00:15,  1.45it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.332 | Val 0.347 | ETA 02:52:29Calculating loss...:  16%|█▌        | 4/25 [00:02<00:12,  1.62it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.332 | Val 0.347 | ETA 02:52:29Calculating loss...:  20%|██        | 5/25 [00:03<00:13,  1.46it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.332 | Val 0.347 | ETA 02:52:29Calculating loss...:  24%|██▍       | 6/25 [00:04<00:13,  1.37it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.332 | Val 0.347 | ETA 02:52:29Calculating loss...:  28%|██▊       | 7/25 [00:04<00:12,  1.48it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.332 | Val 0.347 | ETA 02:52:29Calculating loss...:  32%|███▏      | 8/25 [00:05<00:12,  1.35it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.332 | Val 0.347 | ETA 02:52:29Calculating loss...:  36%|███▌      | 9/25 [00:06<00:12,  1.30it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.332 | Val 0.347 | ETA 02:52:29Calculating loss...:  40%|████      | 10/25 [00:07<00:10,  1.39it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.332 | Val 0.347 | ETA 02:52:29Calculating loss...:  44%|████▍     | 11/25 [00:07<00:09,  1.54it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.332 | Val 0.347 | ETA 02:52:29Calculating loss...:  48%|████▊     | 12/25 [00:08<00:09,  1.43it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.332 | Val 0.347 | ETA 02:52:29Calculating loss...:  52%|█████▏    | 13/25 [00:08<00:07,  1.57it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.332 | Val 0.347 | ETA 02:52:29Calculating loss...:  56%|█████▌    | 14/25 [00:09<00:07,  1.49it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.332 | Val 0.347 | ETA 02:52:29Calculating loss...:  60%|██████    | 15/25 [00:10<00:06,  1.57it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.332 | Val 0.347 | ETA 02:52:29Calculating loss...:  64%|██████▍   | 16/25 [00:10<00:05,  1.63it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.332 | Val 0.347 | ETA 02:52:29Calculating loss...:  68%|██████▊   | 17/25 [00:11<00:05,  1.37it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.332 | Val 0.347 | ETA 02:52:29Calculating loss...:  72%|███████▏  | 18/25 [00:12<00:05,  1.36it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.332 | Val 0.347 | ETA 02:52:29Calculating loss...:  76%|███████▌  | 19/25 [00:13<00:04,  1.31it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.332 | Val 0.347 | ETA 02:52:29Calculating loss...:  80%|████████  | 20/25 [00:13<00:03,  1.46it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.332 | Val 0.347 | ETA 02:52:29Calculating loss...:  84%|████████▍ | 21/25 [00:14<00:02,  1.46it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.332 | Val 0.347 | ETA 02:52:29Calculating loss...:  88%|████████▊ | 22/25 [00:15<00:02,  1.42it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.332 | Val 0.347 | ETA 02:52:29Calculating loss...:  92%|█████████▏| 23/25 [00:16<00:01,  1.36it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.332 | Val 0.347 | ETA 02:52:29Calculating loss...:  96%|█████████▌| 24/25 [00:16<00:00,  1.38it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.332 | Val 0.347 | ETA 02:52:29Calculating loss...: 100%|██████████| 25/25 [00:17<00:00,  1.33it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.332 | Val 0.347 | ETA 02:52:29Calculating loss...: 100%|██████████| 25/25 [00:17<00:00,  1.43it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.332 | Val 0.347 | ETA 02:52:29Iter 2000: Val loss 0.306, Val took 17.527s\n",
      "Run (28.6% total) | Local 2000/7000 | Global 2000/7000 | LR 2.0e-04 | Train 0.332 | Val 0.306 | ETA 02:50:58Iter 2000: Train loss 0.337, Learning Rate 2.000e-04, It/sec 0.515, Tokens/sec 553.971, Trained Tokens 2147962, Peak mem 3.723 GB\n",
      "Run (28.6% total) | Local 2000/7000 | Global 2000/7000 | LR 2.0e-04 | Train 0.337 | Val 0.306 | ETA 02:50:58Iter 2000: Saved adapter weights to ACLED_llama_fine_tuned/adapters_optimised/adapters.safetensors and ACLED_llama_fine_tuned/adapters_optimised/0002000_adapters.safetensors.\n",
      "Run (28.6% total) | Local 2000/7000 | Global 2000/7000 | LR 2.0e-04 | Train 0.337 | Val 0.306 | ETA 02:50:58Iter 2100: Train loss 0.336, Learning Rate 2.000e-04, It/sec 0.519, Tokens/sec 554.091, Trained Tokens 2254657, Peak mem 3.723 GB\n",
      "Run (30.0% total) | Local 2100/7000 | Global 2100/7000 | LR 2.0e-04 | Train 0.336 | Val 0.306 | ETA 02:44:29Iter 2200: Train loss 0.343, Learning Rate 2.000e-04, It/sec 0.480, Tokens/sec 532.274, Trained Tokens 2365485, Peak mem 3.723 GB\n",
      "Run (31.4% total) | Local 2200/7000 | Global 2200/7000 | LR 2.0e-04 | Train 0.343 | Val 0.306 | ETA 02:42:48Iter 2300: Train loss 0.338, Learning Rate 2.000e-04, It/sec 0.479, Tokens/sec 519.393, Trained Tokens 2473836, Peak mem 3.723 GB\n",
      "Run (32.9% total) | Local 2300/7000 | Global 2300/7000 | LR 2.0e-04 | Train 0.338 | Val 0.306 | ETA 02:40:54Iter 2400: Train loss 0.331, Learning Rate 2.000e-04, It/sec 0.501, Tokens/sec 530.715, Trained Tokens 2579727, Peak mem 3.723 GB\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.331 | Val 0.306 | ETA 02:36:09Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.331 | Val 0.306 | ETA 02:36:09Calculating loss...:   4%|▍         | 1/25 [00:00<00:13,  1.76it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.331 | Val 0.306 | ETA 02:36:09Calculating loss...:   8%|▊         | 2/25 [00:01<00:12,  1.89it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.331 | Val 0.306 | ETA 02:36:09Calculating loss...:  12%|█▏        | 3/25 [00:01<00:11,  1.93it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.331 | Val 0.306 | ETA 02:36:09Calculating loss...:  16%|█▌        | 4/25 [00:02<00:10,  1.95it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.331 | Val 0.306 | ETA 02:36:09Calculating loss...:  20%|██        | 5/25 [00:02<00:11,  1.81it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.331 | Val 0.306 | ETA 02:36:09Calculating loss...:  24%|██▍       | 6/25 [00:03<00:13,  1.41it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.331 | Val 0.306 | ETA 02:36:09Calculating loss...:  28%|██▊       | 7/25 [00:04<00:12,  1.45it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.331 | Val 0.306 | ETA 02:36:09Calculating loss...:  32%|███▏      | 8/25 [00:04<00:11,  1.53it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.331 | Val 0.306 | ETA 02:36:09Calculating loss...:  36%|███▌      | 9/25 [00:05<00:11,  1.39it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.331 | Val 0.306 | ETA 02:36:09Calculating loss...:  40%|████      | 10/25 [00:06<00:10,  1.49it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.331 | Val 0.306 | ETA 02:36:09Calculating loss...:  44%|████▍     | 11/25 [00:06<00:09,  1.53it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.331 | Val 0.306 | ETA 02:36:09Calculating loss...:  48%|████▊     | 12/25 [00:07<00:08,  1.55it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.331 | Val 0.306 | ETA 02:36:09Calculating loss...:  52%|█████▏    | 13/25 [00:08<00:08,  1.40it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.331 | Val 0.306 | ETA 02:36:09Calculating loss...:  56%|█████▌    | 14/25 [00:09<00:07,  1.45it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.331 | Val 0.306 | ETA 02:36:09Calculating loss...:  60%|██████    | 15/25 [00:09<00:06,  1.48it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.331 | Val 0.306 | ETA 02:36:09Calculating loss...:  64%|██████▍   | 16/25 [00:10<00:05,  1.52it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.331 | Val 0.306 | ETA 02:36:09Calculating loss...:  68%|██████▊   | 17/25 [00:11<00:05,  1.54it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.331 | Val 0.306 | ETA 02:36:09Calculating loss...:  72%|███████▏  | 18/25 [00:11<00:04,  1.56it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.331 | Val 0.306 | ETA 02:36:09Calculating loss...:  76%|███████▌  | 19/25 [00:12<00:04,  1.49it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.331 | Val 0.306 | ETA 02:36:09Calculating loss...:  80%|████████  | 20/25 [00:12<00:03,  1.52it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.331 | Val 0.306 | ETA 02:36:09Calculating loss...:  84%|████████▍ | 21/25 [00:13<00:02,  1.55it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.331 | Val 0.306 | ETA 02:36:09Calculating loss...:  88%|████████▊ | 22/25 [00:14<00:02,  1.43it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.331 | Val 0.306 | ETA 02:36:09Calculating loss...:  92%|█████████▏| 23/25 [00:15<00:01,  1.51it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.331 | Val 0.306 | ETA 02:36:09Calculating loss...:  96%|█████████▌| 24/25 [00:15<00:00,  1.59it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.331 | Val 0.306 | ETA 02:36:09Calculating loss...: 100%|██████████| 25/25 [00:16<00:00,  1.64it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.331 | Val 0.306 | ETA 02:36:09Calculating loss...: 100%|██████████| 25/25 [00:16<00:00,  1.55it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.331 | Val 0.306 | ETA 02:36:09Iter 2500: Val loss 0.340, Val took 16.135s\n",
      "Run (35.7% total) | Local 2500/7000 | Global 2500/7000 | LR 2.0e-04 | Train 0.331 | Val 0.340 | ETA 02:34:43Iter 2500: Train loss 0.335, Learning Rate 2.000e-04, It/sec 0.506, Tokens/sec 540.899, Trained Tokens 2686662, Peak mem 3.723 GB\n",
      "Run (35.7% total) | Local 2500/7000 | Global 2500/7000 | LR 2.0e-04 | Train 0.335 | Val 0.340 | ETA 02:34:43Iter 2500: Saved adapter weights to ACLED_llama_fine_tuned/adapters_optimised/adapters.safetensors and ACLED_llama_fine_tuned/adapters_optimised/0002500_adapters.safetensors.\n",
      "Run (35.7% total) | Local 2500/7000 | Global 2500/7000 | LR 2.0e-04 | Train 0.335 | Val 0.340 | ETA 02:34:43Iter 2600: Train loss 0.325, Learning Rate 2.000e-04, It/sec 0.501, Tokens/sec 537.044, Trained Tokens 2793814, Peak mem 3.723 GB\n",
      "Run (37.1% total) | Local 2600/7000 | Global 2600/7000 | LR 2.0e-04 | Train 0.325 | Val 0.340 | ETA 02:29:49Iter 2700: Train loss 0.332, Learning Rate 2.000e-04, It/sec 0.506, Tokens/sec 545.909, Trained Tokens 2901793, Peak mem 3.723 GB\n",
      "Run (38.6% total) | Local 2700/7000 | Global 2700/7000 | LR 2.0e-04 | Train 0.332 | Val 0.340 | ETA 02:25:02Iter 2800: Train loss 0.308, Learning Rate 2.000e-04, It/sec 0.509, Tokens/sec 541.166, Trained Tokens 3008194, Peak mem 3.723 GB\n",
      "Run (40.0% total) | Local 2800/7000 | Global 2800/7000 | LR 2.0e-04 | Train 0.308 | Val 0.340 | ETA 02:20:29Iter 2900: Train loss 0.325, Learning Rate 2.000e-04, It/sec 0.505, Tokens/sec 539.267, Trained Tokens 3114920, Peak mem 3.723 GB\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.325 | Val 0.340 | ETA 02:16:36Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.325 | Val 0.340 | ETA 02:16:36Calculating loss...:   4%|▍         | 1/25 [00:00<00:15,  1.57it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.325 | Val 0.340 | ETA 02:16:36Calculating loss...:   8%|▊         | 2/25 [00:01<00:13,  1.68it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.325 | Val 0.340 | ETA 02:16:36Calculating loss...:  12%|█▏        | 3/25 [00:01<00:14,  1.49it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.325 | Val 0.340 | ETA 02:16:36Calculating loss...:  16%|█▌        | 4/25 [00:02<00:13,  1.58it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.325 | Val 0.340 | ETA 02:16:36Calculating loss...:  20%|██        | 5/25 [00:03<00:13,  1.49it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.325 | Val 0.340 | ETA 02:16:36Calculating loss...:  24%|██▍       | 6/25 [00:03<00:12,  1.53it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.325 | Val 0.340 | ETA 02:16:36Calculating loss...:  28%|██▊       | 7/25 [00:04<00:11,  1.51it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.325 | Val 0.340 | ETA 02:16:36Calculating loss...:  32%|███▏      | 8/25 [00:05<00:10,  1.59it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.325 | Val 0.340 | ETA 02:16:36Calculating loss...:  36%|███▌      | 9/25 [00:05<00:09,  1.65it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.325 | Val 0.340 | ETA 02:16:36Calculating loss...:  40%|████      | 10/25 [00:06<00:09,  1.59it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.325 | Val 0.340 | ETA 02:16:36Calculating loss...:  44%|████▍     | 11/25 [00:06<00:08,  1.64it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.325 | Val 0.340 | ETA 02:16:36Calculating loss...:  48%|████▊     | 12/25 [00:07<00:07,  1.68it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.325 | Val 0.340 | ETA 02:16:36Calculating loss...:  52%|█████▏    | 13/25 [00:08<00:07,  1.66it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.325 | Val 0.340 | ETA 02:16:36Calculating loss...:  56%|█████▌    | 14/25 [00:08<00:06,  1.64it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.325 | Val 0.340 | ETA 02:16:36Calculating loss...:  60%|██████    | 15/25 [00:09<00:06,  1.63it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.325 | Val 0.340 | ETA 02:16:36Calculating loss...:  64%|██████▍   | 16/25 [00:09<00:05,  1.63it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.325 | Val 0.340 | ETA 02:16:36Calculating loss...:  68%|██████▊   | 17/25 [00:10<00:05,  1.48it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.325 | Val 0.340 | ETA 02:16:36Calculating loss...:  72%|███████▏  | 18/25 [00:11<00:05,  1.40it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.325 | Val 0.340 | ETA 02:16:36Calculating loss...:  76%|███████▌  | 19/25 [00:12<00:04,  1.45it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.325 | Val 0.340 | ETA 02:16:36Calculating loss...:  80%|████████  | 20/25 [00:13<00:03,  1.38it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.325 | Val 0.340 | ETA 02:16:36Calculating loss...:  84%|████████▍ | 21/25 [00:13<00:02,  1.47it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.325 | Val 0.340 | ETA 02:16:36Calculating loss...:  88%|████████▊ | 22/25 [00:14<00:01,  1.55it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.325 | Val 0.340 | ETA 02:16:36Calculating loss...:  92%|█████████▏| 23/25 [00:14<00:01,  1.61it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.325 | Val 0.340 | ETA 02:16:36Calculating loss...:  96%|█████████▌| 24/25 [00:15<00:00,  1.66it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.325 | Val 0.340 | ETA 02:16:36Calculating loss...: 100%|██████████| 25/25 [00:15<00:00,  1.59it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.325 | Val 0.340 | ETA 02:16:36Calculating loss...: 100%|██████████| 25/25 [00:15<00:00,  1.56it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.325 | Val 0.340 | ETA 02:16:36Iter 3000: Val loss 0.299, Val took 16.004s\n",
      "Run (42.9% total) | Local 3000/7000 | Global 3000/7000 | LR 2.0e-04 | Train 0.325 | Val 0.299 | ETA 02:16:14Iter 3000: Train loss 0.320, Learning Rate 2.000e-04, It/sec 0.498, Tokens/sec 544.823, Trained Tokens 3224267, Peak mem 3.723 GB\n",
      "Run (42.9% total) | Local 3000/7000 | Global 3000/7000 | LR 2.0e-04 | Train 0.320 | Val 0.299 | ETA 02:16:14Iter 3000: Saved adapter weights to ACLED_llama_fine_tuned/adapters_optimised/adapters.safetensors and ACLED_llama_fine_tuned/adapters_optimised/0003000_adapters.safetensors.\n",
      "Run (42.9% total) | Local 3000/7000 | Global 3000/7000 | LR 2.0e-04 | Train 0.320 | Val 0.299 | ETA 02:16:14Iter 3100: Train loss 0.331, Learning Rate 2.000e-04, It/sec 0.502, Tokens/sec 538.072, Trained Tokens 3331485, Peak mem 3.723 GB\n",
      "Run (44.3% total) | Local 3100/7000 | Global 3100/7000 | LR 2.0e-04 | Train 0.331 | Val 0.299 | ETA 02:11:51Iter 3200: Train loss 0.320, Learning Rate 2.000e-04, It/sec 0.512, Tokens/sec 550.936, Trained Tokens 3439176, Peak mem 3.723 GB\n",
      "Run (45.7% total) | Local 3200/7000 | Global 3200/7000 | LR 2.0e-04 | Train 0.320 | Val 0.299 | ETA 02:07:06Iter 3300: Train loss 0.313, Learning Rate 2.000e-04, It/sec 0.512, Tokens/sec 548.528, Trained Tokens 3546343, Peak mem 3.723 GB\n",
      "Run (47.1% total) | Local 3300/7000 | Global 3300/7000 | LR 2.0e-04 | Train 0.313 | Val 0.299 | ETA 02:02:48Iter 3400: Train loss 0.319, Learning Rate 2.000e-04, It/sec 0.489, Tokens/sec 530.676, Trained Tokens 3654786, Peak mem 3.723 GB\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.319 | Val 0.299 | ETA 02:00:26Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.319 | Val 0.299 | ETA 02:00:26Calculating loss...:   4%|▍         | 1/25 [00:00<00:18,  1.32it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.319 | Val 0.299 | ETA 02:00:26Calculating loss...:   8%|▊         | 2/25 [00:01<00:14,  1.56it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.319 | Val 0.299 | ETA 02:00:26Calculating loss...:  12%|█▏        | 3/25 [00:01<00:13,  1.65it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.319 | Val 0.299 | ETA 02:00:26Calculating loss...:  16%|█▌        | 4/25 [00:02<00:12,  1.70it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.319 | Val 0.299 | ETA 02:00:26Calculating loss...:  20%|██        | 5/25 [00:03<00:12,  1.67it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.319 | Val 0.299 | ETA 02:00:26Calculating loss...:  24%|██▍       | 6/25 [00:04<00:15,  1.26it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.319 | Val 0.299 | ETA 02:00:26Calculating loss...:  28%|██▊       | 7/25 [00:04<00:14,  1.28it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.319 | Val 0.299 | ETA 02:00:26Calculating loss...:  32%|███▏      | 8/25 [00:05<00:12,  1.31it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.319 | Val 0.299 | ETA 02:00:26Calculating loss...:  36%|███▌      | 9/25 [00:06<00:11,  1.38it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.319 | Val 0.299 | ETA 02:00:26Calculating loss...:  40%|████      | 10/25 [00:07<00:11,  1.35it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.319 | Val 0.299 | ETA 02:00:26Calculating loss...:  44%|████▍     | 11/25 [00:07<00:10,  1.31it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.319 | Val 0.299 | ETA 02:00:26Calculating loss...:  48%|████▊     | 12/25 [00:08<00:09,  1.43it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.319 | Val 0.299 | ETA 02:00:26Calculating loss...:  52%|█████▏    | 13/25 [00:09<00:08,  1.49it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.319 | Val 0.299 | ETA 02:00:26Calculating loss...:  56%|█████▌    | 14/25 [00:09<00:07,  1.53it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.319 | Val 0.299 | ETA 02:00:26Calculating loss...:  60%|██████    | 15/25 [00:10<00:06,  1.51it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.319 | Val 0.299 | ETA 02:00:26Calculating loss...:  64%|██████▍   | 16/25 [00:11<00:06,  1.38it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.319 | Val 0.299 | ETA 02:00:26Calculating loss...:  68%|██████▊   | 17/25 [00:12<00:05,  1.34it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.319 | Val 0.299 | ETA 02:00:26Calculating loss...:  72%|███████▏  | 18/25 [00:12<00:04,  1.41it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.319 | Val 0.299 | ETA 02:00:26Calculating loss...:  76%|███████▌  | 19/25 [00:13<00:04,  1.39it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.319 | Val 0.299 | ETA 02:00:26Calculating loss...:  80%|████████  | 20/25 [00:14<00:03,  1.34it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.319 | Val 0.299 | ETA 02:00:26Calculating loss...:  84%|████████▍ | 21/25 [00:14<00:02,  1.41it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.319 | Val 0.299 | ETA 02:00:26Calculating loss...:  88%|████████▊ | 22/25 [00:15<00:02,  1.45it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.319 | Val 0.299 | ETA 02:00:26Calculating loss...:  92%|█████████▏| 23/25 [00:16<00:01,  1.49it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.319 | Val 0.299 | ETA 02:00:26Calculating loss...:  96%|█████████▌| 24/25 [00:16<00:00,  1.57it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.319 | Val 0.299 | ETA 02:00:26Calculating loss...: 100%|██████████| 25/25 [00:17<00:00,  1.45it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.319 | Val 0.299 | ETA 02:00:26Calculating loss...: 100%|██████████| 25/25 [00:17<00:00,  1.43it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.319 | Val 0.299 | ETA 02:00:26Iter 3500: Val loss 0.310, Val took 17.504s\n",
      "Run (50.0% total) | Local 3500/7000 | Global 3500/7000 | LR 2.0e-04 | Train 0.319 | Val 0.310 | ETA 01:59:34Iter 3500: Train loss 0.317, Learning Rate 2.000e-04, It/sec 0.501, Tokens/sec 544.994, Trained Tokens 3763554, Peak mem 3.723 GB\n",
      "Run (50.0% total) | Local 3500/7000 | Global 3500/7000 | LR 2.0e-04 | Train 0.317 | Val 0.310 | ETA 01:59:34Iter 3500: Saved adapter weights to ACLED_llama_fine_tuned/adapters_optimised/adapters.safetensors and ACLED_llama_fine_tuned/adapters_optimised/0003500_adapters.safetensors.\n",
      "Run (50.0% total) | Local 3500/7000 | Global 3500/7000 | LR 2.0e-04 | Train 0.317 | Val 0.310 | ETA 01:59:34Iter 3600: Train loss 0.324, Learning Rate 2.000e-04, It/sec 0.516, Tokens/sec 547.675, Trained Tokens 3869692, Peak mem 3.723 GB\n",
      "Run (51.4% total) | Local 3600/7000 | Global 3600/7000 | LR 2.0e-04 | Train 0.324 | Val 0.310 | ETA 01:54:16Iter 3700: Train loss 0.299, Learning Rate 2.000e-04, It/sec 0.507, Tokens/sec 543.698, Trained Tokens 3976948, Peak mem 3.723 GB\n",
      "Run (52.9% total) | Local 3700/7000 | Global 3700/7000 | LR 2.0e-04 | Train 0.299 | Val 0.310 | ETA 01:50:12Iter 3800: Train loss 0.307, Learning Rate 2.000e-04, It/sec 0.498, Tokens/sec 539.965, Trained Tokens 4085372, Peak mem 3.723 GB\n",
      "Run (54.3% total) | Local 3800/7000 | Global 3800/7000 | LR 2.0e-04 | Train 0.307 | Val 0.310 | ETA 01:46:57Iter 3900: Train loss 0.324, Learning Rate 2.000e-04, It/sec 0.499, Tokens/sec 542.800, Trained Tokens 4194047, Peak mem 3.723 GB\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.324 | Val 0.310 | ETA 01:43:35Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.324 | Val 0.310 | ETA 01:43:35Calculating loss...:   4%|▍         | 1/25 [00:00<00:14,  1.71it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.324 | Val 0.310 | ETA 01:43:35Calculating loss...:   8%|▊         | 2/25 [00:01<00:13,  1.73it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.324 | Val 0.310 | ETA 01:43:35Calculating loss...:  12%|█▏        | 3/25 [00:01<00:13,  1.65it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.324 | Val 0.310 | ETA 01:43:35Calculating loss...:  16%|█▌        | 4/25 [00:02<00:13,  1.56it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.324 | Val 0.310 | ETA 01:43:35Calculating loss...:  20%|██        | 5/25 [00:03<00:14,  1.37it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.324 | Val 0.310 | ETA 01:43:35Calculating loss...:  24%|██▍       | 6/25 [00:03<00:13,  1.45it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.324 | Val 0.310 | ETA 01:43:35Calculating loss...:  28%|██▊       | 7/25 [00:04<00:12,  1.45it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.324 | Val 0.310 | ETA 01:43:35Calculating loss...:  32%|███▏      | 8/25 [00:05<00:11,  1.46it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.324 | Val 0.310 | ETA 01:43:35Calculating loss...:  36%|███▌      | 9/25 [00:05<00:10,  1.55it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.324 | Val 0.310 | ETA 01:43:35Calculating loss...:  40%|████      | 10/25 [00:06<00:10,  1.47it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.324 | Val 0.310 | ETA 01:43:35Calculating loss...:  44%|████▍     | 11/25 [00:07<00:09,  1.51it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.324 | Val 0.310 | ETA 01:43:35Calculating loss...:  48%|████▊     | 12/25 [00:08<00:08,  1.46it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.324 | Val 0.310 | ETA 01:43:35Calculating loss...:  52%|█████▏    | 13/25 [00:08<00:08,  1.38it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.324 | Val 0.310 | ETA 01:43:35Calculating loss...:  56%|█████▌    | 14/25 [00:09<00:07,  1.44it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.324 | Val 0.310 | ETA 01:43:35Calculating loss...:  60%|██████    | 15/25 [00:10<00:06,  1.45it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.324 | Val 0.310 | ETA 01:43:35Calculating loss...:  64%|██████▍   | 16/25 [00:10<00:06,  1.50it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.324 | Val 0.310 | ETA 01:43:35Calculating loss...:  68%|██████▊   | 17/25 [00:11<00:05,  1.53it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.324 | Val 0.310 | ETA 01:43:35Calculating loss...:  72%|███████▏  | 18/25 [00:12<00:04,  1.45it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.324 | Val 0.310 | ETA 01:43:35Calculating loss...:  76%|███████▌  | 19/25 [00:12<00:03,  1.50it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.324 | Val 0.310 | ETA 01:43:35Calculating loss...:  80%|████████  | 20/25 [00:13<00:03,  1.58it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.324 | Val 0.310 | ETA 01:43:35Calculating loss...:  84%|████████▍ | 21/25 [00:13<00:02,  1.59it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.324 | Val 0.310 | ETA 01:43:35Calculating loss...:  88%|████████▊ | 22/25 [00:14<00:01,  1.65it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.324 | Val 0.310 | ETA 01:43:35Calculating loss...:  92%|█████████▏| 23/25 [00:15<00:01,  1.46it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.324 | Val 0.310 | ETA 01:43:35Calculating loss...:  96%|█████████▌| 24/25 [00:15<00:00,  1.55it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.324 | Val 0.310 | ETA 01:43:35Calculating loss...: 100%|██████████| 25/25 [00:16<00:00,  1.66it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.324 | Val 0.310 | ETA 01:43:35Calculating loss...: 100%|██████████| 25/25 [00:16<00:00,  1.52it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.324 | Val 0.310 | ETA 01:43:35Iter 4000: Val loss 0.323, Val took 16.424s\n",
      "Run (57.1% total) | Local 4000/7000 | Global 4000/7000 | LR 2.0e-04 | Train 0.324 | Val 0.323 | ETA 01:41:32Iter 4000: Train loss 0.293, Learning Rate 2.000e-04, It/sec 0.515, Tokens/sec 548.759, Trained Tokens 4300596, Peak mem 3.723 GB\n",
      "Run (57.1% total) | Local 4000/7000 | Global 4000/7000 | LR 2.0e-04 | Train 0.293 | Val 0.323 | ETA 01:41:32Iter 4000: Saved adapter weights to ACLED_llama_fine_tuned/adapters_optimised/adapters.safetensors and ACLED_llama_fine_tuned/adapters_optimised/0004000_adapters.safetensors.\n",
      "Run (57.1% total) | Local 4000/7000 | Global 4000/7000 | LR 2.0e-04 | Train 0.293 | Val 0.323 | ETA 01:41:32Iter 4100: Train loss 0.305, Learning Rate 2.000e-04, It/sec 0.499, Tokens/sec 540.582, Trained Tokens 4408966, Peak mem 3.723 GB\n",
      "Run (58.6% total) | Local 4100/7000 | Global 4100/7000 | LR 2.0e-04 | Train 0.305 | Val 0.323 | ETA 01:37:47Iter 4200: Train loss 0.294, Learning Rate 2.000e-04, It/sec 0.501, Tokens/sec 548.291, Trained Tokens 4518305, Peak mem 3.723 GB\n",
      "Run (60.0% total) | Local 4200/7000 | Global 4200/7000 | LR 2.0e-04 | Train 0.294 | Val 0.323 | ETA 01:34:02Iter 4300: Train loss 0.316, Learning Rate 2.000e-04, It/sec 0.511, Tokens/sec 550.625, Trained Tokens 4626099, Peak mem 3.723 GB\n",
      "Run (61.4% total) | Local 4300/7000 | Global 4300/7000 | LR 2.0e-04 | Train 0.316 | Val 0.323 | ETA 01:29:55Iter 4400: Train loss 0.285, Learning Rate 2.000e-04, It/sec 0.525, Tokens/sec 558.000, Trained Tokens 4732468, Peak mem 3.723 GB\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.285 | Val 0.323 | ETA 01:25:24Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.285 | Val 0.323 | ETA 01:25:24Calculating loss...:   4%|▍         | 1/25 [00:00<00:16,  1.44it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.285 | Val 0.323 | ETA 01:25:24Calculating loss...:   8%|▊         | 2/25 [00:01<00:15,  1.45it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.285 | Val 0.323 | ETA 01:25:24Calculating loss...:  12%|█▏        | 3/25 [00:02<00:16,  1.34it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.285 | Val 0.323 | ETA 01:25:24Calculating loss...:  16%|█▌        | 4/25 [00:02<00:13,  1.54it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.285 | Val 0.323 | ETA 01:25:24Calculating loss...:  20%|██        | 5/25 [00:03<00:12,  1.63it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.285 | Val 0.323 | ETA 01:25:24Calculating loss...:  24%|██▍       | 6/25 [00:03<00:12,  1.55it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.285 | Val 0.323 | ETA 01:25:24Calculating loss...:  28%|██▊       | 7/25 [00:04<00:12,  1.43it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.285 | Val 0.323 | ETA 01:25:24Calculating loss...:  32%|███▏      | 8/25 [00:05<00:11,  1.45it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.285 | Val 0.323 | ETA 01:25:24Calculating loss...:  36%|███▌      | 9/25 [00:06<00:10,  1.50it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.285 | Val 0.323 | ETA 01:25:24Calculating loss...:  40%|████      | 10/25 [00:06<00:10,  1.37it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.285 | Val 0.323 | ETA 01:25:24Calculating loss...:  44%|████▍     | 11/25 [00:07<00:09,  1.52it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.285 | Val 0.323 | ETA 01:25:24Calculating loss...:  48%|████▊     | 12/25 [00:08<00:08,  1.47it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.285 | Val 0.323 | ETA 01:25:24Calculating loss...:  52%|█████▏    | 13/25 [00:08<00:07,  1.51it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.285 | Val 0.323 | ETA 01:25:24Calculating loss...:  56%|█████▌    | 14/25 [00:09<00:06,  1.58it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.285 | Val 0.323 | ETA 01:25:24Calculating loss...:  60%|██████    | 15/25 [00:09<00:06,  1.59it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.285 | Val 0.323 | ETA 01:25:24Calculating loss...:  64%|██████▍   | 16/25 [00:10<00:05,  1.60it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.285 | Val 0.323 | ETA 01:25:24Calculating loss...:  68%|██████▊   | 17/25 [00:11<00:05,  1.57it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.285 | Val 0.323 | ETA 01:25:24Calculating loss...:  72%|███████▏  | 18/25 [00:11<00:04,  1.59it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.285 | Val 0.323 | ETA 01:25:24Calculating loss...:  76%|███████▌  | 19/25 [00:12<00:03,  1.61it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.285 | Val 0.323 | ETA 01:25:24Calculating loss...:  80%|████████  | 20/25 [00:13<00:03,  1.39it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.285 | Val 0.323 | ETA 01:25:24Calculating loss...:  84%|████████▍ | 21/25 [00:13<00:02,  1.52it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.285 | Val 0.323 | ETA 01:25:24Calculating loss...:  88%|████████▊ | 22/25 [00:14<00:02,  1.48it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.285 | Val 0.323 | ETA 01:25:24Calculating loss...:  92%|█████████▏| 23/25 [00:15<00:01,  1.48it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.285 | Val 0.323 | ETA 01:25:24Calculating loss...:  96%|█████████▌| 24/25 [00:16<00:00,  1.34it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.285 | Val 0.323 | ETA 01:25:24Calculating loss...: 100%|██████████| 25/25 [00:16<00:00,  1.37it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.285 | Val 0.323 | ETA 01:25:24Calculating loss...: 100%|██████████| 25/25 [00:16<00:00,  1.48it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.285 | Val 0.323 | ETA 01:25:24Iter 4500: Val loss 0.293, Val took 16.899s\n",
      "Run (64.3% total) | Local 4500/7000 | Global 4500/7000 | LR 2.0e-04 | Train 0.285 | Val 0.293 | ETA 01:23:56Iter 4500: Train loss 0.321, Learning Rate 2.000e-04, It/sec 0.508, Tokens/sec 547.411, Trained Tokens 4840292, Peak mem 3.723 GB\n",
      "Run (64.3% total) | Local 4500/7000 | Global 4500/7000 | LR 2.0e-04 | Train 0.321 | Val 0.293 | ETA 01:23:56Iter 4500: Saved adapter weights to ACLED_llama_fine_tuned/adapters_optimised/adapters.safetensors and ACLED_llama_fine_tuned/adapters_optimised/0004500_adapters.safetensors.\n",
      "Run (64.3% total) | Local 4500/7000 | Global 4500/7000 | LR 2.0e-04 | Train 0.321 | Val 0.293 | ETA 01:23:56Iter 4600: Train loss 0.310, Learning Rate 2.000e-04, It/sec 0.515, Tokens/sec 557.111, Trained Tokens 4948475, Peak mem 3.723 GB\n",
      "Run (65.7% total) | Local 4600/7000 | Global 4600/7000 | LR 2.0e-04 | Train 0.310 | Val 0.293 | ETA 01:19:43Iter 4700: Train loss 0.319, Learning Rate 2.000e-04, It/sec 0.525, Tokens/sec 562.126, Trained Tokens 5055537, Peak mem 3.723 GB\n",
      "Run (67.1% total) | Local 4700/7000 | Global 4700/7000 | LR 2.0e-04 | Train 0.319 | Val 0.293 | ETA 01:15:24Iter 4800: Train loss 0.294, Learning Rate 2.000e-04, It/sec 0.497, Tokens/sec 541.219, Trained Tokens 5164448, Peak mem 3.723 GB\n",
      "Run (68.6% total) | Local 4800/7000 | Global 4800/7000 | LR 2.0e-04 | Train 0.294 | Val 0.293 | ETA 01:12:38Iter 4900: Train loss 0.302, Learning Rate 2.000e-04, It/sec 0.528, Tokens/sec 561.554, Trained Tokens 5270878, Peak mem 3.723 GB\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.302 | Val 0.293 | ETA 01:08:26Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.302 | Val 0.293 | ETA 01:08:26Calculating loss...:   4%|▍         | 1/25 [00:00<00:18,  1.29it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.302 | Val 0.293 | ETA 01:08:26Calculating loss...:   8%|▊         | 2/25 [00:01<00:15,  1.53it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.302 | Val 0.293 | ETA 01:08:26Calculating loss...:  12%|█▏        | 3/25 [00:02<00:16,  1.37it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.302 | Val 0.293 | ETA 01:08:26Calculating loss...:  16%|█▌        | 4/25 [00:02<00:15,  1.31it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.302 | Val 0.293 | ETA 01:08:26Calculating loss...:  20%|██        | 5/25 [00:03<00:13,  1.45it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.302 | Val 0.293 | ETA 01:08:26Calculating loss...:  24%|██▍       | 6/25 [00:04<00:14,  1.33it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.302 | Val 0.293 | ETA 01:08:26Calculating loss...:  28%|██▊       | 7/25 [00:05<00:15,  1.20it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.302 | Val 0.293 | ETA 01:08:26Calculating loss...:  32%|███▏      | 8/25 [00:05<00:12,  1.33it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.302 | Val 0.293 | ETA 01:08:26Calculating loss...:  36%|███▌      | 9/25 [00:06<00:10,  1.49it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.302 | Val 0.293 | ETA 01:08:26Calculating loss...:  40%|████      | 10/25 [00:07<00:09,  1.53it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.302 | Val 0.293 | ETA 01:08:26Calculating loss...:  44%|████▍     | 11/25 [00:07<00:09,  1.43it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.302 | Val 0.293 | ETA 01:08:26Calculating loss...:  48%|████▊     | 12/25 [00:08<00:08,  1.48it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.302 | Val 0.293 | ETA 01:08:26Calculating loss...:  52%|█████▏    | 13/25 [00:09<00:07,  1.57it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.302 | Val 0.293 | ETA 01:08:26Calculating loss...:  56%|█████▌    | 14/25 [00:09<00:06,  1.58it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.302 | Val 0.293 | ETA 01:08:26Calculating loss...:  60%|██████    | 15/25 [00:10<00:06,  1.59it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.302 | Val 0.293 | ETA 01:08:26Calculating loss...:  64%|██████▍   | 16/25 [00:10<00:05,  1.54it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.302 | Val 0.293 | ETA 01:08:26Calculating loss...:  68%|██████▊   | 17/25 [00:11<00:05,  1.56it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.302 | Val 0.293 | ETA 01:08:26Calculating loss...:  72%|███████▏  | 18/25 [00:12<00:04,  1.68it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.302 | Val 0.293 | ETA 01:08:26Calculating loss...:  76%|███████▌  | 19/25 [00:12<00:03,  1.56it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.302 | Val 0.293 | ETA 01:08:26Calculating loss...:  80%|████████  | 20/25 [00:13<00:03,  1.45it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.302 | Val 0.293 | ETA 01:08:26Calculating loss...:  84%|████████▍ | 21/25 [00:14<00:03,  1.24it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.302 | Val 0.293 | ETA 01:08:26Calculating loss...:  88%|████████▊ | 22/25 [00:15<00:02,  1.36it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.302 | Val 0.293 | ETA 01:08:26Calculating loss...:  92%|█████████▏| 23/25 [00:15<00:01,  1.42it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.302 | Val 0.293 | ETA 01:08:26Calculating loss...:  96%|█████████▌| 24/25 [00:16<00:00,  1.36it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.302 | Val 0.293 | ETA 01:08:26Calculating loss...: 100%|██████████| 25/25 [00:17<00:00,  1.47it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.302 | Val 0.293 | ETA 01:08:26Calculating loss...: 100%|██████████| 25/25 [00:17<00:00,  1.44it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.302 | Val 0.293 | ETA 01:08:26Iter 5000: Val loss 0.287, Val took 17.316s\n",
      "Run (71.4% total) | Local 5000/7000 | Global 5000/7000 | LR 2.0e-04 | Train 0.302 | Val 0.287 | ETA 01:06:49Iter 5000: Train loss 0.308, Learning Rate 2.000e-04, It/sec 0.508, Tokens/sec 547.613, Trained Tokens 5378575, Peak mem 3.723 GB\n",
      "Run (71.4% total) | Local 5000/7000 | Global 5000/7000 | LR 2.0e-04 | Train 0.308 | Val 0.287 | ETA 01:06:49Iter 5000: Saved adapter weights to ACLED_llama_fine_tuned/adapters_optimised/adapters.safetensors and ACLED_llama_fine_tuned/adapters_optimised/0005000_adapters.safetensors.\n",
      "Run (71.4% total) | Local 5000/7000 | Global 5000/7000 | LR 2.0e-04 | Train 0.308 | Val 0.287 | ETA 01:06:49Iter 5100: Train loss 0.283, Learning Rate 2.000e-04, It/sec 0.504, Tokens/sec 539.790, Trained Tokens 5485649, Peak mem 3.723 GB\n",
      "Run (72.9% total) | Local 5100/7000 | Global 5100/7000 | LR 2.0e-04 | Train 0.283 | Val 0.287 | ETA 01:03:17Iter 5200: Train loss 0.289, Learning Rate 2.000e-04, It/sec 0.504, Tokens/sec 548.603, Trained Tokens 5594420, Peak mem 3.723 GB\n",
      "Run (74.3% total) | Local 5200/7000 | Global 5200/7000 | LR 2.0e-04 | Train 0.289 | Val 0.287 | ETA 00:59:48Iter 5300: Train loss 0.283, Learning Rate 2.000e-04, It/sec 0.514, Tokens/sec 547.405, Trained Tokens 5700940, Peak mem 3.723 GB\n",
      "Run (75.7% total) | Local 5300/7000 | Global 5300/7000 | LR 2.0e-04 | Train 0.283 | Val 0.287 | ETA 00:56:05Iter 5400: Train loss 0.282, Learning Rate 2.000e-04, It/sec 0.495, Tokens/sec 540.968, Trained Tokens 5810323, Peak mem 3.723 GB\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.282 | Val 0.287 | ETA 00:53:07Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.282 | Val 0.287 | ETA 00:53:07Calculating loss...:   4%|▍         | 1/25 [00:00<00:22,  1.06it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.282 | Val 0.287 | ETA 00:53:07Calculating loss...:   8%|▊         | 2/25 [00:01<00:15,  1.45it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.282 | Val 0.287 | ETA 00:53:07Calculating loss...:  12%|█▏        | 3/25 [00:02<00:17,  1.29it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.282 | Val 0.287 | ETA 00:53:07Calculating loss...:  16%|█▌        | 4/25 [00:02<00:14,  1.40it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.282 | Val 0.287 | ETA 00:53:07Calculating loss...:  20%|██        | 5/25 [00:03<00:13,  1.52it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.282 | Val 0.287 | ETA 00:53:07Calculating loss...:  24%|██▍       | 6/25 [00:04<00:12,  1.54it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.282 | Val 0.287 | ETA 00:53:07Calculating loss...:  28%|██▊       | 7/25 [00:04<00:11,  1.56it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.282 | Val 0.287 | ETA 00:53:07Calculating loss...:  32%|███▏      | 8/25 [00:05<00:10,  1.62it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.282 | Val 0.287 | ETA 00:53:07Calculating loss...:  36%|███▌      | 9/25 [00:05<00:09,  1.67it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.282 | Val 0.287 | ETA 00:53:07Calculating loss...:  40%|████      | 10/25 [00:06<00:09,  1.54it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.282 | Val 0.287 | ETA 00:53:07Calculating loss...:  44%|████▍     | 11/25 [00:07<00:09,  1.51it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.282 | Val 0.287 | ETA 00:53:07Calculating loss...:  48%|████▊     | 12/25 [00:08<00:08,  1.45it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.282 | Val 0.287 | ETA 00:53:07Calculating loss...:  52%|█████▏    | 13/25 [00:08<00:08,  1.34it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.282 | Val 0.287 | ETA 00:53:07Calculating loss...:  56%|█████▌    | 14/25 [00:09<00:08,  1.27it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.282 | Val 0.287 | ETA 00:53:07Calculating loss...:  60%|██████    | 15/25 [00:10<00:07,  1.35it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.282 | Val 0.287 | ETA 00:53:07Calculating loss...:  64%|██████▍   | 16/25 [00:11<00:06,  1.42it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.282 | Val 0.287 | ETA 00:53:07Calculating loss...:  68%|██████▊   | 17/25 [00:11<00:05,  1.48it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.282 | Val 0.287 | ETA 00:53:07Calculating loss...:  72%|███████▏  | 18/25 [00:12<00:04,  1.52it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.282 | Val 0.287 | ETA 00:53:07Calculating loss...:  76%|███████▌  | 19/25 [00:12<00:03,  1.54it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.282 | Val 0.287 | ETA 00:53:07Calculating loss...:  80%|████████  | 20/25 [00:13<00:03,  1.44it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.282 | Val 0.287 | ETA 00:53:07Calculating loss...:  84%|████████▍ | 21/25 [00:14<00:02,  1.48it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.282 | Val 0.287 | ETA 00:53:07Calculating loss...:  88%|████████▊ | 22/25 [00:15<00:02,  1.42it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.282 | Val 0.287 | ETA 00:53:07Calculating loss...:  92%|█████████▏| 23/25 [00:15<00:01,  1.51it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.282 | Val 0.287 | ETA 00:53:07Calculating loss...:  96%|█████████▌| 24/25 [00:16<00:00,  1.58it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.282 | Val 0.287 | ETA 00:53:07Calculating loss...: 100%|██████████| 25/25 [00:16<00:00,  1.63it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.282 | Val 0.287 | ETA 00:53:07Calculating loss...: 100%|██████████| 25/25 [00:16<00:00,  1.48it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.282 | Val 0.287 | ETA 00:53:07Iter 5500: Val loss 0.277, Val took 16.866s\n",
      "Run (78.6% total) | Local 5500/7000 | Global 5500/7000 | LR 2.0e-04 | Train 0.282 | Val 0.277 | ETA 00:50:46Iter 5500: Train loss 0.275, Learning Rate 2.000e-04, It/sec 0.507, Tokens/sec 540.385, Trained Tokens 5916840, Peak mem 3.723 GB\n",
      "Run (78.6% total) | Local 5500/7000 | Global 5500/7000 | LR 2.0e-04 | Train 0.275 | Val 0.277 | ETA 00:50:46Iter 5500: Saved adapter weights to ACLED_llama_fine_tuned/adapters_optimised/adapters.safetensors and ACLED_llama_fine_tuned/adapters_optimised/0005500_adapters.safetensors.\n",
      "Run (78.6% total) | Local 5500/7000 | Global 5500/7000 | LR 2.0e-04 | Train 0.275 | Val 0.277 | ETA 00:50:46Iter 5600: Train loss 0.283, Learning Rate 2.000e-04, It/sec 0.497, Tokens/sec 535.574, Trained Tokens 6024585, Peak mem 3.723 GB\n",
      "Run (80.0% total) | Local 5600/7000 | Global 5600/7000 | LR 2.0e-04 | Train 0.283 | Val 0.277 | ETA 00:47:15Iter 5700: Train loss 0.290, Learning Rate 2.000e-04, It/sec 0.496, Tokens/sec 540.400, Trained Tokens 6133452, Peak mem 3.723 GB\n",
      "Run (81.4% total) | Local 5700/7000 | Global 5700/7000 | LR 2.0e-04 | Train 0.290 | Val 0.277 | ETA 00:43:48Iter 5800: Train loss 0.284, Learning Rate 2.000e-04, It/sec 0.495, Tokens/sec 535.466, Trained Tokens 6241667, Peak mem 3.723 GB\n",
      "Run (82.9% total) | Local 5800/7000 | Global 5800/7000 | LR 2.0e-04 | Train 0.284 | Val 0.277 | ETA 00:40:26Iter 5900: Train loss 0.286, Learning Rate 2.000e-04, It/sec 0.497, Tokens/sec 539.541, Trained Tokens 6350321, Peak mem 3.723 GB\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.286 | Val 0.277 | ETA 00:37:01Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.286 | Val 0.277 | ETA 00:37:01Calculating loss...:   4%|▍         | 1/25 [00:00<00:14,  1.71it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.286 | Val 0.277 | ETA 00:37:01Calculating loss...:   8%|▊         | 2/25 [00:01<00:19,  1.20it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.286 | Val 0.277 | ETA 00:37:01Calculating loss...:  12%|█▏        | 3/25 [00:02<00:18,  1.21it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.286 | Val 0.277 | ETA 00:37:01Calculating loss...:  16%|█▌        | 4/25 [00:03<00:18,  1.15it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.286 | Val 0.277 | ETA 00:37:01Calculating loss...:  20%|██        | 5/25 [00:03<00:15,  1.32it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.286 | Val 0.277 | ETA 00:37:01Calculating loss...:  24%|██▍       | 6/25 [00:04<00:13,  1.41it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.286 | Val 0.277 | ETA 00:37:01Calculating loss...:  28%|██▊       | 7/25 [00:05<00:13,  1.38it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.286 | Val 0.277 | ETA 00:37:01Calculating loss...:  32%|███▏      | 8/25 [00:05<00:11,  1.48it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.286 | Val 0.277 | ETA 00:37:01Calculating loss...:  36%|███▌      | 9/25 [00:06<00:11,  1.43it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.286 | Val 0.277 | ETA 00:37:01Calculating loss...:  40%|████      | 10/25 [00:07<00:10,  1.37it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.286 | Val 0.277 | ETA 00:37:01Calculating loss...:  44%|████▍     | 11/25 [00:08<00:09,  1.43it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.286 | Val 0.277 | ETA 00:37:01Calculating loss...:  48%|████▊     | 12/25 [00:08<00:08,  1.52it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.286 | Val 0.277 | ETA 00:37:01Calculating loss...:  52%|█████▏    | 13/25 [00:09<00:07,  1.59it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.286 | Val 0.277 | ETA 00:37:01Calculating loss...:  56%|█████▌    | 14/25 [00:09<00:06,  1.60it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.286 | Val 0.277 | ETA 00:37:01Calculating loss...:  60%|██████    | 15/25 [00:10<00:06,  1.65it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.286 | Val 0.277 | ETA 00:37:01Calculating loss...:  64%|██████▍   | 16/25 [00:11<00:06,  1.49it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.286 | Val 0.277 | ETA 00:37:01Calculating loss...:  68%|██████▊   | 17/25 [00:11<00:04,  1.62it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.286 | Val 0.277 | ETA 00:37:01Calculating loss...:  72%|███████▏  | 18/25 [00:12<00:05,  1.37it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.286 | Val 0.277 | ETA 00:37:01Calculating loss...:  76%|███████▌  | 19/25 [00:13<00:04,  1.43it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.286 | Val 0.277 | ETA 00:37:01Calculating loss...:  80%|████████  | 20/25 [00:14<00:03,  1.33it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.286 | Val 0.277 | ETA 00:37:01Calculating loss...:  84%|████████▍ | 21/25 [00:15<00:03,  1.27it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.286 | Val 0.277 | ETA 00:37:01Calculating loss...:  88%|████████▊ | 22/25 [00:15<00:02,  1.38it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.286 | Val 0.277 | ETA 00:37:01Calculating loss...:  92%|█████████▏| 23/25 [00:16<00:01,  1.48it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.286 | Val 0.277 | ETA 00:37:01Calculating loss...:  96%|█████████▌| 24/25 [00:16<00:00,  1.47it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.286 | Val 0.277 | ETA 00:37:01Calculating loss...: 100%|██████████| 25/25 [00:17<00:00,  1.56it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.286 | Val 0.277 | ETA 00:37:01Calculating loss...: 100%|██████████| 25/25 [00:17<00:00,  1.44it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.286 | Val 0.277 | ETA 00:37:01Iter 6000: Val loss 0.293, Val took 17.401s\n",
      "Run (85.7% total) | Local 6000/7000 | Global 6000/7000 | LR 2.0e-04 | Train 0.286 | Val 0.293 | ETA 00:34:17Iter 6000: Train loss 0.297, Learning Rate 2.000e-04, It/sec 0.503, Tokens/sec 545.943, Trained Tokens 6458836, Peak mem 3.723 GB\n",
      "Run (85.7% total) | Local 6000/7000 | Global 6000/7000 | LR 2.0e-04 | Train 0.297 | Val 0.293 | ETA 00:34:17Iter 6000: Saved adapter weights to ACLED_llama_fine_tuned/adapters_optimised/adapters.safetensors and ACLED_llama_fine_tuned/adapters_optimised/0006000_adapters.safetensors.\n",
      "Run (85.7% total) | Local 6000/7000 | Global 6000/7000 | LR 2.0e-04 | Train 0.297 | Val 0.293 | ETA 00:34:17Iter 6100: Train loss 0.280, Learning Rate 2.000e-04, It/sec 0.531, Tokens/sec 554.906, Trained Tokens 6563309, Peak mem 3.723 GB\n",
      "Run (87.1% total) | Local 6100/7000 | Global 6100/7000 | LR 2.0e-04 | Train 0.280 | Val 0.293 | ETA 00:30:04Iter 6200: Train loss 0.269, Learning Rate 2.000e-04, It/sec 0.478, Tokens/sec 522.791, Trained Tokens 6672618, Peak mem 3.723 GB\n",
      "Run (88.6% total) | Local 6200/7000 | Global 6200/7000 | LR 2.0e-04 | Train 0.269 | Val 0.293 | ETA 00:27:04Iter 6300: Train loss 0.293, Learning Rate 2.000e-04, It/sec 0.502, Tokens/sec 536.751, Trained Tokens 6779471, Peak mem 3.723 GB\n",
      "Run (90.0% total) | Local 6300/7000 | Global 6300/7000 | LR 2.0e-04 | Train 0.293 | Val 0.293 | ETA 00:23:33Iter 6400: Train loss 0.296, Learning Rate 2.000e-04, It/sec 0.485, Tokens/sec 535.798, Trained Tokens 6889965, Peak mem 3.723 GB\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.296 | Val 0.293 | ETA 00:20:19Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.296 | Val 0.293 | ETA 00:20:19Calculating loss...:   4%|▍         | 1/25 [00:00<00:15,  1.56it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.296 | Val 0.293 | ETA 00:20:19Calculating loss...:   8%|▊         | 2/25 [00:01<00:17,  1.32it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.296 | Val 0.293 | ETA 00:20:19Calculating loss...:  12%|█▏        | 3/25 [00:02<00:16,  1.31it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.296 | Val 0.293 | ETA 00:20:19Calculating loss...:  16%|█▌        | 4/25 [00:03<00:15,  1.32it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.296 | Val 0.293 | ETA 00:20:19Calculating loss...:  20%|██        | 5/25 [00:03<00:15,  1.29it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.296 | Val 0.293 | ETA 00:20:19Calculating loss...:  24%|██▍       | 6/25 [00:04<00:16,  1.18it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.296 | Val 0.293 | ETA 00:20:19Calculating loss...:  28%|██▊       | 7/25 [00:05<00:13,  1.33it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.296 | Val 0.293 | ETA 00:20:19Calculating loss...:  32%|███▏      | 8/25 [00:06<00:13,  1.26it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.296 | Val 0.293 | ETA 00:20:19Calculating loss...:  36%|███▌      | 9/25 [00:07<00:12,  1.27it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.296 | Val 0.293 | ETA 00:20:19Calculating loss...:  40%|████      | 10/25 [00:07<00:10,  1.40it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.296 | Val 0.293 | ETA 00:20:19Calculating loss...:  44%|████▍     | 11/25 [00:08<00:10,  1.39it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.296 | Val 0.293 | ETA 00:20:19Calculating loss...:  48%|████▊     | 12/25 [00:08<00:08,  1.49it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.296 | Val 0.293 | ETA 00:20:19Calculating loss...:  52%|█████▏    | 13/25 [00:09<00:07,  1.57it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.296 | Val 0.293 | ETA 00:20:19Calculating loss...:  56%|█████▌    | 14/25 [00:09<00:06,  1.63it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.296 | Val 0.293 | ETA 00:20:19Calculating loss...:  60%|██████    | 15/25 [00:10<00:06,  1.62it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.296 | Val 0.293 | ETA 00:20:19Calculating loss...:  64%|██████▍   | 16/25 [00:11<00:05,  1.72it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.296 | Val 0.293 | ETA 00:20:19Calculating loss...:  68%|██████▊   | 17/25 [00:11<00:05,  1.49it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.296 | Val 0.293 | ETA 00:20:19Calculating loss...:  72%|███████▏  | 18/25 [00:12<00:04,  1.51it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.296 | Val 0.293 | ETA 00:20:19Calculating loss...:  76%|███████▌  | 19/25 [00:13<00:03,  1.53it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.296 | Val 0.293 | ETA 00:20:19Calculating loss...:  80%|████████  | 20/25 [00:13<00:03,  1.56it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.296 | Val 0.293 | ETA 00:20:19Calculating loss...:  84%|████████▍ | 21/25 [00:14<00:02,  1.62it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.296 | Val 0.293 | ETA 00:20:19Calculating loss...:  88%|████████▊ | 22/25 [00:15<00:02,  1.49it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.296 | Val 0.293 | ETA 00:20:19Calculating loss...:  92%|█████████▏| 23/25 [00:15<00:01,  1.52it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.296 | Val 0.293 | ETA 00:20:19Calculating loss...:  96%|█████████▌| 24/25 [00:16<00:00,  1.59it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.296 | Val 0.293 | ETA 00:20:19Calculating loss...: 100%|██████████| 25/25 [00:17<00:00,  1.59it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.296 | Val 0.293 | ETA 00:20:19Calculating loss...: 100%|██████████| 25/25 [00:17<00:00,  1.47it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.296 | Val 0.293 | ETA 00:20:19Iter 6500: Val loss 0.289, Val took 17.042s\n",
      "Run (92.9% total) | Local 6500/7000 | Global 6500/7000 | LR 2.0e-04 | Train 0.296 | Val 0.289 | ETA 00:17:01Iter 6500: Train loss 0.280, Learning Rate 2.000e-04, It/sec 0.522, Tokens/sec 555.473, Trained Tokens 6996384, Peak mem 3.723 GB\n",
      "Run (92.9% total) | Local 6500/7000 | Global 6500/7000 | LR 2.0e-04 | Train 0.280 | Val 0.289 | ETA 00:17:01Iter 6500: Saved adapter weights to ACLED_llama_fine_tuned/adapters_optimised/adapters.safetensors and ACLED_llama_fine_tuned/adapters_optimised/0006500_adapters.safetensors.\n",
      "Run (92.9% total) | Local 6500/7000 | Global 6500/7000 | LR 2.0e-04 | Train 0.280 | Val 0.289 | ETA 00:17:01Iter 6600: Train loss 0.278, Learning Rate 2.000e-04, It/sec 0.484, Tokens/sec 528.084, Trained Tokens 7105591, Peak mem 3.723 GB\n",
      "Run (94.3% total) | Local 6600/7000 | Global 6600/7000 | LR 2.0e-04 | Train 0.278 | Val 0.289 | ETA 00:13:40Iter 6700: Train loss 0.273, Learning Rate 2.000e-04, It/sec 0.503, Tokens/sec 539.490, Trained Tokens 7212929, Peak mem 3.723 GB\n",
      "Run (95.7% total) | Local 6700/7000 | Global 6700/7000 | LR 2.0e-04 | Train 0.273 | Val 0.289 | ETA 00:10:09Iter 6800: Train loss 0.292, Learning Rate 2.000e-04, It/sec 0.500, Tokens/sec 537.550, Trained Tokens 7320520, Peak mem 3.723 GB\n",
      "Run (97.1% total) | Local 6800/7000 | Global 6800/7000 | LR 2.0e-04 | Train 0.292 | Val 0.289 | ETA 00:06:44Iter 6900: Train loss 0.269, Learning Rate 2.000e-04, It/sec 0.514, Tokens/sec 540.740, Trained Tokens 7425788, Peak mem 3.723 GB\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.269 | Val 0.289 | ETA 00:03:20Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.269 | Val 0.289 | ETA 00:03:20Calculating loss...:   4%|▍         | 1/25 [00:00<00:18,  1.31it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.269 | Val 0.289 | ETA 00:03:20Calculating loss...:   8%|▊         | 2/25 [00:01<00:14,  1.55it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.269 | Val 0.289 | ETA 00:03:20Calculating loss...:  12%|█▏        | 3/25 [00:01<00:13,  1.65it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.269 | Val 0.289 | ETA 00:03:20Calculating loss...:  16%|█▌        | 4/25 [00:02<00:12,  1.70it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.269 | Val 0.289 | ETA 00:03:20Calculating loss...:  20%|██        | 5/25 [00:03<00:11,  1.73it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.269 | Val 0.289 | ETA 00:03:20Calculating loss...:  24%|██▍       | 6/25 [00:03<00:12,  1.47it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.269 | Val 0.289 | ETA 00:03:20Calculating loss...:  28%|██▊       | 7/25 [00:04<00:11,  1.54it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.269 | Val 0.289 | ETA 00:03:20Calculating loss...:  32%|███▏      | 8/25 [00:05<00:12,  1.39it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.269 | Val 0.289 | ETA 00:03:20Calculating loss...:  36%|███▌      | 9/25 [00:06<00:11,  1.34it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.269 | Val 0.289 | ETA 00:03:20Calculating loss...:  40%|████      | 10/25 [00:06<00:10,  1.41it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.269 | Val 0.289 | ETA 00:03:20Calculating loss...:  44%|████▍     | 11/25 [00:07<00:10,  1.35it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.269 | Val 0.289 | ETA 00:03:20Calculating loss...:  48%|████▊     | 12/25 [00:08<00:08,  1.46it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.269 | Val 0.289 | ETA 00:03:20Calculating loss...:  52%|█████▏    | 13/25 [00:08<00:07,  1.55it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.269 | Val 0.289 | ETA 00:03:20Calculating loss...:  56%|█████▌    | 14/25 [00:09<00:06,  1.61it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.269 | Val 0.289 | ETA 00:03:20Calculating loss...:  60%|██████    | 15/25 [00:09<00:05,  1.71it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.269 | Val 0.289 | ETA 00:03:20Calculating loss...:  64%|██████▍   | 16/25 [00:10<00:05,  1.53it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.269 | Val 0.289 | ETA 00:03:20Calculating loss...:  68%|██████▊   | 17/25 [00:11<00:05,  1.43it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.269 | Val 0.289 | ETA 00:03:20Calculating loss...:  72%|███████▏  | 18/25 [00:12<00:04,  1.48it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.269 | Val 0.289 | ETA 00:03:20Calculating loss...:  76%|███████▌  | 19/25 [00:12<00:04,  1.39it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.269 | Val 0.289 | ETA 00:03:20Calculating loss...:  80%|████████  | 20/25 [00:13<00:03,  1.34it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.269 | Val 0.289 | ETA 00:03:20Calculating loss...:  84%|████████▍ | 21/25 [00:14<00:02,  1.41it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.269 | Val 0.289 | ETA 00:03:20Calculating loss...:  88%|████████▊ | 22/25 [00:14<00:01,  1.50it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.269 | Val 0.289 | ETA 00:03:20Calculating loss...:  92%|█████████▏| 23/25 [00:15<00:01,  1.45it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.269 | Val 0.289 | ETA 00:03:20Calculating loss...:  96%|█████████▌| 24/25 [00:16<00:00,  1.54it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.269 | Val 0.289 | ETA 00:03:20Calculating loss...: 100%|██████████| 25/25 [00:16<00:00,  1.60it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.269 | Val 0.289 | ETA 00:03:20Calculating loss...: 100%|██████████| 25/25 [00:16<00:00,  1.50it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.269 | Val 0.289 | ETA 00:03:20Iter 7000: Val loss 0.262, Val took 16.693s\n",
      "Run (100.0% total) | Local 7000/7000 | Global 7000/7000 | LR 2.0e-04 | Train 0.269 | Val 0.262 | ETA 00:00:00Iter 7000: Train loss 0.268, Learning Rate 2.000e-04, It/sec 0.506, Tokens/sec 547.111, Trained Tokens 7533993, Peak mem 3.723 GB\n",
      "Run (100.0% total) | Local 7000/7000 | Global 7000/7000 | LR 2.0e-04 | Train 0.268 | Val 0.262 | ETA 00:00:00Iter 7000: Saved adapter weights to ACLED_llama_fine_tuned/adapters_optimised/adapters.safetensors and ACLED_llama_fine_tuned/adapters_optimised/0007000_adapters.safetensors.\n",
      "Run (100.0% total) | Local 7000/7000 | Global 7000/7000 | LR 2.0e-04 | Train 0.268 | Val 0.262 | ETA 00:00:00Saved final weights to ACLED_llama_fine_tuned/adapters_optimised/adapters.safetensors.\n",
      "Run (100.0% total) | Local 7000/7000 | Global 7000/7000 | LR 2.0e-04 | Train 0.268 | Val 0.262 | ETA 00:00:00\n",
      "Stage 1 completed in 03:57:08\n",
      "Latest detected checkpoint step: 7000\n",
      "\n",
      "============================================================\n",
      "ALL STAGES ATTEMPTED\n",
      "Total training time: 03:57:08\n",
      "Final observed train loss: 0.268\n",
      "Final observed val loss: 0.262\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def run_staged_training():\n",
    "\n",
    "    def fmt_eta(td: timedelta) -> str:\n",
    "        s = int(max(td.total_seconds(), 0))\n",
    "        return f\"{s//3600:02d}:{(s%3600)//60:02d}:{s%60:02d}\"\n",
    "\n",
    "    def detect_last_step(adapters_path: Path) -> int:\n",
    "        if not adapters_path.exists():\n",
    "            return 0\n",
    "        pat = re.compile(r\"(\\d+)_adapters\\.safetensors$\")\n",
    "        last = 0\n",
    "        for p in adapters_path.glob(\"*_adapters.safetensors\"):\n",
    "            m = pat.search(p.name)\n",
    "            if m:\n",
    "                with contextlib.suppress(ValueError):\n",
    "                    last = max(last, int(m.group(1)))\n",
    "        return last\n",
    "\n",
    "    def ensure_adapters_for_resume(adapters_path: Path, last: int) -> None:\n",
    "        # Keep adapters.safetensors in sync with latest step-indexed file\n",
    "        if last <= 0:\n",
    "            return\n",
    "        latest_ckpt = adapters_path / f\"{last:07d}_adapters.safetensors\"\n",
    "        promoted = adapters_path / \"adapters.safetensors\"\n",
    "        try:\n",
    "            if (not promoted.exists()) or (promoted.stat().st_size != latest_ckpt.stat().st_size):\n",
    "                tmp = promoted.with_suffix(\".tmp\")\n",
    "                with open(latest_ckpt, \"rb\") as src, open(tmp, \"wb\") as dst:\n",
    "                    while True:\n",
    "                        b = src.read(1024 * 1024)\n",
    "                        if not b:\n",
    "                            break\n",
    "                        dst.write(b)\n",
    "                os.replace(tmp, promoted)\n",
    "                print(f\"Promoted {latest_ckpt.name} -> {promoted.name}\")\n",
    "            else:\n",
    "                print(f\"Resume file present: {promoted.name}\")\n",
    "        except Exception as e:\n",
    "            with contextlib.suppress(Exception):\n",
    "                t = promoted.with_suffix(\".tmp\")\n",
    "                if t.exists():\n",
    "                    t.unlink()\n",
    "            print(f\"WARNING: resume prep failed: {e}\")\n",
    "\n",
    "    # Log parsers\n",
    "    step_pattern       = re.compile(r'Iter\\s+(\\d+):')\n",
    "    train_loss_pattern = re.compile(r'Train loss\\s+([+\\-]?\\d+(?:\\.\\d+)?(?:[eE][+\\-]?\\d+)?)')\n",
    "    val_loss_pattern   = re.compile(r'Val loss\\s+([+\\-]?\\d+(?:\\.\\d+)?(?:[eE][+\\-]?\\d+)?)')\n",
    "\n",
    "    print(\"Starting training (constant LR)\")\n",
    "    print(f\"  target iters = {int(config.ITERATIONS)} | lr = {float(config.LEARNING_RATE)} | batch = {int(config.BATCH_SIZE)}\")\n",
    "\n",
    "    overall_start_time = datetime.now()\n",
    "    last_train_loss = None\n",
    "    last_val_loss = None\n",
    "\n",
    "    # Resume point\n",
    "    last_done = detect_last_step(Path(adapters_dir))\n",
    "    if last_done > 0:\n",
    "        print(f\"Detected checkpoints up to {last_done}. Resuming…\")\n",
    "        ensure_adapters_for_resume(Path(adapters_dir), last_done)\n",
    "    else:\n",
    "        print(\"No checkpoints found. Starting at 0.\")\n",
    "\n",
    "    start_for_stage = max(last_done, 0)\n",
    "    iters_to_run = int(config.ITERATIONS) - start_for_stage\n",
    "    if iters_to_run <= 0:\n",
    "        print(\"Nothing to do (already at or beyond target iterations).\")\n",
    "        return {\n",
    "            \"final_step\": last_done,\n",
    "            \"final_train_loss\": last_train_loss,\n",
    "            \"final_val_loss\": last_val_loss,\n",
    "            \"total_duration\": datetime.now() - overall_start_time,\n",
    "        }\n",
    "\n",
    "    # Commands\n",
    "    cmd = [\n",
    "        \"python\", \"-m\", \"mlx_lm\", \"lora\",\n",
    "        \"--model\", str(config.MODEL_PATH),\n",
    "        \"--train\",\n",
    "        \"--data\", str(data_dir),\n",
    "        \"--batch-size\", str(config.BATCH_SIZE),\n",
    "        \"--iters\", str(iters_to_run),\n",
    "        \"--learning-rate\", str(float(config.LEARNING_RATE)),\n",
    "        \"--steps-per-report\", \"100\",\n",
    "        \"--steps-per-eval\", str(int(config.EVAL_FREQUENCY)),\n",
    "        \"--save-every\", str(int(config.EVAL_FREQUENCY)),\n",
    "        \"--adapter-path\", str(adapters_dir),\n",
    "        \"--max-seq-length\", str(int(config.MAX_SEQ_LEN)),\n",
    "        \"-c\", str(lora_yaml_path),\n",
    "        \"--grad-checkpoint\",\n",
    "        \"--seed\", str(int(config.SEED)),\n",
    "    ]\n",
    "    print(\"Command:\", \" \".join(cmd))\n",
    "\n",
    "    # ETA state: estimate from deltas between Iter lines (skips cold-start cost)\n",
    "    stage_start_time = datetime.now()\n",
    "    print(f\"Progress: 0.0% | Local: 0/{iters_to_run} | Global: {start_for_stage}->{int(config.ITERATIONS)} | ETA: warming up…\")\n",
    "\n",
    "    process = subprocess.Popen(\n",
    "        cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
    "        text=True, bufsize=1, universal_newlines=True\n",
    "    )\n",
    "\n",
    "    local_step = 0\n",
    "    last_iter_seen = None\n",
    "    last_time_seen = None\n",
    "    sec_per_iter = None  # EWMA\n",
    "\n",
    "    try:\n",
    "        for raw_line in process.stdout:\n",
    "            line = raw_line.rstrip()\n",
    "            if not line:\n",
    "                continue\n",
    "            print(line)\n",
    "\n",
    "            now = datetime.now()\n",
    "\n",
    "            m = step_pattern.search(line)\n",
    "            if m:\n",
    "                with contextlib.suppress(ValueError):\n",
    "                    step_now = int(m.group(1))\n",
    "                    # Update timing only when iter increases\n",
    "                    if last_iter_seen is not None and step_now > last_iter_seen:\n",
    "                        d_iter = step_now - last_iter_seen\n",
    "                        d_time = (now - last_time_seen).total_seconds()\n",
    "                        if d_iter > 0 and d_time > 0:\n",
    "                            inst_spi = d_time / d_iter\n",
    "                            sec_per_iter = inst_spi if sec_per_iter is None else (0.7 * sec_per_iter + 0.3 * inst_spi)\n",
    "                    last_iter_seen = step_now\n",
    "                    last_time_seen = now\n",
    "                    local_step = step_now\n",
    "\n",
    "            m = train_loss_pattern.search(line)\n",
    "            if m:\n",
    "                with contextlib.suppress(ValueError):\n",
    "                    last_train_loss = float(m.group(1))\n",
    "\n",
    "            m = val_loss_pattern.search(line)\n",
    "            if m:\n",
    "                with contextlib.suppress(ValueError):\n",
    "                    last_val_loss = float(m.group(1))\n",
    "\n",
    "            # Progress line with ETA once we have a decent estimate\n",
    "            if local_step > 0:\n",
    "                remaining = max(iters_to_run - local_step, 0)\n",
    "                if sec_per_iter is not None and local_step >= 10:\n",
    "                    eta_td = timedelta(seconds=int(remaining * sec_per_iter))\n",
    "                    eta_txt = fmt_eta(eta_td)\n",
    "                else:\n",
    "                    eta_txt = \"estimating…\"\n",
    "                global_step_est = start_for_stage + local_step\n",
    "                pct_total = 100.0 * (global_step_est / float(config.ITERATIONS))\n",
    "                parts = [\n",
    "                    f\"\\rRun ({pct_total:.1f}% total)\",\n",
    "                    f\"Local {local_step}/{iters_to_run}\",\n",
    "                    f\"Global {global_step_est}/{int(config.ITERATIONS)}\",\n",
    "                    f\"LR {float(config.LEARNING_RATE):.1e}\",\n",
    "                ]\n",
    "                if last_train_loss is not None:\n",
    "                    parts.append(f\"Train {last_train_loss:.3f}\")\n",
    "                if last_val_loss is not None:\n",
    "                    parts.append(f\"Val {last_val_loss:.3f}\")\n",
    "                parts.append(f\"ETA {eta_txt}\")\n",
    "                print(\" | \".join(parts), end=\"\", flush=True)\n",
    "\n",
    "        process.wait()\n",
    "    except KeyboardInterrupt:\n",
    "        process.terminate()\n",
    "        print(\"\\nInterrupted by user.\")\n",
    "\n",
    "    duration = datetime.now() - stage_start_time\n",
    "    print(f\"\\nStage 1 completed in {fmt_eta(duration)}\")\n",
    "    if process.returncode != 0:\n",
    "        print(f\"Process failed with return code: {process.returncode}\")\n",
    "\n",
    "    # Refresh last_done after run\n",
    "    last_done = detect_last_step(Path(adapters_dir))\n",
    "    print(f\"Latest detected checkpoint step: {last_done}\")\n",
    "\n",
    "    total_duration = datetime.now() - overall_start_time\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ALL STAGES ATTEMPTED\")\n",
    "    print(f\"Total training time: {fmt_eta(total_duration)}\")\n",
    "    print(f\"Final observed train loss: {last_train_loss:.3f}\" if last_train_loss is not None else \"No final train loss observed\")\n",
    "    print(f\"Final observed val loss: {last_val_loss:.3f}\" if last_val_loss is not None else \"No final val loss observed\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    return {\n",
    "        \"final_step\": last_done,\n",
    "        \"final_train_loss\": last_train_loss,\n",
    "        \"final_val_loss\": last_val_loss,\n",
    "        \"total_duration\": total_duration,\n",
    "    }\n",
    "\n",
    "result = run_staged_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Checkpoint Promotion and Training Metadata Export\n",
    "\n",
    "This step finalises the training session by promoting the last adapter checkpoint to a standard filename and exporting a `training_info.json` manifest for reproducibility.\n",
    "\n",
    "#### Checkpoint Promotion\n",
    "\n",
    "- Searches for adapter files matching the pattern `*_adapters.safetensors` within the output directory.\n",
    "- Chooses the latest checkpoint based on:\n",
    "  - Step number, if present in filename (preferred), or\n",
    "  - File modification time (fallback)\n",
    "- Promotes the selected file to `adapters.safetensors` using an atomic `os.replace` operation to prevent corruption.\n",
    "\n",
    "If no checkpoint is found, a warning is printed and promotion is skipped.\n",
    "\n",
    "#### Training Manifest\n",
    "\n",
    "A final metadata summary is compiled using all available runtime information, including:\n",
    "\n",
    "- **Model and adapter paths**\n",
    "- **Training and validation set sizes**\n",
    "- **Training duration**\n",
    "- **Final observed training and validation losses**\n",
    "- **Start and end timestamps**\n",
    "- **LoRA and training hyperparameters** (e.g., batch size, learning rate, seed)\n",
    "\n",
    "This manifest ensures the run is fully reproducible and can be analysed later without relying on logs or CLI history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promoted 0007000_adapters.safetensors -> adapters.safetensors\n",
      "Metadata saved: ACLED_llama_fine_tuned/adapters_optimised/training_info.json\n"
     ]
    }
   ],
   "source": [
    "adapters_dir = Path(adapters_dir)\n",
    "\n",
    "# Pick latest checkpoint (prefer highest step; fallback to newest modified timestamp)\n",
    "ckpts = list(adapters_dir.glob(\"*_adapters.safetensors\"))\n",
    "step_re = re.compile(r\"^(\\d+)_adapters\\.safetensors$\")\n",
    "def step_num(p: Path) -> int:\n",
    "    m = step_re.match(p.name)\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "latest_ckpt = None\n",
    "if ckpts:\n",
    "    by_step = [p for p in ckpts if step_re.match(p.name)]\n",
    "    latest_ckpt = max(by_step, key=step_num) if by_step else max(ckpts, key=lambda p: p.stat().st_mtime)\n",
    "\n",
    "# Promote to adapters.safetensors (atomic replace = deletes old file)\n",
    "if latest_ckpt:\n",
    "    dst = adapters_dir / \"adapters.safetensors\"\n",
    "    tmp = dst.with_suffix(\".tmp\")\n",
    "    try:\n",
    "        with open(latest_ckpt, \"rb\") as src, open(tmp, \"wb\") as out:\n",
    "            while True:\n",
    "                b = src.read(1024 * 1024)  # 1 MiB chunks\n",
    "                if not b:\n",
    "                    break\n",
    "                out.write(b)\n",
    "        os.replace(tmp, dst)\n",
    "        print(f\"Promoted {latest_ckpt.name} -> {dst.name}\")\n",
    "    except Exception as e:\n",
    "        with contextlib.suppress(Exception):\n",
    "            if tmp.exists():\n",
    "                tmp.unlink()\n",
    "        print(f\"Promotion failed: {e}\")\n",
    "else:\n",
    "    print(\"No checkpoints found to promote.\")\n",
    "\n",
    "# Build a concise manifest (best-effort pulls from `result`/`config` if available)\n",
    "end_time   = datetime.now()\n",
    "_has_result = ('result' in globals()) and isinstance(result, dict) and bool(result)\n",
    "duration   = result.get('total_duration') if _has_result else timedelta(0)\n",
    "final_step = int(result.get('final_step', 0)) if _has_result else 0\n",
    "train_loss = result.get('final_train_loss') if _has_result else None\n",
    "val_loss   = result.get('final_val_loss')   if _has_result else None\n",
    "\n",
    "try:\n",
    "    training_samples   = len(train_data)\n",
    "except Exception:\n",
    "    training_samples   = None\n",
    "try:\n",
    "    validation_samples = len(valid_data)\n",
    "except Exception:\n",
    "    validation_samples = None\n",
    "\n",
    "# pull config if present\n",
    "try:\n",
    "    model_path = str(config.MODEL_PATH)\n",
    "    batch_size = int(getattr(config, \"BATCH_SIZE\", 0))\n",
    "    max_len    = int(getattr(config, \"MAX_SEQ_LEN\", 0))\n",
    "    seed_val   = int(getattr(config, \"SEED\", 0))\n",
    "    lr_val     = float(getattr(config, \"LEARNING_RATE\", 0.0))\n",
    "    iterations = int(getattr(config, \"ITERATIONS\", 0))\n",
    "except Exception:\n",
    "    model_path = batch_size = max_len = seed_val = iterations = None\n",
    "    lr_val = None\n",
    "\n",
    "try:\n",
    "    started_at = overall_start_time.isoformat(timespec=\"seconds\")\n",
    "except Exception:\n",
    "    started_at = end_time.isoformat(timespec=\"seconds\")\n",
    "\n",
    "run_manifest = {\n",
    "    \"version\": \"training_v3\",\n",
    "    \"adapter_dir\": str(adapters_dir),\n",
    "    \"latest_checkpoint\": latest_ckpt.name if latest_ckpt else None,\n",
    "    \"model_path\": model_path,\n",
    "    \"training_samples\": training_samples,\n",
    "    \"validation_samples\": validation_samples,\n",
    "    \"duration\": str(duration),\n",
    "    \"final_step\": final_step,\n",
    "    \"train_loss\": train_loss,\n",
    "    \"val_loss\": val_loss,\n",
    "    \"started_at\": started_at,\n",
    "    \"ended_at\": end_time.isoformat(timespec=\"seconds\"),\n",
    "    \"hyperparameters\": {\n",
    "        \"schedule\": f\"{lr_val} (constant)\" if lr_val else \"constant\",\n",
    "        \"learning_rate\": lr_val,\n",
    "        \"iterations\": iterations,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"max_seq_length\": max_len,\n",
    "        \"lora_r\": 16,\n",
    "        \"lora_alpha\": 32,\n",
    "        \"lora_dropout\": 0.1,\n",
    "        \"grad_checkpoint\": True,\n",
    "        \"seed\": seed_val,\n",
    "    },\n",
    "}\n",
    "\n",
    "manifest_path = adapters_dir / \"training_info.json\"\n",
    "with open(manifest_path, \"w\") as f:\n",
    "    json.dump(run_manifest, f, indent=2)\n",
    "\n",
    "print(f\"Metadata saved: {manifest_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check: Inference with Fine-Tuned Model\n",
    "\n",
    "Before running the full evaluation, a quick sanity check ensures that the fine-tuned model loads correctly and produces a valid structured output.\n",
    "\n",
    "- The model and adapter are loaded using the `load()` function.\n",
    "- A single prompt describing a fictional conflict event is passed to the model using `generate_safe()`.\n",
    "\n",
    "The response is printed for manual inspection to verify that key fields, such as:\n",
    "\n",
    "- `event_date`  \n",
    "- `location`  \n",
    "- `actor_1`  \n",
    "- `fatalities`\n",
    "\n",
    "—are present and correctly formatted in valid JSON.\n",
    "\n",
    "This step helps catch adapter loading issues or output formatting errors early, before launching the full test suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned model (with promoted adapters) for a quick check\n",
      "\n",
      "Model output:\n",
      "\n",
      "{\"event_date\": \"15 March 2025\", \"country\": \"ukraine\", \"location\": [\"Kharkiv\"], \"event_type\": \"remote_violence\", \"actor_1\": \"Russian military forces\", \"actor_2\": \"unknown\", \"fatalities\": \"unknown\", \"civilian_casualties\": 3, \"property_damage\": \"yes - hospitals\", \"weapons_mentioned\": \"unknown\", \"casualty_type\": \"injured\", \"attack_method\": \"airstrike\", \"disorder_type\": \"interstate_conflict\", \"infrastructure_disruption\": [\"medical_services\"]}\n"
     ]
    }
   ],
   "source": [
    "def generate_safe(model, tok, prompt, max_tokens=512, temperature=0.05):\n",
    "    sig = inspect.signature(generate)\n",
    "    kwargs = {\"model\": model, \"tokenizer\": tok, \"prompt\": prompt}\n",
    "    if \"max_tokens\" in sig.parameters: kwargs[\"max_tokens\"] = max_tokens\n",
    "    if \"max_new_tokens\" in sig.parameters: kwargs[\"max_new_tokens\"] = max_tokens\n",
    "    if \"temperature\" in sig.parameters: kwargs[\"temperature\"] = temperature\n",
    "    return generate(**kwargs)\n",
    "\n",
    "try:\n",
    "    print(\"Loading fine-tuned model (with promoted adapters) for a quick check\")\n",
    "    model_v, tok_v = load(config.MODEL_PATH, adapter_path=str(adapters_dir))\n",
    "\n",
    "    prompt = (\n",
    "    \"[INST] Extract relevant information from this conflict event report:\\n\\n\"\n",
    "    \"On 15 March 2025, Russian forces conducted an airstrike on a hospital in Kharkiv, Ukraine. \"\n",
    "    \"3 civilians were wounded. The medical building was destroyed. [/INST]\"\n",
    ")\n",
    "    out = generate_safe(model_v, tok_v, prompt=prompt, max_tokens=512, temperature=0.05)\n",
    "    print(\"\\nModel output:\\n\")\n",
    "    print(out.strip())\n",
    "except Exception as e:\n",
    "    print(f\"Sanity check failed: {e}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation for structured field extraction\n",
    " \n",
    "This cell serves as a compact audit of the field extraction Llama model on ACLED event notes. It prompts the model for a JSON response, validates and scores the output against a fixed set of fields, prints a clear report, and returns a structured results dictionary.\n",
    "\n",
    "**Data**   \n",
    "* Sample up to `num_samples` rows with a fixed seed.  \n",
    "* Use the `notes` column as input to `generate_safe(model, tokenizer, ...)`.\n",
    "\n",
    "**Target fields**  \n",
    "`event_date`, `country`, `location`, `event_type`, `actor_1`, `actor_2`, `fatalities`, `civilian_casualties`, `casualty_type`, `weapons_mentioned`, `attack_method`, `property_damage`, `disorder_type`, `infrastructure_disruption`.\n",
    "\n",
    "**Validity and scoring**  \n",
    "* Locate a JSON object in the model output and parse it.  \n",
    "* Count a field as correct only when the value is meaningful.  \n",
    "  * Strings must be non empty and not placeholders such as unknown or n slash a.  \n",
    "  * Lists must be non empty with at least one meaningful element.  \n",
    "  * Numbers must be positive, except that zero is accepted for fatalities and civilian casualties.\n",
    "\n",
    "**Report contents**  \n",
    "* Rate of valid JSON responses and average field completion.  \n",
    "* Field and category breakdowns with status labels PASS, CAUTION, FAIL.  \n",
    "* One success example and one error example when available.  \n",
    "* A multi criteria assessment that produces an overall verdict.\n",
    "\n",
    "**Criteria**  \n",
    "* Valid JSON at least eighty percent.  \n",
    "* Average field completion at least sixty percent.  \n",
    "* Core Info category at least 70%.  \n",
    "* Error rate at most thirty percent.  \n",
    "\n",
    "The verdict is PASS when at least three criteria are met, PARTIAL when exactly two are met, otherwise FAIL.\n",
    "\n",
    "**Returned data**  \n",
    "A dictionary containing the verdict, valid_json_rate, avg_field_success, category scores, per field scores, counts of criteria met, and up to three success and three error examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model Evaluation\n",
      "Loaded 400 test samples\n",
      "\n",
      "Testing model on 400 samples\n",
      "\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "Valid JSON responses: 400/400 (100.0%)\n",
      "Average field completion: 62.7%\n",
      "\n",
      "Field Extraction Success Rates:\n",
      "Field                     Success    Rate     Status\n",
      "-------------------------------------------------------\n",
      "\n",
      "Core Info:\n",
      "  event_date              400/400    100.0%   PASS\n",
      "  country                 400/400    100.0%   PASS\n",
      "  location                391/400     97.8%   PASS\n",
      "  event_type              400/400    100.0%   PASS\n",
      "\n",
      "Actors:\n",
      "  actor_1                 387/400     96.8%   PASS\n",
      "  actor_2                  61/400     15.2%   FAIL\n",
      "\n",
      "Casualties:\n",
      "  fatalities               57/400     14.2%   FAIL\n",
      "  civilian_casualties      14/400      3.5%   FAIL\n",
      "  casualty_type            90/400     22.5%   FAIL\n",
      "\n",
      "Weapons/Methods:\n",
      "  weapons_mentioned       187/400     46.8%   CAUTION\n",
      "  attack_method           314/400     78.5%   PASS\n",
      "\n",
      "Impact:\n",
      "  property_damage         400/400    100.0%   PASS\n",
      "  disorder_type           380/400     95.0%   PASS\n",
      "  infrastructure_disruption  33/400      8.2%   FAIL\n",
      "\n",
      "Category Performance:\n",
      "  Core Info             99.4% PASS\n",
      "  Actors                56.0% CAUTION\n",
      "  Casualties            13.4% FAIL\n",
      "  Weapons/Methods       62.6% CAUTION\n",
      "  Impact                67.8% CAUTION\n",
      "\n",
      "Success Example (found 11/14 fields):\n",
      "   Text: On 10 November 2024, Russian forces shelled Ukrainian positions near Kopanky, Kharkiv. According to ...\n",
      "   Fields: ['event_date', 'country', 'location', 'event_type', 'actor_1', 'actor_2', 'fatalities', 'casualty_type', 'attack_method', 'property_damage', 'disorder_type']\n",
      "\n",
      "Error Example:\n",
      "   Text: On 15 December 2024, Russian forces clashed with Ukrainian forces near Verkhnokamianske, Donetsk. Ca...\n",
      "   Found: 9/14 fields\n",
      "\n",
      "OVERALL ASSESSMENT:\n",
      "  JSON Format: PASS (≥80% valid responses)\n",
      "  Field Extraction: PASS (≥60% average)\n",
      "  Core Information: PASS (≥70% for basic fields)\n",
      "  Error Rate: FAIL (78.5% > 30%)\n",
      "\n",
      "Criteria met: 3/4\n",
      " MODEL EVALUATION: PASS - Ready for deployment!\n",
      "\n",
      "Results saved to: ACLED_llama_fine_tuned/adapters_optimised/evaluation_results.json\n"
     ]
    }
   ],
   "source": [
    "def model_evaluation(model, tokenizer, num_samples=400):\n",
    "    \"\"\"Evaluation pipeline that helps test field extraction with detailed analysis.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        test_df = pd.read_csv(\"ACLED_data_export/ACLED_test_dataset.csv\")\n",
    "        print(f\"Loaded {len(test_df)} test samples\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Test dataset not found at ACLED_data_export/ACLED_test_dataset.csv\")\n",
    "        return None\n",
    "    \n",
    "    test_sample = test_df.sample(n=min(num_samples, len(test_df)), random_state=42)\n",
    "    \n",
    "    # Target fields: the model is expected to extract\n",
    "    target_fields = [\n",
    "        'event_date', 'country', 'location', 'event_type', 'actor_1', 'actor_2',\n",
    "        'fatalities', 'civilian_casualties', 'casualty_type', 'weapons_mentioned',\n",
    "        'attack_method', 'property_damage', 'disorder_type', 'infrastructure_disruption'\n",
    "    ]\n",
    "    \n",
    "    # Track detailed results\n",
    "    field_success = defaultdict(int)\n",
    "    field_total = defaultdict(int)\n",
    "    valid_json_count = 0\n",
    "    total_tests = len(test_sample)\n",
    "    error_examples = []\n",
    "    success_examples = []\n",
    "    \n",
    "    print(f\"\\nTesting model on {total_tests} samples\")\n",
    "    \n",
    "    for idx, row in test_sample.iterrows():\n",
    "        text = row['notes']\n",
    "        country = row.get('country', 'unknown')\n",
    "        \n",
    "        # Utilise the same prompt\n",
    "        prompt = f\"[INST] Extract relevant information from this conflict event report:\\n\\n{text} [/INST]\"\n",
    "        \n",
    "        try:\n",
    "            response = generate_safe(model, tokenizer, prompt, max_tokens=512, temperature=0.05)\n",
    "            \n",
    "            # Try to parse JSON\n",
    "            json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "            if json_match:\n",
    "                try:\n",
    "                    extracted = json.loads(json_match.group())\n",
    "                    valid_json_count += 1\n",
    "                    \n",
    "                    # Track field completeness\n",
    "                    fields_found = 0\n",
    "                    field_details = {}\n",
    "                    \n",
    "                    for field in target_fields:\n",
    "                        field_total[field] += 1\n",
    "                        \n",
    "                        if field in extracted:\n",
    "                            value = extracted[field]\n",
    "                            # More nuanced success criteria\n",
    "                            if is_meaningful_value(value, field):\n",
    "                                field_success[field] += 1\n",
    "                                fields_found += 1\n",
    "                                field_details[field] = value\n",
    "                    \n",
    "                    # Store example\n",
    "                    example = {\n",
    "                        'text_preview': text[:100] + \"...\",\n",
    "                        'country': country,\n",
    "                        'fields_found': fields_found,\n",
    "                        'total_fields': len(target_fields),\n",
    "                        'completion_rate': fields_found / len(target_fields),\n",
    "                        'extracted_fields': field_details\n",
    "                    }\n",
    "                    \n",
    "                    if fields_found >= len(target_fields) * 0.7:\n",
    "                        success_examples.append(example)\n",
    "                    else:\n",
    "                        error_examples.append(example)\n",
    "                        \n",
    "                except json.JSONDecodeError as e:\n",
    "                    error_examples.append({\n",
    "                        'text_preview': text[:100] + \"...\",\n",
    "                        'error': f\"JSON parse error: {str(e)[:50]}\",\n",
    "                        'response_preview': response[:100] + \"...\"\n",
    "                    })\n",
    "            else:\n",
    "                error_examples.append({\n",
    "                    'text_preview': text[:100] + \"...\",\n",
    "                    'error': \"No JSON found in response\",\n",
    "                    'response_preview': response[:100] + \"...\"\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_examples.append({\n",
    "                'text_preview': text[:100] + \"...\",\n",
    "                'error': f\"Generation error: {str(e)}\"\n",
    "            })\n",
    "    \n",
    "    # The official results\n",
    "    json_success_rate = (valid_json_count / total_tests) * 100\n",
    "    avg_field_success = sum(field_success.values()) / (len(target_fields) * total_tests) * 100\n",
    "    \n",
    "    print(f\"\\nEVALUATION RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Valid JSON responses: {valid_json_count}/{total_tests} ({json_success_rate:.1f}%)\")\n",
    "    print(f\"Average field completion: {avg_field_success:.1f}%\")\n",
    "    \n",
    "    # Field-by-field breakdown with categories\n",
    "    print(f\"\\nField Extraction Success Rates:\")\n",
    "    print(f\"{'Field':<25} {'Success':<10} {'Rate':<8} {'Status'}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    # Group fields by category for better analysis\n",
    "    field_categories = {\n",
    "        'Core Info': ['event_date', 'country', 'location', 'event_type'],\n",
    "        'Actors': ['actor_1', 'actor_2'],\n",
    "        'Casualties': ['fatalities', 'civilian_casualties', 'casualty_type'],\n",
    "        'Weapons/Methods': ['weapons_mentioned', 'attack_method'],\n",
    "        'Impact': ['property_damage', 'disorder_type', 'infrastructure_disruption']\n",
    "    }\n",
    "    \n",
    "    category_scores = {}\n",
    "    \n",
    "    for category, fields in field_categories.items():\n",
    "        category_success = sum(field_success[field] for field in fields)\n",
    "        category_total = sum(field_total[field] for field in fields)\n",
    "        category_rate = (category_success / category_total * 100) if category_total > 0 else 0\n",
    "        category_scores[category] = category_rate\n",
    "        \n",
    "        print(f\"\\n{category}:\")\n",
    "        for field in fields:\n",
    "            success_rate = (field_success[field] / field_total[field] * 100) if field_total[field] > 0 else 0\n",
    "            status = \"PASS\" if success_rate >= 70 else \"CAUTION\" if success_rate >= 40 else \"FAIL\"\n",
    "            print(f\"  {field:<23} {field_success[field]:>3}/{field_total[field]:<3}    {success_rate:>5.1f}%   {status}\")\n",
    "    \n",
    "    print(f\"\\nCategory Performance:\")\n",
    "    for category, score in category_scores.items():\n",
    "        status = \"PASS\" if score >= 70 else \"CAUTION\" if score >= 40 else \"FAIL\"\n",
    "        print(f\"  {category:<20} {score:>5.1f}% {status}\")\n",
    "    \n",
    "    if success_examples:\n",
    "        print(f\"\\nSuccess Example (found {success_examples[0]['fields_found']}/{success_examples[0]['total_fields']} fields):\")\n",
    "        print(f\"   Text: {success_examples[0]['text_preview']}\")\n",
    "        print(f\"   Fields: {list(success_examples[0]['extracted_fields'].keys())}\")\n",
    "    \n",
    "    if error_examples:\n",
    "        print(f\"\\nError Example:\")\n",
    "        print(f\"   Text: {error_examples[0]['text_preview']}\")\n",
    "        if 'error' in error_examples[0]:\n",
    "            print(f\"   Issue: {error_examples[0]['error']}\")\n",
    "        if 'fields_found' in error_examples[0]:\n",
    "            print(f\"   Found: {error_examples[0]['fields_found']}/{error_examples[0]['total_fields']} fields\")\n",
    "    \n",
    "    print(f\"\\nOVERALL ASSESSMENT:\")\n",
    "    \n",
    "    # Multi-criteria evaluation\n",
    "    criteria_met = 0\n",
    "    total_criteria = 4\n",
    "    \n",
    "    if json_success_rate >= 80:\n",
    "        print(\"  JSON Format: PASS (≥80% valid responses)\")\n",
    "        criteria_met += 1\n",
    "    else:\n",
    "        print(f\"  JSON Format: FAIL ({json_success_rate:.1f}% < 80%)\")\n",
    "    \n",
    "    if avg_field_success >= 60:\n",
    "        print(\"  Field Extraction: PASS (≥60% average)\")\n",
    "        criteria_met += 1\n",
    "    else:\n",
    "        print(f\"  Field Extraction: FAIL ({avg_field_success:.1f}% < 60%)\")\n",
    "    \n",
    "    core_performance = category_scores.get('Core Info', 0)\n",
    "    if core_performance >= 70:\n",
    "        print(\"  Core Information: PASS (≥70% for basic fields)\")\n",
    "        criteria_met += 1\n",
    "    else:\n",
    "        print(f\"  Core Information: FAIL ({core_performance:.1f}% < 70%)\")\n",
    "    \n",
    "    if len(error_examples) <= total_tests * 0.3:\n",
    "        print(\"  Error Rate: PASS (≤30% failures)\")\n",
    "        criteria_met += 1\n",
    "    else:\n",
    "        print(f\"  Error Rate: FAIL ({len(error_examples)/total_tests*100:.1f}% > 30%)\")\n",
    "    \n",
    "    print(f\"\\nCriteria met: {criteria_met}/{total_criteria}\")\n",
    "    \n",
    "    if criteria_met >= 3:\n",
    "        print(\" MODEL EVALUATION: PASS - Ready for deployment!\")\n",
    "        verdict = \"PASS\"\n",
    "    elif criteria_met >= 2:\n",
    "        print(\" MODEL EVALUATION: PARTIAL - Functional but needs improvement\")\n",
    "        verdict = \"PARTIAL\"\n",
    "    else:\n",
    "        print(\" MODEL EVALUATION: FAIL - Requires significant improvement\")\n",
    "        verdict = \"FAIL\"\n",
    "    \n",
    "    return {\n",
    "        'verdict': verdict,\n",
    "        'valid_json_rate': json_success_rate / 100,\n",
    "        'avg_field_success': avg_field_success / 100,\n",
    "        'category_scores': {k: v/100 for k, v in category_scores.items()},\n",
    "        'field_success_rates': {field: field_success[field] / field_total[field] for field in target_fields if field_total[field] > 0},\n",
    "        'criteria_met': criteria_met,\n",
    "        'total_criteria': total_criteria,\n",
    "        'examples': {\n",
    "            'success': success_examples[:3],\n",
    "            'errors': error_examples[:3]\n",
    "        }\n",
    "    }\n",
    "\n",
    "def is_meaningful_value(value, field_name):\n",
    "    \"\"\"Check if a field value is meaningful (not empty/unknown)\"\"\"\n",
    "    if value is None:\n",
    "        return False\n",
    "    \n",
    "    # Handle different data types\n",
    "    if isinstance(value, str):\n",
    "        if value.lower().strip() in ['', 'unknown', 'n/a', 'none', 'null']:\n",
    "            return False\n",
    "        return len(value.strip()) > 0\n",
    "    \n",
    "    elif isinstance(value, list):\n",
    "        return len(value) > 0 and any(is_meaningful_value(item, field_name) for item in value)\n",
    "    \n",
    "    elif isinstance(value, (int, float)):\n",
    "        # For numeric fields, 0 might be meaningful\n",
    "        if field_name in ['fatalities', 'civilian_casualties']:\n",
    "            return True  # 0 casualties is meaningful information\n",
    "        return value > 0\n",
    "    \n",
    "    else:\n",
    "        return True  # Other types assumed meaningful if present\n",
    "\n",
    "print(\"Running Model Evaluation\")\n",
    "results = model_evaluation(model_v, tok_v, num_samples=400)\n",
    "\n",
    "if results:\n",
    "    \n",
    "    results['timestamp'] = datetime.now().isoformat()\n",
    "    results['model_info'] = {\n",
    "        'base_model': config.MODEL_PATH,\n",
    "        'adapter_path': str(adapters_dir),\n",
    "        'samples_tested': 400\n",
    "    }\n",
    "    \n",
    "    results_file = Path(adapters_dir) / \"evaluation_results.json\"\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"\\nResults saved to: {results_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics for Structured Field Predictions\n",
    "\n",
    "The function underneath measures how well the model performs on each individual field by calculating **precision**, **recall**, and **F1 score**, using the true positive, false positive, and false negative counts provided for each field.\n",
    "\n",
    "It also calculates two types of overall averages:\n",
    "\n",
    "- **Macro average**: Takes the average of the scores for each field, treating all fields equally.  \n",
    "- **Micro average**: Combines all true/false positives and negatives across fields before calculating the scores. This gives more weight to fields that appear more often.\n",
    "\n",
    "It also returns the **valid JSON rate**, which tells us how often the model produced a well-structured JSON response. This is important for understanding if the model is outputting usable results.\n",
    "\n",
    "Everything is returned as a dictionary, including both the per-field scores and the overall metrics. This makes it easier to spot which fields the model is handling well, and where it might need improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ml_metrics(field_predictions, valid_json_count, total_tests):\n",
    "    \"\"\"Calculate precision, recall, F1 for each field.\"\"\"\n",
    "    results = {\n",
    "        'field_metrics': {},\n",
    "        'overall_metrics': {},\n",
    "        'valid_json_rate': valid_json_count / total_tests\n",
    "    }\n",
    "    \n",
    "    all_tp = all_fp = all_fn = 0\n",
    "    \n",
    "    for field, metrics in field_predictions.items():\n",
    "        tp = metrics['true_positives']\n",
    "        fp = metrics['false_positives']\n",
    "        fn = metrics['false_negatives']\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        results['field_metrics'][field] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'support': tp + fn\n",
    "        }\n",
    "        \n",
    "        all_tp += tp\n",
    "        all_fp += fp\n",
    "        all_fn += fn\n",
    "    \n",
    "    # Macro averages\n",
    "    macro_precision = np.mean([m['precision'] for m in results['field_metrics'].values()])\n",
    "    macro_recall = np.mean([m['recall'] for m in results['field_metrics'].values()])\n",
    "    macro_f1 = np.mean([m['f1_score'] for m in results['field_metrics'].values()])\n",
    "    \n",
    "    # Micro averages\n",
    "    micro_precision = all_tp / (all_tp + all_fp) if (all_tp + all_fp) > 0 else 0\n",
    "    micro_recall = all_tp / (all_tp + all_fn) if (all_tp + all_fn) > 0 else 0\n",
    "    micro_f1 = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0\n",
    "    \n",
    "    results['overall_metrics'] = {\n",
    "        'macro_precision': macro_precision,\n",
    "        'macro_recall': macro_recall,\n",
    "        'macro_f1': macro_f1,\n",
    "        'micro_precision': micro_precision,\n",
    "        'micro_recall': micro_recall,\n",
    "        'micro_f1': micro_f1\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Evaluation Pass: Field-Level Metrics\n",
    "\n",
    "This function runs a second round of evaluation, this time looking more closely at **precision**, **recall**, and **F1 scores** for each individual field in the model’s output. It uses a random sample of up to 400 rows from the ACLED test set.\n",
    "\n",
    "For each example, the model is asked to extract structured data from the `notes` field. If the response includes valid JSON, the script checks whether each expected field has been filled out meaningfully (not empty, not just \"unknown\", etc.).\n",
    "\n",
    "To decide whether a field *should* have been present, a simple rule is used:  \n",
    "> If the input text has more than 50 words, we assume that field should be extractable.\n",
    "\n",
    "Based on this assumption, the function keeps track of how often each field is:\n",
    "\n",
    "- Correctly included when expected (**true positive**)  \n",
    "- Included when it wasn’t needed (**false positive**)  \n",
    "- Missing when it should be there (**false negative**)  \n",
    "- Skipped correctly (**true negative**)\n",
    "\n",
    "All these counts are passed to the `calculate_ml_metrics` function, which then calculates both **macro** and **micro averages**, plus per-field scores. It also logs the **valid JSON rate** to see how often the model is responding in a structured, usable format.\n",
    "\n",
    "This method gives a clearer picture of which fields the model is handling well, and which ones it’s either missing or predicting too often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def secondary_model_evaluation(model, tokeniser, num_samples=400):\n",
    "    \"\"\"Evaluation featuring standard machine learning metrics.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        test_df = pd.read_csv(\"ACLED_data_export/ACLED_test_dataset.csv\")\n",
    "        print(f\"Loaded {len(test_df)} test samples\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Test dataset not found\")\n",
    "        return None\n",
    "    \n",
    "    test_sample = test_df.sample(n=min(num_samples, len(test_df)), random_state=42)\n",
    "    \n",
    "    target_fields = [\n",
    "        'event_date', 'country', 'location', 'event_type', 'actor_1', 'actor_2',\n",
    "        'fatalities', 'civilian_casualties', 'casualty_type', 'weapons_mentioned',\n",
    "        'attack_method', 'property_damage', 'disorder_type', 'infrastructure_disruption'\n",
    "    ]\n",
    "    \n",
    "    # Track confusion matrix elements for each field\n",
    "    field_predictions = {field: {'true_positives': 0, 'false_positives': 0, \n",
    "                                'false_negatives': 0, 'true_negatives': 0} for field in target_fields}\n",
    "    \n",
    "    valid_json_count = 0\n",
    "    total_tests = len(test_sample)\n",
    "    \n",
    "    print(f\"\\nTesting model on {total_tests} samples and providing a set of standard machine learning metrics\")\n",
    "    \n",
    "    for idx, row in test_sample.iterrows():\n",
    "        text = row['notes']\n",
    "        \n",
    "        # Simple ground truth: assume all fields should be extractable if text is substantial\n",
    "        prompt = f\"[INST] Extract relevant information from this conflict event report:\\n\\n{text} [/INST]\"\n",
    "        \n",
    "        try:\n",
    "            response = generate_safe(model, tokeniser, prompt, max_tokens=512, temperature=0.05)\n",
    "            \n",
    "            # Try to extract JSON\n",
    "            json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "            if json_match:\n",
    "                try:\n",
    "                    extracted = json.loads(json_match.group())\n",
    "                    valid_json_count += 1\n",
    "                    \n",
    "                    # For each field, check if meaningfully extracted\n",
    "                    for field in target_fields:\n",
    "                        has_field = field in extracted and extracted[field] not in [None, \"\", \"unknown\", \"N/A\"]\n",
    "                        \n",
    "                        # Simple heuristic: assume field should be present if text > 50 words\n",
    "                        should_have_field = len(text.split()) > 50\n",
    "                        \n",
    "                        if has_field and should_have_field:\n",
    "                            field_predictions[field]['true_positives'] += 1\n",
    "                        elif has_field and not should_have_field:\n",
    "                            field_predictions[field]['false_positives'] += 1\n",
    "                        elif not has_field and should_have_field:\n",
    "                            field_predictions[field]['false_negatives'] += 1\n",
    "                        else:\n",
    "                            field_predictions[field]['true_negatives'] += 1\n",
    "                            \n",
    "                except json.JSONDecodeError:\n",
    "                    # Failed JSON parsing\n",
    "                    for field in target_fields:\n",
    "                        field_predictions[field]['false_negatives'] += 1\n",
    "            else:\n",
    "                # No JSON found\n",
    "                for field in target_fields:\n",
    "                    field_predictions[field]['false_negatives'] += 1\n",
    "                    \n",
    "        except Exception as e:\n",
    "            # Generation failed\n",
    "            for field in target_fields:\n",
    "                field_predictions[field]['false_negatives'] += 1\n",
    "    \n",
    "    results = calculate_ml_metrics(field_predictions, valid_json_count, total_tests)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SECOND BATCH OF EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nOverall Metrics:\")\n",
    "    print(f\"Valid JSON Rate: {results['valid_json_rate']:.3f}\")\n",
    "    print(f\"Macro F1 Score:  {results['overall_metrics']['macro_f1']:.3f}\")\n",
    "    print(f\"Macro Precision: {results['overall_metrics']['macro_precision']:.3f}\")\n",
    "    print(f\"Macro Recall:    {results['overall_metrics']['macro_recall']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nField-Level Metrics:\")\n",
    "    print(f\"{'Field':<25} {'Precision':<10} {'Recall':<10} {'F1':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for field, metrics in results['field_metrics'].items():\n",
    "        print(f\"{field:<25} {metrics['precision']:<10.3f} {metrics['recall']:<10.3f} {metrics['f1_score']:<10.3f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising and Saving Second Evaluation Results\n",
    "\n",
    "After running the second evaluation, this block generates two main visualisations:\n",
    "\n",
    "1. **F1 Score by Field**  \n",
    "   A horizontal bar chart showing how the model did on each output field. This helps highlight which areas are weaker and might need more work or better data.\n",
    "\n",
    "2. **Overall Performance**  \n",
    "   A vertical bar chart with:\n",
    "   - Macro F1  \n",
    "   - Macro precision  \n",
    "   - Macro recall  \n",
    "   - Percentage of valid JSON outputs  \n",
    "\n",
    "   These give a general sense of how reliable the model is overall.\n",
    "\n",
    "The script also adds some metadata to the results, like the timestamp, model paths, and LoRA parameters, so the experiment can be reproduced later. Everything gets saved to `enhanced_evaluation_results.json` in the adapter folder.\n",
    "\n",
    "If there are no valid outputs (for example, if something goes wrong), it skips the visualisations and prints a warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Second Model Evaluation\n",
      "Loaded 400 test samples\n",
      "\n",
      "Testing model on 400 samples and providing a set of standard machine learning metrics\n",
      "\n",
      "============================================================\n",
      "SECOND BATCH OF EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "Overall Metrics:\n",
      "Valid JSON Rate: 1.000\n",
      "Macro F1 Score:  0.291\n",
      "Macro Precision: 0.220\n",
      "Macro Recall:    0.755\n",
      "\n",
      "Field-Level Metrics:\n",
      "Field                     Precision  Recall     F1        \n",
      "------------------------------------------------------------\n",
      "event_date                0.130      1.000      0.230     \n",
      "country                   0.130      1.000      0.230     \n",
      "location                  0.133      1.000      0.235     \n",
      "event_type                0.130      1.000      0.230     \n",
      "actor_1                   0.132      0.981      0.232     \n",
      "actor_2                   0.393      0.462      0.425     \n",
      "fatalities                0.754      0.827      0.789     \n",
      "civilian_casualties       0.143      0.038      0.061     \n",
      "casualty_type             0.500      0.865      0.634     \n",
      "weapons_mentioned         0.144      0.519      0.225     \n",
      "attack_method             0.140      0.846      0.240     \n",
      "property_damage           0.130      1.000      0.230     \n",
      "disorder_type             0.134      0.981      0.236     \n",
      "infrastructure_disruption 0.091      0.058      0.071     \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second batch of evaluation results saved to: ACLED_llama_fine_tuned/adapters_optimised/evaluation_results_002.json\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Font config\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'Arial',\n",
    "    'font.weight': 'bold',\n",
    "    'font.size': 12,\n",
    "    'axes.labelsize': 12,\n",
    "    'axes.titlesize': 14\n",
    "})\n",
    "\n",
    "print(\"Running Second Model Evaluation\")\n",
    "enhanced_results = secondary_model_evaluation(model_v, tok_v, num_samples=400)\n",
    "\n",
    "if enhanced_results:\n",
    "\n",
    "    colour_palette = ['#80CBC4', '#4DB6AC', '#546E7A', '#455A64']\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # F1 Scores by Field\n",
    "    fields = list(enhanced_results['field_metrics'].keys())\n",
    "    f1_scores = [enhanced_results['field_metrics'][field]['f1_score'] for field in fields]\n",
    "    \n",
    "    bars = ax1.barh(fields, f1_scores, color='#80CBC4', edgecolor='#455A64')\n",
    "    ax1.set_xlabel('F1 Score')\n",
    "    ax1.set_title('F1 Score by Field', fontweight='bold')\n",
    "    ax1.set_xlim(0, 1)\n",
    "    \n",
    "    for bar, score in zip(bars, f1_scores):\n",
    "        ax1.text(score + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                 f'{score:.2f}', va='center')\n",
    "\n",
    "    # Overall Metrics\n",
    "    metrics = ['Macro F1', 'Macro Precision', 'Macro Recall', 'JSON Success']\n",
    "    values = [\n",
    "        enhanced_results['overall_metrics']['macro_f1'],\n",
    "        enhanced_results['overall_metrics']['macro_precision'],\n",
    "        enhanced_results['overall_metrics']['macro_recall'],\n",
    "        enhanced_results['valid_json_rate']\n",
    "    ]\n",
    "    \n",
    "    bars = ax2.bar(metrics, values, color=colour_palette)\n",
    "    ax2.set_ylabel('Score')\n",
    "    ax2.set_title('Overall Performance', fontweight='bold')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, value in zip(bars, values):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                 f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Append metadata\n",
    "    enhanced_results['evaluation_metadata'] = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'model_info': {\n",
    "            'base_model': config.MODEL_PATH,\n",
    "            'adapter_path': str(adapters_dir),\n",
    "            'samples_tested': 400\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    enhanced_results_file = Path(adapters_dir) / \"evaluation_results_002.json\"\n",
    "    with open(enhanced_results_file, 'w') as f:\n",
    "        json.dump(enhanced_results, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"Second batch of evaluation results saved to: {enhanced_results_file}\")\n",
    "else:\n",
    "\n",
    "    print(\"Second evaluation failed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
