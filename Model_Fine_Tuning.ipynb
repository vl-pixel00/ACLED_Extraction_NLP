{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Llama 3.2 1B for Conflict Event Extraction  \n",
    "\n",
    "This notebook demonstrates how to fine-tune a quantised **Llama 3.2 1B** model using **LoRA** to extract structured information from ACLED conflict event narratives.  \n",
    "\n",
    "The goal is to convert unstructured text into a consistent JSON schema covering **14 fields**, including event date, location, actors, casualties, and other conflict attributes.  \n",
    "\n",
    "### Workflow Overview  \n",
    "1. Prepare and validate the ACLED dataset  \n",
    "2. Configure LoRA adapters for the model  \n",
    "3. Fine-tune the model with optimised training settings  \n",
    "4. Evaluate performance using structured accuracy metrics  \n",
    "\n",
    "This pipeline supports applications in automated conflict monitoring, research, and real-time analysis.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Dependencies\n",
    "\n",
    "This cell imports all core libraries and external dependencies required for model fine-tuning and evaluation within the MLX framework.\n",
    "\n",
    "#### Standard Python Libraries\n",
    "- `os`, `re`, `json`, `subprocess`, `random`: Provide support for file operations, regular expression handling, JSON parsing, shell command execution, and random seed control.\n",
    "- `Path`, `datetime`, `timedelta`: Facilitate structured file path management and time tracking.\n",
    "- `dataclass`, `typing`, `collections`: Enable structured configurations using dataclasses, type annotations, and grouped data structures.\n",
    "\n",
    "#### Data Processing\n",
    "- `pandas`, `numpy`: Used for structured data manipulation and efficient numerical computation.\n",
    "\n",
    "#### MLX Ecosystem\n",
    "- `mlx.core`: The core MLX library for tensor operations, optimised for Apple Silicon using the Metal backend.\n",
    "- `mlx_lm`: A high-level module for loading models, generating text, and managing fine-tuning workflows compatible with the MLX backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import inspect\n",
    "import subprocess\n",
    "import random\n",
    "import contextlib\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from contextlib import suppress\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import mlx.core as mx\n",
    "from mlx_lm import load, generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Configuration and Environment Setup\n",
    "\n",
    "This cell defines the core configuration parameters for fine-tuning the Llama 3.2 1B model on the ACLED dataset using Low-Rank Adaptation (LoRA), and prepares the training environment accordingly.\n",
    "\n",
    "#### Configuration Parameters\n",
    "\n",
    "A `@dataclass` is used to define the training setup:\n",
    "\n",
    "- **Model path**: `\"llama_models/Llama-3.2-1B-Instruct-4bit\"` — the base instruction-tuned model checkpoint.\n",
    "- **Dataset path**: `\"ACLED_data_export/ACLED_finetuning_dataset.jsonl\"` — the preprocessed dataset formatted for instruction–response fine-tuning.\n",
    "- **Output directory**: `\"ACLED_llama_fine_tuned\"` — location for all outputs, including adapter checkpoints and logs.\n",
    "\n",
    "Key hyperparameters:\n",
    "\n",
    "- `NUM_EPOCHS = 3`: Number of training epochs.\n",
    "- `LEARNING_RATE = 2e-4`: Initial learning rate.\n",
    "- `BATCH_SIZE = 4`: Training batch size.\n",
    "- `MAX_SEQ_LEN = 1024`: Maximum sequence length for each input sample.\n",
    "- `ITERATIONS = 6000`: Total number of training iterations.\n",
    "- `EVAL_FREQUENCY = 500`: Evaluate and checkpoint every 500 steps.\n",
    "- `SEED = 42`: Random seed for reproducibility.\n",
    "\n",
    "#### Environment Setup\n",
    "\n",
    "- **Random Seed Initialisation**:\n",
    "  Ensures reproducible results across Python (`random`), NumPy (`np`), and MLX (`mx`).\n",
    "\n",
    "- **Output Directories**:\n",
    "  Automatically creates the output structure:\n",
    "  - `data/` for dataset fragments (if needed),\n",
    "  - `adapters_optimised/` for saving LoRA checkpoints.\n",
    "\n",
    "- **Tokenizer Parallelism**:\n",
    "  Disables tokeniser multiprocessing via `TOKENIZERS_PARALLELISM=false` to avoid potential conflicts during training.\n",
    "\n",
    "#### MLX Hardware Check\n",
    "\n",
    "The cell also prints:\n",
    "\n",
    "- MLX version.\n",
    "- Default device and its type (CPU or GPU).\n",
    "- Whether Metal Performance Shaders (MPS) are available.\n",
    "- Active memory usage in gigabytes.\n",
    "\n",
    "If GPU acceleration is available via Metal, it is confirmed; otherwise, a warning is issued to highlight that training may be significantly slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACLED Conflict Event Extraction — Llama 3.2 1B LoRA Fine-tuning\n",
      "========================================================================\n",
      "\n",
      "MLX version: 0.28.0\n",
      "Default device: Device(gpu, 0)\n",
      "Device type: DeviceType.gpu\n",
      "Is Metal available: True\n",
      "Active memory: 0.00 GB\n",
      "MPS (Metal Performance Shaders) GPU acceleration is enabled.\n",
      "\n",
      "Training Configuration:\n",
      "        MODEL_PATH: llama_models/Llama-3.2-1B-Instruct-4bit\n",
      "         DATA_PATH: ACLED_data_export/ACLED_finetuning_dataset.jsonl\n",
      "        OUTPUT_DIR: ACLED_llama_fine_tuned\n",
      "     LEARNING_RATE: 0.0002\n",
      "        BATCH_SIZE: 4\n",
      "       MAX_SEQ_LEN: 1024\n",
      "        ITERATIONS: 7000\n",
      "              SEED: 42\n",
      "    EVAL_FREQUENCY: 500\n"
     ]
    }
   ],
   "source": [
    "print(\"ACLED Conflict Event Extraction — Llama 3.2 1B LoRA Fine-tuning\")\n",
    "print(\"=\" * 72)\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    MODEL_PATH: str = \"llama_models/Llama-3.2-1B-Instruct-4bit\"\n",
    "    DATA_PATH: str  = \"ACLED_data_export/ACLED_finetuning_dataset.jsonl\"\n",
    "    OUTPUT_DIR: str = \"ACLED_llama_fine_tuned\"\n",
    "\n",
    "    LEARNING_RATE: float = 2e-4  \n",
    "    BATCH_SIZE: int      = 4\n",
    "    MAX_SEQ_LEN: int     = 1024  \n",
    "    ITERATIONS: int      = 7000\n",
    "    SEED: int            = 42\n",
    "    EVAL_FREQUENCY: int  = 500\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Random seeds for reproducibility\n",
    "random.seed(config.SEED)\n",
    "np.random.seed(config.SEED)\n",
    "mx.random.seed(config.SEED)\n",
    "\n",
    "out_dir      = Path(config.OUTPUT_DIR)\n",
    "data_dir     = out_dir / \"data\"\n",
    "adapters_dir = out_dir / \"adapters_optimised\"\n",
    "for p in (out_dir, data_dir, adapters_dir):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Prevent tokenizer multiprocessing issues\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# MLX version and device check\n",
    "print(f\"\\nMLX version: {mx.__version__}\")\n",
    "with suppress(Exception):\n",
    "    device = mx.default_device()\n",
    "    print(f\"Default device: {device}\")\n",
    "    print(f\"Device type: {device.type}\")\n",
    "    print(f\"Is Metal available: {mx.metal.is_available()}\")\n",
    "    print(f\"Active memory: {mx.get_active_memory() / (1024**3):.2f} GB\")\n",
    "\n",
    "    if getattr(device.type, \"name\", str(device.type)).lower() == \"gpu\":\n",
    "        print(\"MPS (Metal Performance Shaders) GPU acceleration is enabled.\")\n",
    "    else:\n",
    "        print(\"WARNING: Not using GPU acceleration — training may be slow.\")\n",
    "\n",
    "print(\"\\nTraining Configuration:\")\n",
    "for key, value in config.__dict__.items():\n",
    "    print(f\"{key:>18}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation\n",
    "\n",
    "#### The cell below performs the following:\n",
    "\n",
    "- Reads and validates the fine-tuning dataset from the specified JSONL file.\n",
    "- Filters out malformed lines and entries missing the `instruction` or `output` fields.\n",
    "- Converts each entry into Llama-compatible format using `[INST] ... [/INST]` syntax.\n",
    "- Shuffles and splits the dataset into training (80%) and validation (20%) sets using a fixed seed.\n",
    "- Saves the resulting files as `train.jsonl` and `valid.jsonl` for use during fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset for fine-tuning\n",
      "Total usable: 25,000\n",
      "Malformed/missing keys: 0\n",
      "Train: 20,000 -> ACLED_llama_fine_tuned/data/train.jsonl\n",
      "Valid: 5,000 -> ACLED_llama_fine_tuned/data/valid.jsonl\n",
      "Preview: [INST] Extract relevant information from this conflict event report:  On 23 February 2025, Russian forces shelled Ukrainian positions near Kupiansk, Kharkiv. According to Russian sources, up to 200 Uk\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing dataset for fine-tuning\")\n",
    "\n",
    "src_path = Path(config.DATA_PATH)\n",
    "assert src_path.is_file(), f\"Data file not found: {src_path}\"\n",
    "\n",
    "# Format instruction-output pairs into Llama-compatible input\n",
    "def to_llama_text(instr: str, out: str) -> str:\n",
    "    instr = instr.strip()\n",
    "    out = out.strip()\n",
    "    return f\"[INST] {instr} [/INST] {out}\"\n",
    "\n",
    "formatted = []\n",
    "bad = 0\n",
    "\n",
    "# Read dataset line by line and parse valid JSON entries\n",
    "with src_path.open(\"r\") as f:\n",
    "    for i, line in enumerate(f, 1):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            obj = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            bad += 1\n",
    "            continue\n",
    "        if \"instruction\" not in obj or \"output\" not in obj:\n",
    "            bad += 1\n",
    "            continue\n",
    "        text = to_llama_text(obj[\"instruction\"], obj[\"output\"])\n",
    "        formatted.append({\"text\": text})\n",
    "\n",
    "# Ensure dataset is not empty after filtering\n",
    "assert len(formatted) > 0, \"Dataset is empty after strict parse\"\n",
    "\n",
    "rng = random.Random(config.SEED)\n",
    "rng.shuffle(formatted)\n",
    "split_idx = int(0.8 * len(formatted))\n",
    "train_data = formatted[:split_idx]\n",
    "valid_data = formatted[split_idx:]\n",
    "\n",
    "train_path = data_dir / \"train.jsonl\"\n",
    "valid_path = data_dir / \"valid.jsonl\"\n",
    "with train_path.open(\"w\") as f:\n",
    "    for s in train_data:\n",
    "        f.write(json.dumps(s, ensure_ascii=False) + \"\\n\")\n",
    "with valid_path.open(\"w\") as f:\n",
    "    for s in valid_data:\n",
    "        f.write(json.dumps(s, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Total usable: {len(formatted):,}\")\n",
    "print(f\"Malformed/missing keys: {bad:,}\")\n",
    "print(f\"Train: {len(train_data):,} -> {train_path}\")\n",
    "print(f\"Valid: {len(valid_data):,} -> {valid_path}\")\n",
    "if train_data:\n",
    "    print(\"Preview:\", train_data[0][\"text\"][:200].replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA Configuration\n",
    "\n",
    "This step creates a YAML configuration file containing the LoRA hyperparameters used for training. The file is written to the output directory and will be passed to the MLX-LM training command. Saving this configuration separately ensures the training run is reproducible and the adapter settings are explicitly documented.\n",
    "\n",
    "The following values are defined:\n",
    "\n",
    "- `lora_r`: 16 (rank of the low-rank adaptation)\n",
    "- `lora_alpha`: 32 (scaling factor)\n",
    "- `lora_dropout`: 0.1 (dropout rate applied to adapter layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing LoRA configuration\n",
      "Wrote ACLED_llama_fine_tuned/lora_optimised.yaml\n"
     ]
    }
   ],
   "source": [
    "print(\"Writing LoRA configuration\")\n",
    "\n",
    "lora_yaml_path = out_dir / \"lora_optimised.yaml\"\n",
    "lora_yaml_path.write_text(\n",
    "    \"lora_r: 16\\n\"\n",
    "    \"lora_alpha: 32\\n\"\n",
    "    \"lora_dropout: 0.1\\n\"\n",
    ")\n",
    "print(f\"Wrote {lora_yaml_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning Execution with Checkpoint Support and ETA Tracking\n",
    "\n",
    "This cell executes the main training loop using the `mlx_lm` CLI with LoRA on the quantised Llama 3.2 1B model. The pipeline supports resumable training, live loss reporting, and time estimation. All parameters are inherited from the `Config` class.\n",
    "\n",
    "#### Adaptive Resume Logic and Checkpoint Management\n",
    "\n",
    "Before training begins, the script checks for previously saved adapter checkpoints:\n",
    "\n",
    "- Uses file pattern matching to identify the last completed training step.\n",
    "- If a checkpoint is found, it is promoted to `adapters.safetensors` for resumption.\n",
    "- If not, training begins from iteration 0.\n",
    "\n",
    "This ensures safe recovery from interruptions and avoids redundant computation.\n",
    "\n",
    "#### Command-Line Interface and Training Parameters\n",
    "\n",
    "The training command is constructed as a subprocess call with the following options:\n",
    "\n",
    "- `--model`: Path to the 4-bit quantised Llama 3.2 model\n",
    "- `--data`: Directory containing formatted `train.jsonl` and `valid.jsonl` files\n",
    "- `--batch-size`, `--iters`, `--learning-rate`, `--max-seq-length`: Controlled by config\n",
    "- `--adapter-path`: Directory to save LoRA adapter checkpoints\n",
    "- `--grad-checkpoint`: Enables memory-efficient training\n",
    "- `-c`: Points to the YAML file specifying LoRA hyperparameters\n",
    "\n",
    "These arguments ensure reproducibility and make the setup easily extendable for further experimentation.\n",
    "\n",
    "#### Live Log Parsing and ETA Monitoring\n",
    "\n",
    "The subprocess output is parsed in real time using regular expressions to extract:\n",
    "\n",
    "- **Step Number** (`Iter N`)\n",
    "- **Training Loss** (`Train loss …`)\n",
    "- **Validation Loss** (`Val loss …`)\n",
    "\n",
    "Once iteration progress begins, a smoothed estimate of time per step is calculated and used to print:\n",
    "\n",
    "- Percentage of training completed\n",
    "- Current local and global step counts\n",
    "- Current learning rate\n",
    "- Most recent training and validation losses\n",
    "- Estimated time remaining (ETA)\n",
    "\n",
    "The display updates live, providing detailed feedback without requiring external logging tools.\n",
    "\n",
    "#### Final Output and Summary\n",
    "\n",
    "After completion or interruption, the script prints:\n",
    "\n",
    "- The final checkpointed step\n",
    "- Total training time in hours, minutes, and seconds\n",
    "- Final observed training and validation losses (if available)\n",
    "\n",
    "This design enables local fine-tuning on macOS with Apple Silicon and Metal acceleration, providing a stable, resumable, and transparent fine-tuning loop suitable for both experimentation and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training (constant LR)\n",
      "  target iters = 7000 | lr = 0.0002 | batch = 4\n",
      "No checkpoints found. Starting at 0.\n",
      "Command: python -m mlx_lm lora --model llama_models/Llama-3.2-1B-Instruct-4bit --train --data ACLED_llama_fine_tuned/data --batch-size 4 --iters 7000 --learning-rate 0.0002 --steps-per-report 100 --steps-per-eval 500 --save-every 500 --adapter-path ACLED_llama_fine_tuned/adapters_optimised --max-seq-length 1024 -c ACLED_llama_fine_tuned/lora_optimised.yaml --grad-checkpoint --seed 42\n",
      "Progress: 0.0% | Local: 0/7000 | Global: 0→7000 | ETA: warming up…\n",
      "Loading configuration file ACLED_llama_fine_tuned/lora_optimised.yaml\n",
      "Loading pretrained model\n",
      "Loading datasets\n",
      "Training\n",
      "Trainable parameters: 0.069% (0.852M/1235.814M)\n",
      "Starting training..., iters: 7000\n",
      "Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "Calculating loss...:   4%|▍         | 1/25 [00:00<00:14,  1.64it/s]\n",
      "Calculating loss...:   8%|▊         | 2/25 [00:01<00:17,  1.32it/s]\n",
      "Calculating loss...:  12%|█▏        | 3/25 [00:01<00:14,  1.53it/s]\n",
      "Calculating loss...:  16%|█▌        | 4/25 [00:02<00:15,  1.38it/s]\n",
      "Calculating loss...:  20%|██        | 5/25 [00:03<00:13,  1.49it/s]\n",
      "Calculating loss...:  24%|██▍       | 6/25 [00:03<00:12,  1.57it/s]\n",
      "Calculating loss...:  28%|██▊       | 7/25 [00:04<00:11,  1.55it/s]\n",
      "Calculating loss...:  32%|███▏      | 8/25 [00:05<00:11,  1.42it/s]\n",
      "Calculating loss...:  36%|███▌      | 9/25 [00:06<00:12,  1.27it/s]\n",
      "Calculating loss...:  40%|████      | 10/25 [00:07<00:10,  1.38it/s]\n",
      "Calculating loss...:  44%|████▍     | 11/25 [00:07<00:09,  1.43it/s]\n",
      "Calculating loss...:  48%|████▊     | 12/25 [00:08<00:08,  1.47it/s]\n",
      "Calculating loss...:  52%|█████▏    | 13/25 [00:08<00:07,  1.59it/s]\n",
      "Calculating loss...:  56%|█████▌    | 14/25 [00:09<00:06,  1.57it/s]\n",
      "Calculating loss...:  60%|██████    | 15/25 [00:09<00:05,  1.67it/s]\n",
      "Calculating loss...:  64%|██████▍   | 16/25 [00:10<00:05,  1.69it/s]\n",
      "Calculating loss...:  68%|██████▊   | 17/25 [00:11<00:05,  1.50it/s]\n",
      "Calculating loss...:  72%|███████▏  | 18/25 [00:11<00:04,  1.57it/s]\n",
      "Calculating loss...:  76%|███████▌  | 19/25 [00:12<00:03,  1.56it/s]\n",
      "Calculating loss...:  80%|████████  | 20/25 [00:13<00:03,  1.61it/s]\n",
      "Calculating loss...:  84%|████████▍ | 21/25 [00:14<00:02,  1.46it/s]\n",
      "Calculating loss...:  88%|████████▊ | 22/25 [00:14<00:02,  1.49it/s]\n",
      "Calculating loss...:  92%|█████████▏| 23/25 [00:15<00:01,  1.51it/s]\n",
      "Calculating loss...:  96%|█████████▌| 24/25 [00:16<00:00,  1.36it/s]\n",
      "Calculating loss...: 100%|██████████| 25/25 [00:17<00:00,  1.31it/s]\n",
      "Calculating loss...: 100%|██████████| 25/25 [00:17<00:00,  1.47it/s]\n",
      "Iter 1: Val loss 2.813, Val took 17.057s\n",
      "Run (0.0% total) | Local 1/7000 | Global 1/7000 | LR 2.0e-04 | Val 2.813 | ETA estimating…Iter 100: Train loss 0.939, Learning Rate 2.000e-04, It/sec 0.515, Tokens/sec 548.587, Trained Tokens 106603, Peak mem 3.712 GB\n",
      "Run (1.4% total) | Local 100/7000 | Global 100/7000 | LR 2.0e-04 | Train 0.939 | Val 2.813 | ETA 03:45:57Iter 200: Train loss 0.684, Learning Rate 2.000e-04, It/sec 0.494, Tokens/sec 529.821, Trained Tokens 213852, Peak mem 3.719 GB\n",
      "Run (2.9% total) | Local 200/7000 | Global 200/7000 | LR 2.0e-04 | Train 0.684 | Val 2.813 | ETA 03:44:46Iter 300: Train loss 0.610, Learning Rate 2.000e-04, It/sec 0.507, Tokens/sec 546.733, Trained Tokens 321779, Peak mem 3.719 GB\n",
      "Run (4.3% total) | Local 300/7000 | Global 300/7000 | LR 2.0e-04 | Train 0.610 | Val 2.813 | ETA 03:41:13Iter 400: Train loss 0.492, Learning Rate 2.000e-04, It/sec 0.471, Tokens/sec 515.589, Trained Tokens 431261, Peak mem 3.719 GB\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.492 | Val 2.813 | ETA 03:42:39Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.492 | Val 2.813 | ETA 03:42:39Calculating loss...:   4%|▍         | 1/25 [00:00<00:20,  1.17it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.492 | Val 2.813 | ETA 03:42:39Calculating loss...:   8%|▊         | 2/25 [00:01<00:17,  1.35it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.492 | Val 2.813 | ETA 03:42:39Calculating loss...:  12%|█▏        | 3/25 [00:02<00:18,  1.18it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.492 | Val 2.813 | ETA 03:42:39Calculating loss...:  16%|█▌        | 4/25 [00:03<00:15,  1.38it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.492 | Val 2.813 | ETA 03:42:39Calculating loss...:  20%|██        | 5/25 [00:03<00:15,  1.32it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.492 | Val 2.813 | ETA 03:42:39Calculating loss...:  24%|██▍       | 6/25 [00:04<00:13,  1.43it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.492 | Val 2.813 | ETA 03:42:39Calculating loss...:  28%|██▊       | 7/25 [00:05<00:14,  1.27it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.492 | Val 2.813 | ETA 03:42:39Calculating loss...:  32%|███▏      | 8/25 [00:05<00:12,  1.38it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.492 | Val 2.813 | ETA 03:42:39Calculating loss...:  36%|███▌      | 9/25 [00:06<00:11,  1.41it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.492 | Val 2.813 | ETA 03:42:39Calculating loss...:  40%|████      | 10/25 [00:07<00:11,  1.30it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.492 | Val 2.813 | ETA 03:42:39Calculating loss...:  44%|████▍     | 11/25 [00:08<00:09,  1.40it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.492 | Val 2.813 | ETA 03:42:39Calculating loss...:  48%|████▊     | 12/25 [00:09<00:10,  1.29it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.492 | Val 2.813 | ETA 03:42:39Calculating loss...:  52%|█████▏    | 13/25 [00:09<00:08,  1.39it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.492 | Val 2.813 | ETA 03:42:39Calculating loss...:  56%|█████▌    | 14/25 [00:10<00:08,  1.31it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.492 | Val 2.813 | ETA 03:42:39Calculating loss...:  60%|██████    | 15/25 [00:11<00:07,  1.41it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.492 | Val 2.813 | ETA 03:42:39Calculating loss...:  64%|██████▍   | 16/25 [00:11<00:06,  1.45it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.492 | Val 2.813 | ETA 03:42:39Calculating loss...:  68%|██████▊   | 17/25 [00:12<00:05,  1.52it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.492 | Val 2.813 | ETA 03:42:39Calculating loss...:  72%|███████▏  | 18/25 [00:12<00:04,  1.58it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.492 | Val 2.813 | ETA 03:42:39Calculating loss...:  76%|███████▌  | 19/25 [00:13<00:04,  1.31it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.492 | Val 2.813 | ETA 03:42:39Calculating loss...:  80%|████████  | 20/25 [00:14<00:03,  1.41it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.492 | Val 2.813 | ETA 03:42:39Calculating loss...:  84%|████████▍ | 21/25 [00:15<00:03,  1.32it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.492 | Val 2.813 | ETA 03:42:39Calculating loss...:  88%|████████▊ | 22/25 [00:16<00:02,  1.37it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.492 | Val 2.813 | ETA 03:42:39Calculating loss...:  92%|█████████▏| 23/25 [00:16<00:01,  1.43it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.492 | Val 2.813 | ETA 03:42:39Calculating loss...:  96%|█████████▌| 24/25 [00:17<00:00,  1.31it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.492 | Val 2.813 | ETA 03:42:39Calculating loss...: 100%|██████████| 25/25 [00:18<00:00,  1.38it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.492 | Val 2.813 | ETA 03:42:39Calculating loss...: 100%|██████████| 25/25 [00:18<00:00,  1.37it/s]\n",
      "Run (5.7% total) | Local 400/7000 | Global 400/7000 | LR 2.0e-04 | Train 0.492 | Val 2.813 | ETA 03:42:39Iter 500: Val loss 0.388, Val took 18.277s\n",
      "Run (7.1% total) | Local 500/7000 | Global 500/7000 | LR 2.0e-04 | Train 0.492 | Val 0.388 | ETA 03:45:09Iter 500: Train loss 0.436, Learning Rate 2.000e-04, It/sec 0.490, Tokens/sec 524.108, Trained Tokens 538140, Peak mem 3.719 GB\n",
      "Run (7.1% total) | Local 500/7000 | Global 500/7000 | LR 2.0e-04 | Train 0.436 | Val 0.388 | ETA 03:45:09Iter 500: Saved adapter weights to ACLED_llama_fine_tuned/adapters_optimised/adapters.safetensors and ACLED_llama_fine_tuned/adapters_optimised/0000500_adapters.safetensors.\n",
      "Run (7.1% total) | Local 500/7000 | Global 500/7000 | LR 2.0e-04 | Train 0.436 | Val 0.388 | ETA 03:45:09Iter 600: Train loss 0.414, Learning Rate 2.000e-04, It/sec 0.534, Tokens/sec 562.218, Trained Tokens 643470, Peak mem 3.719 GB\n",
      "Run (8.6% total) | Local 600/7000 | Global 600/7000 | LR 2.0e-04 | Train 0.414 | Val 0.388 | ETA 03:35:10Iter 700: Train loss 0.403, Learning Rate 2.000e-04, It/sec 0.511, Tokens/sec 545.297, Trained Tokens 750215, Peak mem 3.719 GB\n",
      "Run (10.0% total) | Local 700/7000 | Global 700/7000 | LR 2.0e-04 | Train 0.403 | Val 0.388 | ETA 03:29:58Iter 800: Train loss 0.391, Learning Rate 2.000e-04, It/sec 0.511, Tokens/sec 553.211, Trained Tokens 858505, Peak mem 3.719 GB\n",
      "Run (11.4% total) | Local 800/7000 | Global 800/7000 | LR 2.0e-04 | Train 0.391 | Val 0.388 | ETA 03:25:22Iter 900: Train loss 0.357, Learning Rate 2.000e-04, It/sec 0.519, Tokens/sec 555.410, Trained Tokens 965441, Peak mem 3.723 GB\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.357 | Val 0.388 | ETA 03:20:12Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.357 | Val 0.388 | ETA 03:20:12Calculating loss...:   4%|▍         | 1/25 [00:00<00:21,  1.12it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.357 | Val 0.388 | ETA 03:20:12Calculating loss...:   8%|▊         | 2/25 [00:01<00:16,  1.44it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.357 | Val 0.388 | ETA 03:20:12Calculating loss...:  12%|█▏        | 3/25 [00:02<00:14,  1.51it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.357 | Val 0.388 | ETA 03:20:12Calculating loss...:  16%|█▌        | 4/25 [00:02<00:13,  1.55it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.357 | Val 0.388 | ETA 03:20:12Calculating loss...:  20%|██        | 5/25 [00:03<00:12,  1.58it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.357 | Val 0.388 | ETA 03:20:12Calculating loss...:  24%|██▍       | 6/25 [00:03<00:11,  1.65it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.357 | Val 0.388 | ETA 03:20:12Calculating loss...:  28%|██▊       | 7/25 [00:04<00:10,  1.64it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.357 | Val 0.388 | ETA 03:20:12Calculating loss...:  32%|███▏      | 8/25 [00:05<00:11,  1.49it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.357 | Val 0.388 | ETA 03:20:12Calculating loss...:  36%|███▌      | 9/25 [00:06<00:12,  1.32it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.357 | Val 0.388 | ETA 03:20:12Calculating loss...:  40%|████      | 10/25 [00:06<00:10,  1.48it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.357 | Val 0.388 | ETA 03:20:12Calculating loss...:  44%|████▍     | 11/25 [00:07<00:09,  1.48it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.357 | Val 0.388 | ETA 03:20:12Calculating loss...:  48%|████▊     | 12/25 [00:08<00:08,  1.52it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.357 | Val 0.388 | ETA 03:20:12Calculating loss...:  52%|█████▏    | 13/25 [00:08<00:08,  1.46it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.357 | Val 0.388 | ETA 03:20:12Calculating loss...:  56%|█████▌    | 14/25 [00:09<00:07,  1.39it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.357 | Val 0.388 | ETA 03:20:12Calculating loss...:  60%|██████    | 15/25 [00:10<00:07,  1.34it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.357 | Val 0.388 | ETA 03:20:12Calculating loss...:  64%|██████▍   | 16/25 [00:11<00:06,  1.31it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.357 | Val 0.388 | ETA 03:20:12Calculating loss...:  68%|██████▊   | 17/25 [00:12<00:06,  1.25it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.357 | Val 0.388 | ETA 03:20:12Calculating loss...:  72%|███████▏  | 18/25 [00:12<00:05,  1.28it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.357 | Val 0.388 | ETA 03:20:12Calculating loss...:  76%|███████▌  | 19/25 [00:13<00:04,  1.37it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.357 | Val 0.388 | ETA 03:20:12Calculating loss...:  80%|████████  | 20/25 [00:14<00:03,  1.43it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.357 | Val 0.388 | ETA 03:20:12Calculating loss...:  84%|████████▍ | 21/25 [00:14<00:02,  1.48it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.357 | Val 0.388 | ETA 03:20:12Calculating loss...:  88%|████████▊ | 22/25 [00:15<00:01,  1.52it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.357 | Val 0.388 | ETA 03:20:12Calculating loss...:  92%|█████████▏| 23/25 [00:15<00:01,  1.54it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.357 | Val 0.388 | ETA 03:20:12Calculating loss...:  96%|█████████▌| 24/25 [00:16<00:00,  1.56it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.357 | Val 0.388 | ETA 03:20:12Calculating loss...: 100%|██████████| 25/25 [00:17<00:00,  1.49it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.357 | Val 0.388 | ETA 03:20:12Calculating loss...: 100%|██████████| 25/25 [00:17<00:00,  1.45it/s]\n",
      "Run (12.9% total) | Local 900/7000 | Global 900/7000 | LR 2.0e-04 | Train 0.357 | Val 0.388 | ETA 03:20:12Iter 1000: Val loss 0.391, Val took 17.262s\n",
      "Run (14.3% total) | Local 1000/7000 | Global 1000/7000 | LR 2.0e-04 | Train 0.357 | Val 0.391 | ETA 03:22:28Iter 1000: Train loss 0.367, Learning Rate 2.000e-04, It/sec 0.500, Tokens/sec 543.966, Trained Tokens 1074299, Peak mem 3.723 GB\n",
      "Run (14.3% total) | Local 1000/7000 | Global 1000/7000 | LR 2.0e-04 | Train 0.367 | Val 0.391 | ETA 03:22:28Iter 1000: Saved adapter weights to ACLED_llama_fine_tuned/adapters_optimised/adapters.safetensors and ACLED_llama_fine_tuned/adapters_optimised/0001000_adapters.safetensors.\n",
      "Run (14.3% total) | Local 1000/7000 | Global 1000/7000 | LR 2.0e-04 | Train 0.367 | Val 0.391 | ETA 03:22:28Iter 1100: Train loss 0.366, Learning Rate 2.000e-04, It/sec 0.510, Tokens/sec 550.961, Trained Tokens 1182352, Peak mem 3.723 GB\n",
      "Run (15.7% total) | Local 1100/7000 | Global 1100/7000 | LR 2.0e-04 | Train 0.366 | Val 0.391 | ETA 03:17:15Iter 1200: Train loss 0.352, Learning Rate 2.000e-04, It/sec 0.516, Tokens/sec 553.485, Trained Tokens 1289697, Peak mem 3.723 GB\n",
      "Run (17.1% total) | Local 1200/7000 | Global 1200/7000 | LR 2.0e-04 | Train 0.352 | Val 0.391 | ETA 03:12:01Iter 1300: Train loss 0.362, Learning Rate 2.000e-04, It/sec 0.513, Tokens/sec 547.352, Trained Tokens 1396463, Peak mem 3.723 GB\n",
      "Run (18.6% total) | Local 1300/7000 | Global 1300/7000 | LR 2.0e-04 | Train 0.362 | Val 0.391 | ETA 03:07:43Iter 1400: Train loss 0.349, Learning Rate 2.000e-04, It/sec 0.526, Tokens/sec 548.786, Trained Tokens 1500849, Peak mem 3.723 GB\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.349 | Val 0.391 | ETA 03:02:24Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.349 | Val 0.391 | ETA 03:02:24Calculating loss...:   4%|▍         | 1/25 [00:00<00:19,  1.25it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.349 | Val 0.391 | ETA 03:02:24Calculating loss...:   8%|▊         | 2/25 [00:01<00:15,  1.44it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.349 | Val 0.391 | ETA 03:02:24Calculating loss...:  12%|█▏        | 3/25 [00:01<00:13,  1.58it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.349 | Val 0.391 | ETA 03:02:24Calculating loss...:  16%|█▌        | 4/25 [00:02<00:13,  1.54it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.349 | Val 0.391 | ETA 03:02:24Calculating loss...:  20%|██        | 5/25 [00:03<00:12,  1.61it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.349 | Val 0.391 | ETA 03:02:24Calculating loss...:  24%|██▍       | 6/25 [00:03<00:11,  1.66it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.349 | Val 0.391 | ETA 03:02:24Calculating loss...:  28%|██▊       | 7/25 [00:04<00:10,  1.71it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.349 | Val 0.391 | ETA 03:02:24Calculating loss...:  32%|███▏      | 8/25 [00:05<00:11,  1.53it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.349 | Val 0.391 | ETA 03:02:24Calculating loss...:  36%|███▌      | 9/25 [00:05<00:09,  1.60it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.349 | Val 0.391 | ETA 03:02:24Calculating loss...:  40%|████      | 10/25 [00:06<00:09,  1.66it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.349 | Val 0.391 | ETA 03:02:24Calculating loss...:  44%|████▍     | 11/25 [00:07<00:09,  1.47it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.349 | Val 0.391 | ETA 03:02:24Calculating loss...:  48%|████▊     | 12/25 [00:07<00:09,  1.35it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.349 | Val 0.391 | ETA 03:02:24Calculating loss...:  52%|█████▏    | 13/25 [00:08<00:08,  1.46it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.349 | Val 0.391 | ETA 03:02:24Calculating loss...:  56%|█████▌    | 14/25 [00:09<00:06,  1.59it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.349 | Val 0.391 | ETA 03:02:24Calculating loss...:  60%|██████    | 15/25 [00:09<00:06,  1.60it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.349 | Val 0.391 | ETA 03:02:24Calculating loss...:  64%|██████▍   | 16/25 [00:10<00:05,  1.51it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.349 | Val 0.391 | ETA 03:02:24Calculating loss...:  68%|██████▊   | 17/25 [00:11<00:05,  1.42it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.349 | Val 0.391 | ETA 03:02:24Calculating loss...:  72%|███████▏  | 18/25 [00:11<00:04,  1.43it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.349 | Val 0.391 | ETA 03:02:24Calculating loss...:  76%|███████▌  | 19/25 [00:12<00:04,  1.36it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.349 | Val 0.391 | ETA 03:02:24Calculating loss...:  80%|████████  | 20/25 [00:13<00:04,  1.23it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.349 | Val 0.391 | ETA 03:02:24Calculating loss...:  84%|████████▍ | 21/25 [00:14<00:03,  1.32it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.349 | Val 0.391 | ETA 03:02:24Calculating loss...:  88%|████████▊ | 22/25 [00:14<00:02,  1.44it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.349 | Val 0.391 | ETA 03:02:24Calculating loss...:  92%|█████████▏| 23/25 [00:15<00:01,  1.58it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.349 | Val 0.391 | ETA 03:02:24Calculating loss...:  96%|█████████▌| 24/25 [00:16<00:00,  1.46it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.349 | Val 0.391 | ETA 03:02:24Calculating loss...: 100%|██████████| 25/25 [00:16<00:00,  1.39it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.349 | Val 0.391 | ETA 03:02:24Calculating loss...: 100%|██████████| 25/25 [00:16<00:00,  1.47it/s]\n",
      "Run (20.0% total) | Local 1400/7000 | Global 1400/7000 | LR 2.0e-04 | Train 0.349 | Val 0.391 | ETA 03:02:24Iter 1500: Val loss 0.338, Val took 16.996s\n",
      "Run (21.4% total) | Local 1500/7000 | Global 1500/7000 | LR 2.0e-04 | Train 0.349 | Val 0.338 | ETA 03:03:16Iter 1500: Train loss 0.326, Learning Rate 2.000e-04, It/sec 0.513, Tokens/sec 544.560, Trained Tokens 1607088, Peak mem 3.723 GB\n",
      "Run (21.4% total) | Local 1500/7000 | Global 1500/7000 | LR 2.0e-04 | Train 0.326 | Val 0.338 | ETA 03:03:16Iter 1500: Saved adapter weights to ACLED_llama_fine_tuned/adapters_optimised/adapters.safetensors and ACLED_llama_fine_tuned/adapters_optimised/0001500_adapters.safetensors.\n",
      "Run (21.4% total) | Local 1500/7000 | Global 1500/7000 | LR 2.0e-04 | Train 0.326 | Val 0.338 | ETA 03:03:16Iter 1600: Train loss 0.361, Learning Rate 2.000e-04, It/sec 0.506, Tokens/sec 547.410, Trained Tokens 1715276, Peak mem 3.723 GB\n",
      "Run (22.9% total) | Local 1600/7000 | Global 1600/7000 | LR 2.0e-04 | Train 0.361 | Val 0.338 | ETA 02:59:21Iter 1700: Train loss 0.327, Learning Rate 2.000e-04, It/sec 0.504, Tokens/sec 540.472, Trained Tokens 1822501, Peak mem 3.723 GB\n",
      "Run (24.3% total) | Local 1700/7000 | Global 1700/7000 | LR 2.0e-04 | Train 0.327 | Val 0.338 | ETA 02:55:50Iter 1800: Train loss 0.362, Learning Rate 2.000e-04, It/sec 0.500, Tokens/sec 543.882, Trained Tokens 1931366, Peak mem 3.723 GB\n",
      "Run (25.7% total) | Local 1800/7000 | Global 1800/7000 | LR 2.0e-04 | Train 0.362 | Val 0.338 | ETA 02:52:50Iter 1900: Train loss 0.321, Learning Rate 2.000e-04, It/sec 0.502, Tokens/sec 547.101, Trained Tokens 2040428, Peak mem 3.723 GB\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.321 | Val 0.338 | ETA 02:49:31Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.321 | Val 0.338 | ETA 02:49:31Calculating loss...:   4%|▍         | 1/25 [00:00<00:15,  1.58it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.321 | Val 0.338 | ETA 02:49:31Calculating loss...:   8%|▊         | 2/25 [00:01<00:14,  1.58it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.321 | Val 0.338 | ETA 02:49:31Calculating loss...:  12%|█▏        | 3/25 [00:02<00:15,  1.45it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.321 | Val 0.338 | ETA 02:49:31Calculating loss...:  16%|█▌        | 4/25 [00:02<00:12,  1.63it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.321 | Val 0.338 | ETA 02:49:31Calculating loss...:  20%|██        | 5/25 [00:03<00:13,  1.46it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.321 | Val 0.338 | ETA 02:49:31Calculating loss...:  24%|██▍       | 6/25 [00:04<00:13,  1.38it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.321 | Val 0.338 | ETA 02:49:31Calculating loss...:  28%|██▊       | 7/25 [00:04<00:12,  1.49it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.321 | Val 0.338 | ETA 02:49:31Calculating loss...:  32%|███▏      | 8/25 [00:05<00:12,  1.36it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.321 | Val 0.338 | ETA 02:49:31Calculating loss...:  36%|███▌      | 9/25 [00:06<00:12,  1.32it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.321 | Val 0.338 | ETA 02:49:31Calculating loss...:  40%|████      | 10/25 [00:06<00:10,  1.40it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.321 | Val 0.338 | ETA 02:49:31Calculating loss...:  44%|████▍     | 11/25 [00:07<00:09,  1.55it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.321 | Val 0.338 | ETA 02:49:31Calculating loss...:  48%|████▊     | 12/25 [00:08<00:09,  1.44it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.321 | Val 0.338 | ETA 02:49:31Calculating loss...:  52%|█████▏    | 13/25 [00:08<00:07,  1.58it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.321 | Val 0.338 | ETA 02:49:31Calculating loss...:  56%|█████▌    | 14/25 [00:09<00:07,  1.50it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.321 | Val 0.338 | ETA 02:49:31Calculating loss...:  60%|██████    | 15/25 [00:10<00:06,  1.58it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.321 | Val 0.338 | ETA 02:49:31Calculating loss...:  64%|██████▍   | 16/25 [00:10<00:05,  1.64it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.321 | Val 0.338 | ETA 02:49:31Calculating loss...:  68%|██████▊   | 17/25 [00:11<00:05,  1.38it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.321 | Val 0.338 | ETA 02:49:31Calculating loss...:  72%|███████▏  | 18/25 [00:12<00:05,  1.37it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.321 | Val 0.338 | ETA 02:49:31Calculating loss...:  76%|███████▌  | 19/25 [00:13<00:04,  1.33it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.321 | Val 0.338 | ETA 02:49:31Calculating loss...:  80%|████████  | 20/25 [00:13<00:03,  1.48it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.321 | Val 0.338 | ETA 02:49:31Calculating loss...:  84%|████████▍ | 21/25 [00:14<00:02,  1.48it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.321 | Val 0.338 | ETA 02:49:31Calculating loss...:  88%|████████▊ | 22/25 [00:15<00:02,  1.44it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.321 | Val 0.338 | ETA 02:49:31Calculating loss...:  92%|█████████▏| 23/25 [00:15<00:01,  1.37it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.321 | Val 0.338 | ETA 02:49:31Calculating loss...:  96%|█████████▌| 24/25 [00:16<00:00,  1.40it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.321 | Val 0.338 | ETA 02:49:31Calculating loss...: 100%|██████████| 25/25 [00:17<00:00,  1.35it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.321 | Val 0.338 | ETA 02:49:31Calculating loss...: 100%|██████████| 25/25 [00:17<00:00,  1.44it/s]\n",
      "Run (27.1% total) | Local 1900/7000 | Global 1900/7000 | LR 2.0e-04 | Train 0.321 | Val 0.338 | ETA 02:49:31Iter 2000: Val loss 0.289, Val took 17.380s\n",
      "Run (28.6% total) | Local 2000/7000 | Global 2000/7000 | LR 2.0e-04 | Train 0.321 | Val 0.289 | ETA 02:53:16Iter 2000: Train loss 0.326, Learning Rate 2.000e-04, It/sec 0.473, Tokens/sec 508.620, Trained Tokens 2147962, Peak mem 3.723 GB\n",
      "Run (28.6% total) | Local 2000/7000 | Global 2000/7000 | LR 2.0e-04 | Train 0.326 | Val 0.289 | ETA 02:53:16Iter 2000: Saved adapter weights to ACLED_llama_fine_tuned/adapters_optimised/adapters.safetensors and ACLED_llama_fine_tuned/adapters_optimised/0002000_adapters.safetensors.\n",
      "Run (28.6% total) | Local 2000/7000 | Global 2000/7000 | LR 2.0e-04 | Train 0.326 | Val 0.289 | ETA 02:53:16Iter 2100: Train loss 0.325, Learning Rate 2.000e-04, It/sec 0.526, Tokens/sec 561.038, Trained Tokens 2254657, Peak mem 3.723 GB\n",
      "Run (30.0% total) | Local 2100/7000 | Global 2100/7000 | LR 2.0e-04 | Train 0.325 | Val 0.289 | ETA 02:45:29Iter 2200: Train loss 0.332, Learning Rate 2.000e-04, It/sec 0.490, Tokens/sec 543.008, Trained Tokens 2365485, Peak mem 3.723 GB\n",
      "Run (31.4% total) | Local 2200/7000 | Global 2200/7000 | LR 2.0e-04 | Train 0.332 | Val 0.289 | ETA 02:42:29Iter 2300: Train loss 0.329, Learning Rate 2.000e-04, It/sec 0.514, Tokens/sec 556.881, Trained Tokens 2473836, Peak mem 3.723 GB\n",
      "Run (32.9% total) | Local 2300/7000 | Global 2300/7000 | LR 2.0e-04 | Train 0.329 | Val 0.289 | ETA 02:37:07Iter 2400: Train loss 0.320, Learning Rate 2.000e-04, It/sec 0.490, Tokens/sec 518.789, Trained Tokens 2579727, Peak mem 3.723 GB\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.320 | Val 0.289 | ETA 02:34:38Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.320 | Val 0.289 | ETA 02:34:38Calculating loss...:   4%|▍         | 1/25 [00:00<00:13,  1.78it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.320 | Val 0.289 | ETA 02:34:38Calculating loss...:   8%|▊         | 2/25 [00:01<00:12,  1.91it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.320 | Val 0.289 | ETA 02:34:38Calculating loss...:  12%|█▏        | 3/25 [00:01<00:11,  1.96it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.320 | Val 0.289 | ETA 02:34:38Calculating loss...:  16%|█▌        | 4/25 [00:02<00:10,  1.98it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.320 | Val 0.289 | ETA 02:34:38Calculating loss...:  20%|██        | 5/25 [00:02<00:10,  1.83it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.320 | Val 0.289 | ETA 02:34:38Calculating loss...:  24%|██▍       | 6/25 [00:03<00:13,  1.43it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.320 | Val 0.289 | ETA 02:34:38Calculating loss...:  28%|██▊       | 7/25 [00:04<00:12,  1.48it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.320 | Val 0.289 | ETA 02:34:38Calculating loss...:  32%|███▏      | 8/25 [00:04<00:10,  1.55it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.320 | Val 0.289 | ETA 02:34:38Calculating loss...:  36%|███▌      | 9/25 [00:05<00:11,  1.39it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.320 | Val 0.289 | ETA 02:34:38Calculating loss...:  40%|████      | 10/25 [00:06<00:09,  1.50it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.320 | Val 0.289 | ETA 02:34:38Calculating loss...:  44%|████▍     | 11/25 [00:06<00:09,  1.54it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.320 | Val 0.289 | ETA 02:34:38Calculating loss...:  48%|████▊     | 12/25 [00:07<00:08,  1.57it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.320 | Val 0.289 | ETA 02:34:38Calculating loss...:  52%|█████▏    | 13/25 [00:08<00:08,  1.42it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.320 | Val 0.289 | ETA 02:34:38Calculating loss...:  56%|█████▌    | 14/25 [00:08<00:07,  1.47it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.320 | Val 0.289 | ETA 02:34:38Calculating loss...:  60%|██████    | 15/25 [00:09<00:06,  1.52it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.320 | Val 0.289 | ETA 02:34:38Calculating loss...:  64%|██████▍   | 16/25 [00:10<00:05,  1.53it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.320 | Val 0.289 | ETA 02:34:38Calculating loss...:  68%|██████▊   | 17/25 [00:10<00:05,  1.56it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.320 | Val 0.289 | ETA 02:34:38Calculating loss...:  72%|███████▏  | 18/25 [00:11<00:04,  1.58it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.320 | Val 0.289 | ETA 02:34:38Calculating loss...:  76%|███████▌  | 19/25 [00:12<00:03,  1.50it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.320 | Val 0.289 | ETA 02:34:38Calculating loss...:  80%|████████  | 20/25 [00:12<00:03,  1.54it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.320 | Val 0.289 | ETA 02:34:38Calculating loss...:  84%|████████▍ | 21/25 [00:13<00:02,  1.56it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.320 | Val 0.289 | ETA 02:34:38Calculating loss...:  88%|████████▊ | 22/25 [00:14<00:02,  1.45it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.320 | Val 0.289 | ETA 02:34:38Calculating loss...:  92%|█████████▏| 23/25 [00:14<00:01,  1.53it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.320 | Val 0.289 | ETA 02:34:38Calculating loss...:  96%|█████████▌| 24/25 [00:15<00:00,  1.59it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.320 | Val 0.289 | ETA 02:34:38Calculating loss...: 100%|██████████| 25/25 [00:15<00:00,  1.65it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.320 | Val 0.289 | ETA 02:34:38Calculating loss...: 100%|██████████| 25/25 [00:15<00:00,  1.57it/s]\n",
      "Run (34.3% total) | Local 2400/7000 | Global 2400/7000 | LR 2.0e-04 | Train 0.320 | Val 0.289 | ETA 02:34:38Iter 2500: Val loss 0.326, Val took 15.954s\n",
      "Run (35.7% total) | Local 2500/7000 | Global 2500/7000 | LR 2.0e-04 | Train 0.320 | Val 0.326 | ETA 02:33:35Iter 2500: Train loss 0.323, Learning Rate 2.000e-04, It/sec 0.507, Tokens/sec 541.678, Trained Tokens 2686662, Peak mem 3.723 GB\n",
      "Run (35.7% total) | Local 2500/7000 | Global 2500/7000 | LR 2.0e-04 | Train 0.323 | Val 0.326 | ETA 02:33:35Iter 2500: Saved adapter weights to ACLED_llama_fine_tuned/adapters_optimised/adapters.safetensors and ACLED_llama_fine_tuned/adapters_optimised/0002500_adapters.safetensors.\n",
      "Run (35.7% total) | Local 2500/7000 | Global 2500/7000 | LR 2.0e-04 | Train 0.323 | Val 0.326 | ETA 02:33:35Iter 2600: Train loss 0.314, Learning Rate 2.000e-04, It/sec 0.507, Tokens/sec 543.202, Trained Tokens 2793814, Peak mem 3.723 GB\n",
      "Run (37.1% total) | Local 2600/7000 | Global 2600/7000 | LR 2.0e-04 | Train 0.314 | Val 0.326 | ETA 02:28:32Iter 2700: Train loss 0.320, Learning Rate 2.000e-04, It/sec 0.480, Tokens/sec 518.604, Trained Tokens 2901793, Peak mem 3.723 GB\n",
      "Run (38.6% total) | Local 2700/7000 | Global 2700/7000 | LR 2.0e-04 | Train 0.320 | Val 0.326 | ETA 02:26:25Iter 2800: Train loss 0.299, Learning Rate 2.000e-04, It/sec 0.508, Tokens/sec 540.095, Trained Tokens 3008194, Peak mem 3.723 GB\n",
      "Run (40.0% total) | Local 2800/7000 | Global 2800/7000 | LR 2.0e-04 | Train 0.299 | Val 0.326 | ETA 02:21:30Iter 2900: Train loss 0.313, Learning Rate 2.000e-04, It/sec 0.512, Tokens/sec 546.849, Trained Tokens 3114920, Peak mem 3.723 GB\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.313 | Val 0.326 | ETA 02:16:44Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.313 | Val 0.326 | ETA 02:16:44Calculating loss...:   4%|▍         | 1/25 [00:00<00:14,  1.60it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.313 | Val 0.326 | ETA 02:16:44Calculating loss...:   8%|▊         | 2/25 [00:01<00:13,  1.71it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.313 | Val 0.326 | ETA 02:16:44Calculating loss...:  12%|█▏        | 3/25 [00:01<00:14,  1.52it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.313 | Val 0.326 | ETA 02:16:44Calculating loss...:  16%|█▌        | 4/25 [00:02<00:12,  1.62it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.313 | Val 0.326 | ETA 02:16:44Calculating loss...:  20%|██        | 5/25 [00:03<00:13,  1.51it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.313 | Val 0.326 | ETA 02:16:44Calculating loss...:  24%|██▍       | 6/25 [00:03<00:12,  1.53it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.313 | Val 0.326 | ETA 02:16:44Calculating loss...:  28%|██▊       | 7/25 [00:04<00:11,  1.51it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.313 | Val 0.326 | ETA 02:16:44Calculating loss...:  32%|███▏      | 8/25 [00:05<00:10,  1.59it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.313 | Val 0.326 | ETA 02:16:44Calculating loss...:  36%|███▌      | 9/25 [00:05<00:09,  1.66it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.313 | Val 0.326 | ETA 02:16:44Calculating loss...:  40%|████      | 10/25 [00:06<00:09,  1.61it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.313 | Val 0.326 | ETA 02:16:44Calculating loss...:  44%|████▍     | 11/25 [00:06<00:08,  1.67it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.313 | Val 0.326 | ETA 02:16:44Calculating loss...:  48%|████▊     | 12/25 [00:07<00:07,  1.71it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.313 | Val 0.326 | ETA 02:16:44Calculating loss...:  52%|█████▏    | 13/25 [00:08<00:07,  1.68it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.313 | Val 0.326 | ETA 02:16:44Calculating loss...:  56%|█████▌    | 14/25 [00:08<00:06,  1.66it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.313 | Val 0.326 | ETA 02:16:44Calculating loss...:  60%|██████    | 15/25 [00:09<00:06,  1.64it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.313 | Val 0.326 | ETA 02:16:44Calculating loss...:  64%|██████▍   | 16/25 [00:09<00:05,  1.64it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.313 | Val 0.326 | ETA 02:16:44Calculating loss...:  68%|██████▊   | 17/25 [00:10<00:05,  1.50it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.313 | Val 0.326 | ETA 02:16:44Calculating loss...:  72%|███████▏  | 18/25 [00:11<00:04,  1.42it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.313 | Val 0.326 | ETA 02:16:44Calculating loss...:  76%|███████▌  | 19/25 [00:12<00:04,  1.48it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.313 | Val 0.326 | ETA 02:16:44Calculating loss...:  80%|████████  | 20/25 [00:12<00:03,  1.47it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.313 | Val 0.326 | ETA 02:16:44Calculating loss...:  84%|████████▍ | 21/25 [00:13<00:02,  1.56it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.313 | Val 0.326 | ETA 02:16:44Calculating loss...:  88%|████████▊ | 22/25 [00:13<00:01,  1.63it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.313 | Val 0.326 | ETA 02:16:44Calculating loss...:  92%|█████████▏| 23/25 [00:14<00:01,  1.67it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.313 | Val 0.326 | ETA 02:16:44Calculating loss...:  96%|█████████▌| 24/25 [00:14<00:00,  1.71it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.313 | Val 0.326 | ETA 02:16:44Calculating loss...: 100%|██████████| 25/25 [00:15<00:00,  1.63it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.313 | Val 0.326 | ETA 02:16:44Calculating loss...: 100%|██████████| 25/25 [00:15<00:00,  1.60it/s]\n",
      "Run (41.4% total) | Local 2900/7000 | Global 2900/7000 | LR 2.0e-04 | Train 0.313 | Val 0.326 | ETA 02:16:44Iter 3000: Val loss 0.289, Val took 15.667s\n",
      "Run (42.9% total) | Local 3000/7000 | Global 3000/7000 | LR 2.0e-04 | Train 0.313 | Val 0.289 | ETA 02:18:07Iter 3000: Train loss 0.310, Learning Rate 2.000e-04, It/sec 0.476, Tokens/sec 521.028, Trained Tokens 3224267, Peak mem 3.723 GB\n",
      "Run (42.9% total) | Local 3000/7000 | Global 3000/7000 | LR 2.0e-04 | Train 0.310 | Val 0.289 | ETA 02:18:07Iter 3000: Saved adapter weights to ACLED_llama_fine_tuned/adapters_optimised/adapters.safetensors and ACLED_llama_fine_tuned/adapters_optimised/0003000_adapters.safetensors.\n",
      "Run (42.9% total) | Local 3000/7000 | Global 3000/7000 | LR 2.0e-04 | Train 0.310 | Val 0.289 | ETA 02:18:07Iter 3100: Train loss 0.319, Learning Rate 2.000e-04, It/sec 0.511, Tokens/sec 547.834, Trained Tokens 3331485, Peak mem 3.723 GB\n",
      "Run (44.3% total) | Local 3100/7000 | Global 3100/7000 | LR 2.0e-04 | Train 0.319 | Val 0.289 | ETA 02:12:27Iter 3200: Train loss 0.313, Learning Rate 2.000e-04, It/sec 0.513, Tokens/sec 552.864, Trained Tokens 3439176, Peak mem 3.723 GB\n",
      "Run (45.7% total) | Local 3200/7000 | Global 3200/7000 | LR 2.0e-04 | Train 0.313 | Val 0.289 | ETA 02:07:22Iter 3300: Train loss 0.302, Learning Rate 2.000e-04, It/sec 0.515, Tokens/sec 551.933, Trained Tokens 3546343, Peak mem 3.723 GB\n",
      "Run (47.1% total) | Local 3300/7000 | Global 3300/7000 | LR 2.0e-04 | Train 0.302 | Val 0.289 | ETA 02:02:46Iter 3400: Train loss 0.309, Learning Rate 2.000e-04, It/sec 0.460, Tokens/sec 498.463, Trained Tokens 3654786, Peak mem 3.723 GB\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.309 | Val 0.289 | ETA 02:02:48Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.309 | Val 0.289 | ETA 02:02:48Calculating loss...:   4%|▍         | 1/25 [00:00<00:17,  1.33it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.309 | Val 0.289 | ETA 02:02:48Calculating loss...:   8%|▊         | 2/25 [00:01<00:14,  1.57it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.309 | Val 0.289 | ETA 02:02:48Calculating loss...:  12%|█▏        | 3/25 [00:01<00:13,  1.64it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.309 | Val 0.289 | ETA 02:02:48Calculating loss...:  16%|█▌        | 4/25 [00:02<00:12,  1.70it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.309 | Val 0.289 | ETA 02:02:48Calculating loss...:  20%|██        | 5/25 [00:03<00:11,  1.67it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.309 | Val 0.289 | ETA 02:02:48Calculating loss...:  24%|██▍       | 6/25 [00:04<00:13,  1.36it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.309 | Val 0.289 | ETA 02:02:48Calculating loss...:  28%|██▊       | 7/25 [00:04<00:12,  1.47it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.309 | Val 0.289 | ETA 02:02:48Calculating loss...:  32%|███▏      | 8/25 [00:05<00:10,  1.56it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.309 | Val 0.289 | ETA 02:02:48Calculating loss...:  36%|███▌      | 9/25 [00:05<00:10,  1.59it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.309 | Val 0.289 | ETA 02:02:48Calculating loss...:  40%|████      | 10/25 [00:06<00:10,  1.50it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.309 | Val 0.289 | ETA 02:02:48Calculating loss...:  44%|████▍     | 11/25 [00:07<00:09,  1.41it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.309 | Val 0.289 | ETA 02:02:48Calculating loss...:  48%|████▊     | 12/25 [00:07<00:08,  1.52it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.309 | Val 0.289 | ETA 02:02:48Calculating loss...:  52%|█████▏    | 13/25 [00:08<00:07,  1.56it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.309 | Val 0.289 | ETA 02:02:48Calculating loss...:  56%|█████▌    | 14/25 [00:09<00:07,  1.55it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.309 | Val 0.289 | ETA 02:02:48Calculating loss...:  60%|██████    | 15/25 [00:09<00:06,  1.52it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.309 | Val 0.289 | ETA 02:02:48Calculating loss...:  64%|██████▍   | 16/25 [00:10<00:06,  1.40it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.309 | Val 0.289 | ETA 02:02:48Calculating loss...:  68%|██████▊   | 17/25 [00:11<00:05,  1.36it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.309 | Val 0.289 | ETA 02:02:48Calculating loss...:  72%|███████▏  | 18/25 [00:12<00:04,  1.44it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.309 | Val 0.289 | ETA 02:02:48Calculating loss...:  76%|███████▌  | 19/25 [00:12<00:04,  1.41it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.309 | Val 0.289 | ETA 02:02:48Calculating loss...:  80%|████████  | 20/25 [00:13<00:03,  1.36it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.309 | Val 0.289 | ETA 02:02:48Calculating loss...:  84%|████████▍ | 21/25 [00:14<00:02,  1.43it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.309 | Val 0.289 | ETA 02:02:48Calculating loss...:  88%|████████▊ | 22/25 [00:14<00:02,  1.48it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.309 | Val 0.289 | ETA 02:02:48Calculating loss...:  92%|█████████▏| 23/25 [00:15<00:01,  1.51it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.309 | Val 0.289 | ETA 02:02:48Calculating loss...:  96%|█████████▌| 24/25 [00:16<00:00,  1.59it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.309 | Val 0.289 | ETA 02:02:48Calculating loss...: 100%|██████████| 25/25 [00:16<00:00,  1.47it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.309 | Val 0.289 | ETA 02:02:48Calculating loss...: 100%|██████████| 25/25 [00:16<00:00,  1.49it/s]\n",
      "Run (48.6% total) | Local 3400/7000 | Global 3400/7000 | LR 2.0e-04 | Train 0.309 | Val 0.289 | ETA 02:02:48Iter 3500: Val loss 0.302, Val took 16.805s\n",
      "Run (50.0% total) | Local 3500/7000 | Global 3500/7000 | LR 2.0e-04 | Train 0.309 | Val 0.302 | ETA 02:01:25Iter 3500: Train loss 0.308, Learning Rate 2.000e-04, It/sec 0.496, Tokens/sec 539.296, Trained Tokens 3763554, Peak mem 3.723 GB\n",
      "Run (50.0% total) | Local 3500/7000 | Global 3500/7000 | LR 2.0e-04 | Train 0.308 | Val 0.302 | ETA 02:01:25Iter 3500: Saved adapter weights to ACLED_llama_fine_tuned/adapters_optimised/adapters.safetensors and ACLED_llama_fine_tuned/adapters_optimised/0003500_adapters.safetensors.\n",
      "Run (50.0% total) | Local 3500/7000 | Global 3500/7000 | LR 2.0e-04 | Train 0.308 | Val 0.302 | ETA 02:01:25Iter 3600: Train loss 0.312, Learning Rate 2.000e-04, It/sec 0.522, Tokens/sec 554.030, Trained Tokens 3869692, Peak mem 3.723 GB\n",
      "Run (51.4% total) | Local 3600/7000 | Global 3600/7000 | LR 2.0e-04 | Train 0.312 | Val 0.302 | ETA 01:55:09Iter 3700: Train loss 0.290, Learning Rate 2.000e-04, It/sec 0.487, Tokens/sec 522.069, Trained Tokens 3976948, Peak mem 3.723 GB\n",
      "Run (52.9% total) | Local 3700/7000 | Global 3700/7000 | LR 2.0e-04 | Train 0.290 | Val 0.302 | ETA 01:52:09Iter 3800: Train loss 0.297, Learning Rate 2.000e-04, It/sec 0.520, Tokens/sec 563.747, Trained Tokens 4085372, Peak mem 3.723 GB\n",
      "Run (54.3% total) | Local 3800/7000 | Global 3800/7000 | LR 2.0e-04 | Train 0.297 | Val 0.302 | ETA 02:06:33Iter 3900: Train loss 0.314, Learning Rate 2.000e-04, It/sec 0.480, Tokens/sec 521.870, Trained Tokens 4194047, Peak mem 3.723 GB\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.314 | Val 0.302 | ETA 01:58:07Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.314 | Val 0.302 | ETA 01:58:07Calculating loss...:   4%|▍         | 1/25 [00:00<00:14,  1.68it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.314 | Val 0.302 | ETA 01:58:07Calculating loss...:   8%|▊         | 2/25 [00:01<00:13,  1.71it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.314 | Val 0.302 | ETA 01:58:07Calculating loss...:  12%|█▏        | 3/25 [00:01<00:13,  1.65it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.314 | Val 0.302 | ETA 01:58:07Calculating loss...:  16%|█▌        | 4/25 [00:02<00:13,  1.57it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.314 | Val 0.302 | ETA 01:58:07Calculating loss...:  20%|██        | 5/25 [00:03<00:14,  1.38it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.314 | Val 0.302 | ETA 01:58:07Calculating loss...:  24%|██▍       | 6/25 [00:03<00:13,  1.45it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.314 | Val 0.302 | ETA 01:58:07Calculating loss...:  28%|██▊       | 7/25 [00:04<00:12,  1.45it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.314 | Val 0.302 | ETA 01:58:07Calculating loss...:  32%|███▏      | 8/25 [00:05<00:11,  1.46it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.314 | Val 0.302 | ETA 01:58:07Calculating loss...:  36%|███▌      | 9/25 [00:05<00:10,  1.53it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.314 | Val 0.302 | ETA 01:58:07Calculating loss...:  40%|████      | 10/25 [00:06<00:10,  1.47it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.314 | Val 0.302 | ETA 01:58:07Calculating loss...:  44%|████▍     | 11/25 [00:07<00:09,  1.51it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.314 | Val 0.302 | ETA 01:58:07Calculating loss...:  48%|████▊     | 12/25 [00:08<00:08,  1.46it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.314 | Val 0.302 | ETA 01:58:07Calculating loss...:  52%|█████▏    | 13/25 [00:08<00:08,  1.38it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.314 | Val 0.302 | ETA 01:58:07Calculating loss...:  56%|█████▌    | 14/25 [00:09<00:07,  1.44it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.314 | Val 0.302 | ETA 01:58:07Calculating loss...:  60%|██████    | 15/25 [00:10<00:06,  1.45it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.314 | Val 0.302 | ETA 01:58:07Calculating loss...:  64%|██████▍   | 16/25 [00:10<00:06,  1.49it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.314 | Val 0.302 | ETA 01:58:07Calculating loss...:  68%|██████▊   | 17/25 [00:11<00:05,  1.52it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.314 | Val 0.302 | ETA 01:58:07Calculating loss...:  72%|███████▏  | 18/25 [00:12<00:04,  1.46it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.314 | Val 0.302 | ETA 01:58:07Calculating loss...:  76%|███████▌  | 19/25 [00:12<00:03,  1.50it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.314 | Val 0.302 | ETA 01:58:07Calculating loss...:  80%|████████  | 20/25 [00:13<00:03,  1.58it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.314 | Val 0.302 | ETA 01:58:07Calculating loss...:  84%|████████▍ | 21/25 [00:13<00:02,  1.59it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.314 | Val 0.302 | ETA 01:58:07Calculating loss...:  88%|████████▊ | 22/25 [00:14<00:01,  1.65it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.314 | Val 0.302 | ETA 01:58:07Calculating loss...:  92%|█████████▏| 23/25 [00:15<00:01,  1.46it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.314 | Val 0.302 | ETA 01:58:07Calculating loss...:  96%|█████████▌| 24/25 [00:15<00:00,  1.54it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.314 | Val 0.302 | ETA 01:58:07Calculating loss...: 100%|██████████| 25/25 [00:16<00:00,  1.66it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.314 | Val 0.302 | ETA 01:58:07Calculating loss...: 100%|██████████| 25/25 [00:16<00:00,  1.52it/s]\n",
      "Run (55.7% total) | Local 3900/7000 | Global 3900/7000 | LR 2.0e-04 | Train 0.314 | Val 0.302 | ETA 01:58:07Iter 4000: Val loss 0.314, Val took 16.453s\n",
      "Run (57.1% total) | Local 4000/7000 | Global 4000/7000 | LR 2.0e-04 | Train 0.314 | Val 0.314 | ETA 01:50:53Iter 4000: Train loss 0.287, Learning Rate 2.000e-04, It/sec 0.524, Tokens/sec 558.262, Trained Tokens 4300596, Peak mem 3.723 GB\n",
      "Run (57.1% total) | Local 4000/7000 | Global 4000/7000 | LR 2.0e-04 | Train 0.287 | Val 0.314 | ETA 01:50:53Iter 4000: Saved adapter weights to ACLED_llama_fine_tuned/adapters_optimised/adapters.safetensors and ACLED_llama_fine_tuned/adapters_optimised/0004000_adapters.safetensors.\n",
      "Run (57.1% total) | Local 4000/7000 | Global 4000/7000 | LR 2.0e-04 | Train 0.287 | Val 0.314 | ETA 01:50:53Iter 4100: Train loss 0.298, Learning Rate 2.000e-04, It/sec 0.501, Tokens/sec 543.328, Trained Tokens 4408966, Peak mem 3.723 GB\n",
      "Run (58.6% total) | Local 4100/7000 | Global 4100/7000 | LR 2.0e-04 | Train 0.298 | Val 0.314 | ETA 01:43:58Iter 4200: Train loss 0.286, Learning Rate 2.000e-04, It/sec 0.504, Tokens/sec 550.805, Trained Tokens 4518305, Peak mem 3.723 GB\n",
      "Run (60.0% total) | Local 4200/7000 | Global 4200/7000 | LR 2.0e-04 | Train 0.286 | Val 0.314 | ETA 01:38:05Iter 4300: Train loss 0.307, Learning Rate 2.000e-04, It/sec 0.514, Tokens/sec 553.565, Trained Tokens 4626099, Peak mem 3.723 GB\n",
      "Run (61.4% total) | Local 4300/7000 | Global 4300/7000 | LR 2.0e-04 | Train 0.307 | Val 0.314 | ETA 01:32:30Iter 4400: Train loss 0.278, Learning Rate 2.000e-04, It/sec 0.525, Tokens/sec 558.218, Trained Tokens 4732468, Peak mem 3.723 GB\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.278 | Val 0.314 | ETA 01:27:09Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.278 | Val 0.314 | ETA 01:27:09Calculating loss...:   4%|▍         | 1/25 [00:00<00:16,  1.42it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.278 | Val 0.314 | ETA 01:27:09Calculating loss...:   8%|▊         | 2/25 [00:01<00:16,  1.40it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.278 | Val 0.314 | ETA 01:27:09Calculating loss...:  12%|█▏        | 3/25 [00:02<00:16,  1.30it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.278 | Val 0.314 | ETA 01:27:09Calculating loss...:  16%|█▌        | 4/25 [00:02<00:13,  1.51it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.278 | Val 0.314 | ETA 01:27:09Calculating loss...:  20%|██        | 5/25 [00:03<00:12,  1.61it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.278 | Val 0.314 | ETA 01:27:09Calculating loss...:  24%|██▍       | 6/25 [00:03<00:12,  1.56it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.278 | Val 0.314 | ETA 01:27:09Calculating loss...:  28%|██▊       | 7/25 [00:04<00:12,  1.44it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.278 | Val 0.314 | ETA 01:27:09Calculating loss...:  32%|███▏      | 8/25 [00:05<00:11,  1.45it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.278 | Val 0.314 | ETA 01:27:09Calculating loss...:  36%|███▌      | 9/25 [00:06<00:10,  1.50it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.278 | Val 0.314 | ETA 01:27:09Calculating loss...:  40%|████      | 10/25 [00:06<00:10,  1.40it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.278 | Val 0.314 | ETA 01:27:09Calculating loss...:  44%|████▍     | 11/25 [00:07<00:09,  1.54it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.278 | Val 0.314 | ETA 01:27:09Calculating loss...:  48%|████▊     | 12/25 [00:08<00:08,  1.47it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.278 | Val 0.314 | ETA 01:27:09Calculating loss...:  52%|█████▏    | 13/25 [00:08<00:07,  1.51it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.278 | Val 0.314 | ETA 01:27:09Calculating loss...:  56%|█████▌    | 14/25 [00:09<00:06,  1.58it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.278 | Val 0.314 | ETA 01:27:09Calculating loss...:  60%|██████    | 15/25 [00:09<00:06,  1.59it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.278 | Val 0.314 | ETA 01:27:09Calculating loss...:  64%|██████▍   | 16/25 [00:10<00:05,  1.59it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.278 | Val 0.314 | ETA 01:27:09Calculating loss...:  68%|██████▊   | 17/25 [00:11<00:05,  1.55it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.278 | Val 0.314 | ETA 01:27:09Calculating loss...:  72%|███████▏  | 18/25 [00:11<00:04,  1.57it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.278 | Val 0.314 | ETA 01:27:09Calculating loss...:  76%|███████▌  | 19/25 [00:12<00:03,  1.62it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.278 | Val 0.314 | ETA 01:27:09Calculating loss...:  80%|████████  | 20/25 [00:13<00:03,  1.48it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.278 | Val 0.314 | ETA 01:27:09Calculating loss...:  84%|████████▍ | 21/25 [00:13<00:02,  1.61it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.278 | Val 0.314 | ETA 01:27:09Calculating loss...:  88%|████████▊ | 22/25 [00:14<00:01,  1.59it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.278 | Val 0.314 | ETA 01:27:09Calculating loss...:  92%|█████████▏| 23/25 [00:15<00:01,  1.61it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.278 | Val 0.314 | ETA 01:27:09Calculating loss...:  96%|█████████▌| 24/25 [00:15<00:00,  1.48it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.278 | Val 0.314 | ETA 01:27:09Calculating loss...: 100%|██████████| 25/25 [00:16<00:00,  1.52it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.278 | Val 0.314 | ETA 01:27:09Calculating loss...: 100%|██████████| 25/25 [00:16<00:00,  1.52it/s]\n",
      "Run (62.9% total) | Local 4400/7000 | Global 4400/7000 | LR 2.0e-04 | Train 0.278 | Val 0.314 | ETA 01:27:09Iter 4500: Val loss 0.283, Val took 16.429s\n",
      "Run (64.3% total) | Local 4500/7000 | Global 4500/7000 | LR 2.0e-04 | Train 0.278 | Val 0.283 | ETA 01:24:55Iter 4500: Train loss 0.314, Learning Rate 2.000e-04, It/sec 0.510, Tokens/sec 549.696, Trained Tokens 4840292, Peak mem 3.723 GB\n",
      "Run (64.3% total) | Local 4500/7000 | Global 4500/7000 | LR 2.0e-04 | Train 0.314 | Val 0.283 | ETA 01:24:55Iter 4500: Saved adapter weights to ACLED_llama_fine_tuned/adapters_optimised/adapters.safetensors and ACLED_llama_fine_tuned/adapters_optimised/0004500_adapters.safetensors.\n",
      "Run (64.3% total) | Local 4500/7000 | Global 4500/7000 | LR 2.0e-04 | Train 0.314 | Val 0.283 | ETA 01:24:55Iter 4600: Train loss 0.306, Learning Rate 2.000e-04, It/sec 0.511, Tokens/sec 552.434, Trained Tokens 4948475, Peak mem 3.723 GB\n",
      "Run (65.7% total) | Local 4600/7000 | Global 4600/7000 | LR 2.0e-04 | Train 0.306 | Val 0.283 | ETA 01:20:35Iter 4700: Train loss 0.312, Learning Rate 2.000e-04, It/sec 0.500, Tokens/sec 535.803, Trained Tokens 5055537, Peak mem 3.723 GB\n",
      "Run (67.1% total) | Local 4700/7000 | Global 4700/7000 | LR 2.0e-04 | Train 0.312 | Val 0.283 | ETA 01:17:03Iter 4800: Train loss 0.289, Learning Rate 2.000e-04, It/sec 0.492, Tokens/sec 535.943, Trained Tokens 5164448, Peak mem 3.723 GB\n",
      "Run (68.6% total) | Local 4800/7000 | Global 4800/7000 | LR 2.0e-04 | Train 0.289 | Val 0.283 | ETA 01:13:58Iter 4900: Train loss 0.296, Learning Rate 2.000e-04, It/sec 0.523, Tokens/sec 557.109, Trained Tokens 5270878, Peak mem 3.723 GB\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.296 | Val 0.283 | ETA 01:09:29Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.296 | Val 0.283 | ETA 01:09:29Calculating loss...:   4%|▍         | 1/25 [00:00<00:18,  1.27it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.296 | Val 0.283 | ETA 01:09:29Calculating loss...:   8%|▊         | 2/25 [00:01<00:15,  1.44it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.296 | Val 0.283 | ETA 01:09:29Calculating loss...:  12%|█▏        | 3/25 [00:02<00:16,  1.31it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.296 | Val 0.283 | ETA 01:09:29Calculating loss...:  16%|█▌        | 4/25 [00:03<00:16,  1.26it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.296 | Val 0.283 | ETA 01:09:29Calculating loss...:  20%|██        | 5/25 [00:03<00:14,  1.40it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.296 | Val 0.283 | ETA 01:09:29Calculating loss...:  24%|██▍       | 6/25 [00:04<00:15,  1.24it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.296 | Val 0.283 | ETA 01:09:29Calculating loss...:  28%|██▊       | 7/25 [00:05<00:15,  1.16it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.296 | Val 0.283 | ETA 01:09:29Calculating loss...:  32%|███▏      | 8/25 [00:06<00:13,  1.30it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.296 | Val 0.283 | ETA 01:09:29Calculating loss...:  36%|███▌      | 9/25 [00:06<00:10,  1.46it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.296 | Val 0.283 | ETA 01:09:29Calculating loss...:  40%|████      | 10/25 [00:07<00:10,  1.44it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.296 | Val 0.283 | ETA 01:09:29Calculating loss...:  44%|████▍     | 11/25 [00:08<00:10,  1.38it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.296 | Val 0.283 | ETA 01:09:29Calculating loss...:  48%|████▊     | 12/25 [00:08<00:09,  1.44it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.296 | Val 0.283 | ETA 01:09:29Calculating loss...:  52%|█████▏    | 13/25 [00:09<00:07,  1.52it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.296 | Val 0.283 | ETA 01:09:29Calculating loss...:  56%|█████▌    | 14/25 [00:10<00:07,  1.54it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.296 | Val 0.283 | ETA 01:09:29Calculating loss...:  60%|██████    | 15/25 [00:10<00:06,  1.51it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.296 | Val 0.283 | ETA 01:09:29Calculating loss...:  64%|██████▍   | 16/25 [00:11<00:06,  1.49it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.296 | Val 0.283 | ETA 01:09:29Calculating loss...:  68%|██████▊   | 17/25 [00:12<00:05,  1.52it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.296 | Val 0.283 | ETA 01:09:29Calculating loss...:  72%|███████▏  | 18/25 [00:12<00:04,  1.64it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.296 | Val 0.283 | ETA 01:09:29Calculating loss...:  76%|███████▌  | 19/25 [00:13<00:04,  1.50it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.296 | Val 0.283 | ETA 01:09:29Calculating loss...:  80%|████████  | 20/25 [00:14<00:03,  1.40it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.296 | Val 0.283 | ETA 01:09:29Calculating loss...:  84%|████████▍ | 21/25 [00:15<00:03,  1.20it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.296 | Val 0.283 | ETA 01:09:29Calculating loss...:  88%|████████▊ | 22/25 [00:15<00:02,  1.33it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.296 | Val 0.283 | ETA 01:09:29Calculating loss...:  92%|█████████▏| 23/25 [00:16<00:01,  1.40it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.296 | Val 0.283 | ETA 01:09:29Calculating loss...:  96%|█████████▌| 24/25 [00:17<00:00,  1.34it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.296 | Val 0.283 | ETA 01:09:29Calculating loss...: 100%|██████████| 25/25 [00:17<00:00,  1.44it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.296 | Val 0.283 | ETA 01:09:29Calculating loss...: 100%|██████████| 25/25 [00:17<00:00,  1.40it/s]\n",
      "Run (70.0% total) | Local 4900/7000 | Global 4900/7000 | LR 2.0e-04 | Train 0.296 | Val 0.283 | ETA 01:09:29Iter 5000: Val loss 0.281, Val took 17.880s\n",
      "Run (71.4% total) | Local 5000/7000 | Global 5000/7000 | LR 2.0e-04 | Train 0.296 | Val 0.281 | ETA 01:09:05Iter 5000: Train loss 0.302, Learning Rate 2.000e-04, It/sec 0.473, Tokens/sec 508.977, Trained Tokens 5378575, Peak mem 3.723 GB\n",
      "Run (71.4% total) | Local 5000/7000 | Global 5000/7000 | LR 2.0e-04 | Train 0.302 | Val 0.281 | ETA 01:09:05Iter 5000: Saved adapter weights to ACLED_llama_fine_tuned/adapters_optimised/adapters.safetensors and ACLED_llama_fine_tuned/adapters_optimised/0005000_adapters.safetensors.\n",
      "Run (71.4% total) | Local 5000/7000 | Global 5000/7000 | LR 2.0e-04 | Train 0.302 | Val 0.281 | ETA 01:09:05Iter 5100: Train loss 0.277, Learning Rate 2.000e-04, It/sec 0.494, Tokens/sec 529.107, Trained Tokens 5485649, Peak mem 3.723 GB\n",
      "Run (72.9% total) | Local 5100/7000 | Global 5100/7000 | LR 2.0e-04 | Train 0.277 | Val 0.281 | ETA 01:05:10Iter 5200: Train loss 0.283, Learning Rate 2.000e-04, It/sec 0.506, Tokens/sec 550.492, Trained Tokens 5594420, Peak mem 3.723 GB\n",
      "Run (74.3% total) | Local 5200/7000 | Global 5200/7000 | LR 2.0e-04 | Train 0.283 | Val 0.281 | ETA 01:01:00Iter 5300: Train loss 0.279, Learning Rate 2.000e-04, It/sec 0.478, Tokens/sec 509.434, Trained Tokens 5700940, Peak mem 3.723 GB\n",
      "Run (75.7% total) | Local 5300/7000 | Global 5300/7000 | LR 2.0e-04 | Train 0.279 | Val 0.281 | ETA 00:58:06Iter 5400: Train loss 0.276, Learning Rate 2.000e-04, It/sec 0.491, Tokens/sec 536.609, Trained Tokens 5810323, Peak mem 3.723 GB\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.276 | Val 0.281 | ETA 00:54:35Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.276 | Val 0.281 | ETA 00:54:35Calculating loss...:   4%|▍         | 1/25 [00:00<00:22,  1.06it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.276 | Val 0.281 | ETA 00:54:35Calculating loss...:   8%|▊         | 2/25 [00:01<00:15,  1.47it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.276 | Val 0.281 | ETA 00:54:35Calculating loss...:  12%|█▏        | 3/25 [00:02<00:16,  1.30it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.276 | Val 0.281 | ETA 00:54:35Calculating loss...:  16%|█▌        | 4/25 [00:02<00:15,  1.39it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.276 | Val 0.281 | ETA 00:54:35Calculating loss...:  20%|██        | 5/25 [00:03<00:13,  1.51it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.276 | Val 0.281 | ETA 00:54:35Calculating loss...:  24%|██▍       | 6/25 [00:04<00:12,  1.54it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.276 | Val 0.281 | ETA 00:54:35Calculating loss...:  28%|██▊       | 7/25 [00:04<00:11,  1.55it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.276 | Val 0.281 | ETA 00:54:35Calculating loss...:  32%|███▏      | 8/25 [00:05<00:10,  1.62it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.276 | Val 0.281 | ETA 00:54:35Calculating loss...:  36%|███▌      | 9/25 [00:05<00:09,  1.67it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.276 | Val 0.281 | ETA 00:54:35Calculating loss...:  40%|████      | 10/25 [00:06<00:09,  1.55it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.276 | Val 0.281 | ETA 00:54:35Calculating loss...:  44%|████▍     | 11/25 [00:07<00:09,  1.52it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.276 | Val 0.281 | ETA 00:54:35Calculating loss...:  48%|████▊     | 12/25 [00:08<00:08,  1.46it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.276 | Val 0.281 | ETA 00:54:35Calculating loss...:  52%|█████▏    | 13/25 [00:08<00:08,  1.34it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.276 | Val 0.281 | ETA 00:54:35Calculating loss...:  56%|█████▌    | 14/25 [00:09<00:08,  1.27it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.276 | Val 0.281 | ETA 00:54:35Calculating loss...:  60%|██████    | 15/25 [00:10<00:07,  1.35it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.276 | Val 0.281 | ETA 00:54:35Calculating loss...:  64%|██████▍   | 16/25 [00:11<00:06,  1.42it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.276 | Val 0.281 | ETA 00:54:35Calculating loss...:  68%|██████▊   | 17/25 [00:11<00:05,  1.48it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.276 | Val 0.281 | ETA 00:54:35Calculating loss...:  72%|███████▏  | 18/25 [00:12<00:04,  1.51it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.276 | Val 0.281 | ETA 00:54:35Calculating loss...:  76%|███████▌  | 19/25 [00:12<00:03,  1.54it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.276 | Val 0.281 | ETA 00:54:35Calculating loss...:  80%|████████  | 20/25 [00:13<00:03,  1.43it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.276 | Val 0.281 | ETA 00:54:35Calculating loss...:  84%|████████▍ | 21/25 [00:14<00:02,  1.48it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.276 | Val 0.281 | ETA 00:54:35Calculating loss...:  88%|████████▊ | 22/25 [00:15<00:02,  1.42it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.276 | Val 0.281 | ETA 00:54:35Calculating loss...:  92%|█████████▏| 23/25 [00:15<00:01,  1.51it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.276 | Val 0.281 | ETA 00:54:35Calculating loss...:  96%|█████████▌| 24/25 [00:16<00:00,  1.59it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.276 | Val 0.281 | ETA 00:54:35Calculating loss...: 100%|██████████| 25/25 [00:16<00:00,  1.65it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.276 | Val 0.281 | ETA 00:54:35Calculating loss...: 100%|██████████| 25/25 [00:16<00:00,  1.49it/s]\n",
      "Run (77.1% total) | Local 5400/7000 | Global 5400/7000 | LR 2.0e-04 | Train 0.276 | Val 0.281 | ETA 00:54:35Iter 5500: Val loss 0.273, Val took 16.832s\n",
      "Run (78.6% total) | Local 5500/7000 | Global 5500/7000 | LR 2.0e-04 | Train 0.276 | Val 0.273 | ETA 00:51:49Iter 5500: Train loss 0.271, Learning Rate 2.000e-04, It/sec 0.505, Tokens/sec 537.434, Trained Tokens 5916840, Peak mem 3.723 GB\n",
      "Run (78.6% total) | Local 5500/7000 | Global 5500/7000 | LR 2.0e-04 | Train 0.271 | Val 0.273 | ETA 00:51:49Iter 5500: Saved adapter weights to ACLED_llama_fine_tuned/adapters_optimised/adapters.safetensors and ACLED_llama_fine_tuned/adapters_optimised/0005500_adapters.safetensors.\n",
      "Run (78.6% total) | Local 5500/7000 | Global 5500/7000 | LR 2.0e-04 | Train 0.271 | Val 0.273 | ETA 00:51:49Iter 5600: Train loss 0.276, Learning Rate 2.000e-04, It/sec 0.498, Tokens/sec 537.011, Trained Tokens 6024585, Peak mem 3.723 GB\n",
      "Run (80.0% total) | Local 5600/7000 | Global 5600/7000 | LR 2.0e-04 | Train 0.276 | Val 0.273 | ETA 00:47:53Iter 5700: Train loss 0.284, Learning Rate 2.000e-04, It/sec 0.495, Tokens/sec 538.776, Trained Tokens 6133452, Peak mem 3.723 GB\n",
      "Run (81.4% total) | Local 5700/7000 | Global 5700/7000 | LR 2.0e-04 | Train 0.284 | Val 0.273 | ETA 00:44:16Iter 5800: Train loss 0.277, Learning Rate 2.000e-04, It/sec 0.491, Tokens/sec 530.866, Trained Tokens 6241667, Peak mem 3.723 GB\n",
      "Run (82.9% total) | Local 5800/7000 | Global 5800/7000 | LR 2.0e-04 | Train 0.277 | Val 0.273 | ETA 00:40:50Iter 5900: Train loss 0.280, Learning Rate 2.000e-04, It/sec 0.495, Tokens/sec 537.849, Trained Tokens 6350321, Peak mem 3.723 GB\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.280 | Val 0.273 | ETA 00:37:18Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.280 | Val 0.273 | ETA 00:37:18Calculating loss...:   4%|▍         | 1/25 [00:00<00:13,  1.74it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.280 | Val 0.273 | ETA 00:37:18Calculating loss...:   8%|▊         | 2/25 [00:01<00:19,  1.19it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.280 | Val 0.273 | ETA 00:37:18Calculating loss...:  12%|█▏        | 3/25 [00:02<00:18,  1.20it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.280 | Val 0.273 | ETA 00:37:18Calculating loss...:  16%|█▌        | 4/25 [00:03<00:18,  1.13it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.280 | Val 0.273 | ETA 00:37:18Calculating loss...:  20%|██        | 5/25 [00:03<00:15,  1.30it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.280 | Val 0.273 | ETA 00:37:18Calculating loss...:  24%|██▍       | 6/25 [00:04<00:13,  1.39it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.280 | Val 0.273 | ETA 00:37:18Calculating loss...:  28%|██▊       | 7/25 [00:05<00:13,  1.37it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.280 | Val 0.273 | ETA 00:37:18Calculating loss...:  32%|███▏      | 8/25 [00:05<00:11,  1.48it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.280 | Val 0.273 | ETA 00:37:18Calculating loss...:  36%|███▌      | 9/25 [00:06<00:11,  1.43it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.280 | Val 0.273 | ETA 00:37:18Calculating loss...:  40%|████      | 10/25 [00:07<00:11,  1.36it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.280 | Val 0.273 | ETA 00:37:18Calculating loss...:  44%|████▍     | 11/25 [00:08<00:09,  1.41it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.280 | Val 0.273 | ETA 00:37:18Calculating loss...:  48%|████▊     | 12/25 [00:08<00:08,  1.50it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.280 | Val 0.273 | ETA 00:37:18Calculating loss...:  52%|█████▏    | 13/25 [00:09<00:07,  1.58it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.280 | Val 0.273 | ETA 00:37:18Calculating loss...:  56%|█████▌    | 14/25 [00:09<00:06,  1.59it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.280 | Val 0.273 | ETA 00:37:18Calculating loss...:  60%|██████    | 15/25 [00:10<00:06,  1.64it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.280 | Val 0.273 | ETA 00:37:18Calculating loss...:  64%|██████▍   | 16/25 [00:11<00:06,  1.49it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.280 | Val 0.273 | ETA 00:37:18Calculating loss...:  68%|██████▊   | 17/25 [00:11<00:04,  1.62it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.280 | Val 0.273 | ETA 00:37:18Calculating loss...:  72%|███████▏  | 18/25 [00:12<00:05,  1.36it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.280 | Val 0.273 | ETA 00:37:18Calculating loss...:  76%|███████▌  | 19/25 [00:13<00:04,  1.42it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.280 | Val 0.273 | ETA 00:37:18Calculating loss...:  80%|████████  | 20/25 [00:14<00:03,  1.31it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.280 | Val 0.273 | ETA 00:37:18Calculating loss...:  84%|████████▍ | 21/25 [00:15<00:03,  1.26it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.280 | Val 0.273 | ETA 00:37:18Calculating loss...:  88%|████████▊ | 22/25 [00:15<00:02,  1.37it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.280 | Val 0.273 | ETA 00:37:18Calculating loss...:  92%|█████████▏| 23/25 [00:16<00:01,  1.46it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.280 | Val 0.273 | ETA 00:37:18Calculating loss...:  96%|█████████▌| 24/25 [00:16<00:00,  1.46it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.280 | Val 0.273 | ETA 00:37:18Calculating loss...: 100%|██████████| 25/25 [00:17<00:00,  1.54it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.280 | Val 0.273 | ETA 00:37:18Calculating loss...: 100%|██████████| 25/25 [00:17<00:00,  1.43it/s]\n",
      "Run (84.3% total) | Local 5900/7000 | Global 5900/7000 | LR 2.0e-04 | Train 0.280 | Val 0.273 | ETA 00:37:18Iter 6000: Val loss 0.288, Val took 17.536s\n",
      "Run (85.7% total) | Local 6000/7000 | Global 6000/7000 | LR 2.0e-04 | Train 0.280 | Val 0.288 | ETA 00:34:30Iter 6000: Train loss 0.293, Learning Rate 2.000e-04, It/sec 0.502, Tokens/sec 544.314, Trained Tokens 6458836, Peak mem 3.723 GB\n",
      "Run (85.7% total) | Local 6000/7000 | Global 6000/7000 | LR 2.0e-04 | Train 0.293 | Val 0.288 | ETA 00:34:30Iter 6000: Saved adapter weights to ACLED_llama_fine_tuned/adapters_optimised/adapters.safetensors and ACLED_llama_fine_tuned/adapters_optimised/0006000_adapters.safetensors.\n",
      "Run (85.7% total) | Local 6000/7000 | Global 6000/7000 | LR 2.0e-04 | Train 0.293 | Val 0.288 | ETA 00:34:30Iter 6100: Train loss 0.275, Learning Rate 2.000e-04, It/sec 0.533, Tokens/sec 556.876, Trained Tokens 6563309, Peak mem 3.723 GB\n",
      "Run (87.1% total) | Local 6100/7000 | Global 6100/7000 | LR 2.0e-04 | Train 0.275 | Val 0.288 | ETA 00:30:11Iter 6200: Train loss 0.266, Learning Rate 2.000e-04, It/sec 0.483, Tokens/sec 528.101, Trained Tokens 6672618, Peak mem 3.723 GB\n",
      "Run (88.6% total) | Local 6200/7000 | Global 6200/7000 | LR 2.0e-04 | Train 0.266 | Val 0.288 | ETA 00:27:03Iter 6300: Train loss 0.287, Learning Rate 2.000e-04, It/sec 0.505, Tokens/sec 539.221, Trained Tokens 6779471, Peak mem 3.723 GB\n",
      "Run (90.0% total) | Local 6300/7000 | Global 6300/7000 | LR 2.0e-04 | Train 0.287 | Val 0.288 | ETA 00:23:30Iter 6400: Train loss 0.290, Learning Rate 2.000e-04, It/sec 0.486, Tokens/sec 537.427, Trained Tokens 6889965, Peak mem 3.723 GB\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.290 | Val 0.288 | ETA 00:20:16Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.290 | Val 0.288 | ETA 00:20:16Calculating loss...:   4%|▍         | 1/25 [00:00<00:15,  1.59it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.290 | Val 0.288 | ETA 00:20:16Calculating loss...:   8%|▊         | 2/25 [00:01<00:17,  1.34it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.290 | Val 0.288 | ETA 00:20:16Calculating loss...:  12%|█▏        | 3/25 [00:02<00:16,  1.33it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.290 | Val 0.288 | ETA 00:20:16Calculating loss...:  16%|█▌        | 4/25 [00:02<00:15,  1.33it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.290 | Val 0.288 | ETA 00:20:16Calculating loss...:  20%|██        | 5/25 [00:03<00:15,  1.28it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.290 | Val 0.288 | ETA 00:20:16Calculating loss...:  24%|██▍       | 6/25 [00:04<00:15,  1.19it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.290 | Val 0.288 | ETA 00:20:16Calculating loss...:  28%|██▊       | 7/25 [00:05<00:13,  1.33it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.290 | Val 0.288 | ETA 00:20:16Calculating loss...:  32%|███▏      | 8/25 [00:06<00:13,  1.27it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.290 | Val 0.288 | ETA 00:20:16Calculating loss...:  36%|███▌      | 9/25 [00:06<00:12,  1.29it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.290 | Val 0.288 | ETA 00:20:16Calculating loss...:  40%|████      | 10/25 [00:07<00:10,  1.41it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.290 | Val 0.288 | ETA 00:20:16Calculating loss...:  44%|████▍     | 11/25 [00:08<00:10,  1.39it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.290 | Val 0.288 | ETA 00:20:16Calculating loss...:  48%|████▊     | 12/25 [00:08<00:08,  1.47it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.290 | Val 0.288 | ETA 00:20:16Calculating loss...:  52%|█████▏    | 13/25 [00:09<00:07,  1.55it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.290 | Val 0.288 | ETA 00:20:16Calculating loss...:  56%|█████▌    | 14/25 [00:09<00:06,  1.62it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.290 | Val 0.288 | ETA 00:20:16Calculating loss...:  60%|██████    | 15/25 [00:10<00:06,  1.62it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.290 | Val 0.288 | ETA 00:20:16Calculating loss...:  64%|██████▍   | 16/25 [00:11<00:05,  1.72it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.290 | Val 0.288 | ETA 00:20:16Calculating loss...:  68%|██████▊   | 17/25 [00:11<00:05,  1.50it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.290 | Val 0.288 | ETA 00:20:16Calculating loss...:  72%|███████▏  | 18/25 [00:12<00:04,  1.52it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.290 | Val 0.288 | ETA 00:20:16Calculating loss...:  76%|███████▌  | 19/25 [00:13<00:03,  1.55it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.290 | Val 0.288 | ETA 00:20:16Calculating loss...:  80%|████████  | 20/25 [00:13<00:03,  1.56it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.290 | Val 0.288 | ETA 00:20:16Calculating loss...:  84%|████████▍ | 21/25 [00:14<00:02,  1.62it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.290 | Val 0.288 | ETA 00:20:16Calculating loss...:  88%|████████▊ | 22/25 [00:15<00:02,  1.48it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.290 | Val 0.288 | ETA 00:20:16Calculating loss...:  92%|█████████▏| 23/25 [00:15<00:01,  1.51it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.290 | Val 0.288 | ETA 00:20:16Calculating loss...:  96%|█████████▌| 24/25 [00:16<00:00,  1.58it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.290 | Val 0.288 | ETA 00:20:16Calculating loss...: 100%|██████████| 25/25 [00:16<00:00,  1.59it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.290 | Val 0.288 | ETA 00:20:16Calculating loss...: 100%|██████████| 25/25 [00:16<00:00,  1.47it/s]\n",
      "Run (91.4% total) | Local 6400/7000 | Global 6400/7000 | LR 2.0e-04 | Train 0.290 | Val 0.288 | ETA 00:20:16Iter 6500: Val loss 0.289, Val took 16.998s\n",
      "Run (92.9% total) | Local 6500/7000 | Global 6500/7000 | LR 2.0e-04 | Train 0.290 | Val 0.289 | ETA 00:17:06Iter 6500: Train loss 0.276, Learning Rate 2.000e-04, It/sec 0.510, Tokens/sec 542.521, Trained Tokens 6996384, Peak mem 3.723 GB\n",
      "Run (92.9% total) | Local 6500/7000 | Global 6500/7000 | LR 2.0e-04 | Train 0.276 | Val 0.289 | ETA 00:17:06Iter 6500: Saved adapter weights to ACLED_llama_fine_tuned/adapters_optimised/adapters.safetensors and ACLED_llama_fine_tuned/adapters_optimised/0006500_adapters.safetensors.\n",
      "Run (92.9% total) | Local 6500/7000 | Global 6500/7000 | LR 2.0e-04 | Train 0.276 | Val 0.289 | ETA 00:17:06Iter 6600: Train loss 0.275, Learning Rate 2.000e-04, It/sec 0.488, Tokens/sec 533.112, Trained Tokens 7105591, Peak mem 3.723 GB\n",
      "Run (94.3% total) | Local 6600/7000 | Global 6600/7000 | LR 2.0e-04 | Train 0.275 | Val 0.289 | ETA 00:13:40Iter 6700: Train loss 0.269, Learning Rate 2.000e-04, It/sec 0.489, Tokens/sec 524.785, Trained Tokens 7212929, Peak mem 3.723 GB\n",
      "Run (95.7% total) | Local 6700/7000 | Global 6700/7000 | LR 2.0e-04 | Train 0.269 | Val 0.289 | ETA 00:10:15Iter 6800: Train loss 0.285, Learning Rate 2.000e-04, It/sec 0.503, Tokens/sec 540.914, Trained Tokens 7320520, Peak mem 3.723 GB\n",
      "Run (97.1% total) | Local 6800/7000 | Global 6800/7000 | LR 2.0e-04 | Train 0.285 | Val 0.289 | ETA 00:06:46Iter 6900: Train loss 0.264, Learning Rate 2.000e-04, It/sec 0.516, Tokens/sec 543.540, Trained Tokens 7425788, Peak mem 3.723 GB\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.264 | Val 0.289 | ETA 00:03:20Calculating loss...:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.264 | Val 0.289 | ETA 00:03:20Calculating loss...:   4%|▍         | 1/25 [00:00<00:18,  1.33it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.264 | Val 0.289 | ETA 00:03:20Calculating loss...:   8%|▊         | 2/25 [00:01<00:14,  1.55it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.264 | Val 0.289 | ETA 00:03:20Calculating loss...:  12%|█▏        | 3/25 [00:01<00:13,  1.64it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.264 | Val 0.289 | ETA 00:03:20Calculating loss...:  16%|█▌        | 4/25 [00:02<00:12,  1.70it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.264 | Val 0.289 | ETA 00:03:20Calculating loss...:  20%|██        | 5/25 [00:03<00:11,  1.73it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.264 | Val 0.289 | ETA 00:03:20Calculating loss...:  24%|██▍       | 6/25 [00:03<00:12,  1.48it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.264 | Val 0.289 | ETA 00:03:20Calculating loss...:  28%|██▊       | 7/25 [00:04<00:11,  1.57it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.264 | Val 0.289 | ETA 00:03:20Calculating loss...:  32%|███▏      | 8/25 [00:05<00:12,  1.41it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.264 | Val 0.289 | ETA 00:03:20Calculating loss...:  36%|███▌      | 9/25 [00:06<00:11,  1.35it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.264 | Val 0.289 | ETA 00:03:20Calculating loss...:  40%|████      | 10/25 [00:06<00:10,  1.41it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.264 | Val 0.289 | ETA 00:03:20Calculating loss...:  44%|████▍     | 11/25 [00:07<00:10,  1.35it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.264 | Val 0.289 | ETA 00:03:20Calculating loss...:  48%|████▊     | 12/25 [00:08<00:08,  1.46it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.264 | Val 0.289 | ETA 00:03:20Calculating loss...:  52%|█████▏    | 13/25 [00:08<00:07,  1.55it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.264 | Val 0.289 | ETA 00:03:20Calculating loss...:  56%|█████▌    | 14/25 [00:09<00:06,  1.62it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.264 | Val 0.289 | ETA 00:03:20Calculating loss...:  60%|██████    | 15/25 [00:09<00:05,  1.72it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.264 | Val 0.289 | ETA 00:03:20Calculating loss...:  64%|██████▍   | 16/25 [00:10<00:05,  1.54it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.264 | Val 0.289 | ETA 00:03:20Calculating loss...:  68%|██████▊   | 17/25 [00:11<00:05,  1.43it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.264 | Val 0.289 | ETA 00:03:20Calculating loss...:  72%|███████▏  | 18/25 [00:11<00:04,  1.48it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.264 | Val 0.289 | ETA 00:03:20Calculating loss...:  76%|███████▌  | 19/25 [00:12<00:04,  1.39it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.264 | Val 0.289 | ETA 00:03:20Calculating loss...:  80%|████████  | 20/25 [00:13<00:03,  1.34it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.264 | Val 0.289 | ETA 00:03:20Calculating loss...:  84%|████████▍ | 21/25 [00:14<00:02,  1.41it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.264 | Val 0.289 | ETA 00:03:20Calculating loss...:  88%|████████▊ | 22/25 [00:14<00:01,  1.51it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.264 | Val 0.289 | ETA 00:03:20Calculating loss...:  92%|█████████▏| 23/25 [00:15<00:01,  1.45it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.264 | Val 0.289 | ETA 00:03:20Calculating loss...:  96%|█████████▌| 24/25 [00:16<00:00,  1.54it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.264 | Val 0.289 | ETA 00:03:20Calculating loss...: 100%|██████████| 25/25 [00:16<00:00,  1.61it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.264 | Val 0.289 | ETA 00:03:20Calculating loss...: 100%|██████████| 25/25 [00:16<00:00,  1.50it/s]\n",
      "Run (98.6% total) | Local 6900/7000 | Global 6900/7000 | LR 2.0e-04 | Train 0.264 | Val 0.289 | ETA 00:03:20Iter 7000: Val loss 0.255, Val took 16.621s\n",
      "Run (100.0% total) | Local 7000/7000 | Global 7000/7000 | LR 2.0e-04 | Train 0.264 | Val 0.255 | ETA 00:00:00Iter 7000: Train loss 0.262, Learning Rate 2.000e-04, It/sec 0.485, Tokens/sec 525.146, Trained Tokens 7533993, Peak mem 3.723 GB\n",
      "Run (100.0% total) | Local 7000/7000 | Global 7000/7000 | LR 2.0e-04 | Train 0.262 | Val 0.255 | ETA 00:00:00Iter 7000: Saved adapter weights to ACLED_llama_fine_tuned/adapters_optimised/adapters.safetensors and ACLED_llama_fine_tuned/adapters_optimised/0007000_adapters.safetensors.\n",
      "Run (100.0% total) | Local 7000/7000 | Global 7000/7000 | LR 2.0e-04 | Train 0.262 | Val 0.255 | ETA 00:00:00Saved final weights to ACLED_llama_fine_tuned/adapters_optimised/adapters.safetensors.\n",
      "Run (100.0% total) | Local 7000/7000 | Global 7000/7000 | LR 2.0e-04 | Train 0.262 | Val 0.255 | ETA 00:00:00\n",
      "Stage 1 completed in 03:59:00\n",
      "Latest detected checkpoint step: 7000\n",
      "\n",
      "============================================================\n",
      "ALL STAGES ATTEMPTED\n",
      "Total training time: 03:59:00\n",
      "Final observed train loss: 0.262\n",
      "Final observed val loss: 0.255\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def run_staged_training():\n",
    "\n",
    "    def fmt_eta(td: timedelta) -> str:\n",
    "        s = int(max(td.total_seconds(), 0))\n",
    "        return f\"{s//3600:02d}:{(s%3600)//60:02d}:{s%60:02d}\"\n",
    "\n",
    "    def detect_last_step(adapters_path: Path) -> int:\n",
    "        if not adapters_path.exists():\n",
    "            return 0\n",
    "        pat = re.compile(r\"(\\d+)_adapters\\.safetensors$\")\n",
    "        last = 0\n",
    "        for p in adapters_path.glob(\"*_adapters.safetensors\"):\n",
    "            m = pat.search(p.name)\n",
    "            if m:\n",
    "                with contextlib.suppress(ValueError):\n",
    "                    last = max(last, int(m.group(1)))\n",
    "        return last\n",
    "\n",
    "    def ensure_adapters_for_resume(adapters_path: Path, last: int) -> None:\n",
    "        # Keep adapters.safetensors in sync with latest step-indexed file\n",
    "        if last <= 0:\n",
    "            return\n",
    "        latest_ckpt = adapters_path / f\"{last:07d}_adapters.safetensors\"\n",
    "        promoted = adapters_path / \"adapters.safetensors\"\n",
    "        try:\n",
    "            if (not promoted.exists()) or (promoted.stat().st_size != latest_ckpt.stat().st_size):\n",
    "                tmp = promoted.with_suffix(\".tmp\")\n",
    "                with open(latest_ckpt, \"rb\") as src, open(tmp, \"wb\") as dst:\n",
    "                    while True:\n",
    "                        b = src.read(1024 * 1024)\n",
    "                        if not b:\n",
    "                            break\n",
    "                        dst.write(b)\n",
    "                os.replace(tmp, promoted)\n",
    "                print(f\"Promoted {latest_ckpt.name} -> {promoted.name}\")\n",
    "            else:\n",
    "                print(f\"Resume file present: {promoted.name}\")\n",
    "        except Exception as e:\n",
    "            with contextlib.suppress(Exception):\n",
    "                t = promoted.with_suffix(\".tmp\")\n",
    "                if t.exists():\n",
    "                    t.unlink()\n",
    "            print(f\"WARNING: resume prep failed: {e}\")\n",
    "\n",
    "    # Log parsers\n",
    "    step_pattern       = re.compile(r'Iter\\s+(\\d+):')\n",
    "    train_loss_pattern = re.compile(r'Train loss\\s+([+\\-]?\\d+(?:\\.\\d+)?(?:[eE][+\\-]?\\d+)?)')\n",
    "    val_loss_pattern   = re.compile(r'Val loss\\s+([+\\-]?\\d+(?:\\.\\d+)?(?:[eE][+\\-]?\\d+)?)')\n",
    "\n",
    "    print(\"Starting training (constant LR)\")\n",
    "    print(f\"  target iters = {int(config.ITERATIONS)} | lr = {float(config.LEARNING_RATE)} | batch = {int(config.BATCH_SIZE)}\")\n",
    "\n",
    "    overall_start_time = datetime.now()\n",
    "    last_train_loss = None\n",
    "    last_val_loss = None\n",
    "\n",
    "    # Resume point\n",
    "    last_done = detect_last_step(Path(adapters_dir))\n",
    "    if last_done > 0:\n",
    "        print(f\"Detected checkpoints up to {last_done}. Resuming…\")\n",
    "        ensure_adapters_for_resume(Path(adapters_dir), last_done)\n",
    "    else:\n",
    "        print(\"No checkpoints found. Starting at 0.\")\n",
    "\n",
    "    start_for_stage = max(last_done, 0)\n",
    "    iters_to_run = int(config.ITERATIONS) - start_for_stage\n",
    "    if iters_to_run <= 0:\n",
    "        print(\"Nothing to do (already at or beyond target iterations).\")\n",
    "        return {\n",
    "            \"final_step\": last_done,\n",
    "            \"final_train_loss\": last_train_loss,\n",
    "            \"final_val_loss\": last_val_loss,\n",
    "            \"total_duration\": datetime.now() - overall_start_time,\n",
    "        }\n",
    "\n",
    "    # Commands\n",
    "    cmd = [\n",
    "        \"python\", \"-m\", \"mlx_lm\", \"lora\",\n",
    "        \"--model\", str(config.MODEL_PATH),\n",
    "        \"--train\",\n",
    "        \"--data\", str(data_dir),\n",
    "        \"--batch-size\", str(config.BATCH_SIZE),\n",
    "        \"--iters\", str(iters_to_run),\n",
    "        \"--learning-rate\", str(float(config.LEARNING_RATE)),\n",
    "        \"--steps-per-report\", \"100\",\n",
    "        \"--steps-per-eval\", str(int(config.EVAL_FREQUENCY)),\n",
    "        \"--save-every\", str(int(config.EVAL_FREQUENCY)),\n",
    "        \"--adapter-path\", str(adapters_dir),\n",
    "        \"--max-seq-length\", str(int(config.MAX_SEQ_LEN)),\n",
    "        \"-c\", str(lora_yaml_path),\n",
    "        \"--grad-checkpoint\",\n",
    "        \"--seed\", str(int(config.SEED)),\n",
    "    ]\n",
    "    print(\"Command:\", \" \".join(cmd))\n",
    "\n",
    "    # ETA state: estimate from deltas between Iter lines (skips cold-start cost)\n",
    "    stage_start_time = datetime.now()\n",
    "    print(f\"Progress: 0.0% | Local: 0/{iters_to_run} | Global: {start_for_stage}→{int(config.ITERATIONS)} | ETA: warming up…\")\n",
    "\n",
    "    process = subprocess.Popen(\n",
    "        cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
    "        text=True, bufsize=1, universal_newlines=True\n",
    "    )\n",
    "\n",
    "    local_step = 0\n",
    "    last_iter_seen = None\n",
    "    last_time_seen = None\n",
    "    sec_per_iter = None  # EWMA\n",
    "\n",
    "    try:\n",
    "        for raw_line in process.stdout:\n",
    "            line = raw_line.rstrip()\n",
    "            if not line:\n",
    "                continue\n",
    "            print(line)\n",
    "\n",
    "            now = datetime.now()\n",
    "\n",
    "            m = step_pattern.search(line)\n",
    "            if m:\n",
    "                with contextlib.suppress(ValueError):\n",
    "                    step_now = int(m.group(1))\n",
    "                    # Update timing only when iter increases\n",
    "                    if last_iter_seen is not None and step_now > last_iter_seen:\n",
    "                        d_iter = step_now - last_iter_seen\n",
    "                        d_time = (now - last_time_seen).total_seconds()\n",
    "                        if d_iter > 0 and d_time > 0:\n",
    "                            inst_spi = d_time / d_iter\n",
    "                            sec_per_iter = inst_spi if sec_per_iter is None else (0.7 * sec_per_iter + 0.3 * inst_spi)\n",
    "                    last_iter_seen = step_now\n",
    "                    last_time_seen = now\n",
    "                    local_step = step_now\n",
    "\n",
    "            m = train_loss_pattern.search(line)\n",
    "            if m:\n",
    "                with contextlib.suppress(ValueError):\n",
    "                    last_train_loss = float(m.group(1))\n",
    "\n",
    "            m = val_loss_pattern.search(line)\n",
    "            if m:\n",
    "                with contextlib.suppress(ValueError):\n",
    "                    last_val_loss = float(m.group(1))\n",
    "\n",
    "            # Progress line with ETA once we have a decent estimate\n",
    "            if local_step > 0:\n",
    "                remaining = max(iters_to_run - local_step, 0)\n",
    "                if sec_per_iter is not None and local_step >= 10:\n",
    "                    eta_td = timedelta(seconds=int(remaining * sec_per_iter))\n",
    "                    eta_txt = fmt_eta(eta_td)\n",
    "                else:\n",
    "                    eta_txt = \"estimating…\"\n",
    "                global_step_est = start_for_stage + local_step\n",
    "                pct_total = 100.0 * (global_step_est / float(config.ITERATIONS))\n",
    "                parts = [\n",
    "                    f\"\\rRun ({pct_total:.1f}% total)\",\n",
    "                    f\"Local {local_step}/{iters_to_run}\",\n",
    "                    f\"Global {global_step_est}/{int(config.ITERATIONS)}\",\n",
    "                    f\"LR {float(config.LEARNING_RATE):.1e}\",\n",
    "                ]\n",
    "                if last_train_loss is not None:\n",
    "                    parts.append(f\"Train {last_train_loss:.3f}\")\n",
    "                if last_val_loss is not None:\n",
    "                    parts.append(f\"Val {last_val_loss:.3f}\")\n",
    "                parts.append(f\"ETA {eta_txt}\")\n",
    "                print(\" | \".join(parts), end=\"\", flush=True)\n",
    "\n",
    "        process.wait()\n",
    "    except KeyboardInterrupt:\n",
    "        process.terminate()\n",
    "        print(\"\\nInterrupted by user.\")\n",
    "\n",
    "    duration = datetime.now() - stage_start_time\n",
    "    print(f\"\\nStage 1 completed in {fmt_eta(duration)}\")\n",
    "    if process.returncode != 0:\n",
    "        print(f\"Process failed with return code: {process.returncode}\")\n",
    "\n",
    "    # Refresh last_done after run\n",
    "    last_done = detect_last_step(Path(adapters_dir))\n",
    "    print(f\"Latest detected checkpoint step: {last_done}\")\n",
    "\n",
    "    total_duration = datetime.now() - overall_start_time\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ALL STAGES ATTEMPTED\")\n",
    "    print(f\"Total training time: {fmt_eta(total_duration)}\")\n",
    "    print(f\"Final observed train loss: {last_train_loss:.3f}\" if last_train_loss is not None else \"No final train loss observed\")\n",
    "    print(f\"Final observed val loss: {last_val_loss:.3f}\" if last_val_loss is not None else \"No final val loss observed\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    return {\n",
    "        \"final_step\": last_done,\n",
    "        \"final_train_loss\": last_train_loss,\n",
    "        \"final_val_loss\": last_val_loss,\n",
    "        \"total_duration\": total_duration,\n",
    "    }\n",
    "\n",
    "result = run_staged_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Checkpoint Promotion and Training Metadata Export\n",
    "\n",
    "This cell finalises the training session by promoting the latest adapter checkpoint to a consistent filename and generating a `training_info.json` manifest for reproducibility.\n",
    "\n",
    "#### Checkpoint Promotion\n",
    "\n",
    "- Searches for adapter files matching the pattern `*_adapters.safetensors` within the output directory.\n",
    "- Chooses the latest checkpoint based on:\n",
    "  - Step number, if present in filename (preferred), or\n",
    "  - File modification time (fallback)\n",
    "- Promotes the selected file to `adapters.safetensors` using an atomic `os.replace` operation to prevent corruption.\n",
    "\n",
    "If no checkpoint is found, a warning is printed and promotion is skipped.\n",
    "\n",
    "#### Training Manifest (`training_info.json`)\n",
    "\n",
    "A final metadata summary is compiled using all available runtime information, including:\n",
    "\n",
    "- **Model and adapter paths**\n",
    "- **Training and validation set sizes**\n",
    "- **Training duration**\n",
    "- **Final observed training and validation losses**\n",
    "- **Start and end timestamps**\n",
    "- **LoRA and training hyperparameters** (e.g., batch size, learning rate, seed)\n",
    "\n",
    "This metadata allows future reproducibility and analysis without needing to reparse logs or rerun training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promoted 0007000_adapters.safetensors -> adapters.safetensors\n",
      "Metadata saved: ACLED_llama_fine_tuned/adapters_optimised/training_info.json\n"
     ]
    }
   ],
   "source": [
    "adapters_dir = Path(adapters_dir)\n",
    "\n",
    "# Pick latest checkpoint (prefer highest step; fallback to newest mtime)\n",
    "ckpts = list(adapters_dir.glob(\"*_adapters.safetensors\"))\n",
    "step_re = re.compile(r\"^(\\d+)_adapters\\.safetensors$\")\n",
    "def step_num(p: Path) -> int:\n",
    "    m = step_re.match(p.name)\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "latest_ckpt = None\n",
    "if ckpts:\n",
    "    by_step = [p for p in ckpts if step_re.match(p.name)]\n",
    "    latest_ckpt = max(by_step, key=step_num) if by_step else max(ckpts, key=lambda p: p.stat().st_mtime)\n",
    "\n",
    "# Promote to adapters.safetensors (atomic replace)\n",
    "if latest_ckpt:\n",
    "    dst = adapters_dir / \"adapters.safetensors\"\n",
    "    tmp = dst.with_suffix(\".tmp\")\n",
    "    try:\n",
    "        with open(latest_ckpt, \"rb\") as src, open(tmp, \"wb\") as out:\n",
    "            while True:\n",
    "                b = src.read(1024 * 1024)  # 1 MiB chunks\n",
    "                if not b:\n",
    "                    break\n",
    "                out.write(b)\n",
    "        os.replace(tmp, dst)\n",
    "        print(f\"Promoted {latest_ckpt.name} -> {dst.name}\")\n",
    "    except Exception as e:\n",
    "        with contextlib.suppress(Exception):\n",
    "            if tmp.exists():\n",
    "                tmp.unlink()\n",
    "        print(f\"Promotion failed: {e}\")\n",
    "else:\n",
    "    print(\"No checkpoints found to promote.\")\n",
    "\n",
    "#nBuild a concise manifest (best-effort pulls from `result`/`config` if available)\n",
    "end_time   = datetime.now()\n",
    "_has_result = ('result' in globals()) and isinstance(result, dict) and bool(result)\n",
    "duration   = result.get('total_duration') if _has_result else timedelta(0)\n",
    "final_step = int(result.get('final_step', 0)) if _has_result else 0\n",
    "train_loss = result.get('final_train_loss') if _has_result else None\n",
    "val_loss   = result.get('final_val_loss')   if _has_result else None\n",
    "\n",
    "try:\n",
    "    training_samples   = len(train_data)\n",
    "except Exception:\n",
    "    training_samples   = None\n",
    "try:\n",
    "    validation_samples = len(valid_data)\n",
    "except Exception:\n",
    "    validation_samples = None\n",
    "\n",
    "# pull config if present\n",
    "try:\n",
    "    model_path = str(config.MODEL_PATH)\n",
    "    batch_size = int(getattr(config, \"BATCH_SIZE\", 0))\n",
    "    max_len    = int(getattr(config, \"MAX_SEQ_LEN\", 0))\n",
    "    seed_val   = int(getattr(config, \"SEED\", 0))\n",
    "    lr_val     = float(getattr(config, \"LEARNING_RATE\", 0.0))\n",
    "    iterations = int(getattr(config, \"ITERATIONS\", 0))\n",
    "except Exception:\n",
    "    model_path = batch_size = max_len = seed_val = iterations = None\n",
    "    lr_val = None\n",
    "\n",
    "try:\n",
    "    started_at = overall_start_time.isoformat(timespec=\"seconds\")\n",
    "except Exception:\n",
    "    started_at = end_time.isoformat(timespec=\"seconds\")\n",
    "\n",
    "run_manifest = {\n",
    "    \"version\": \"training_v3\",\n",
    "    \"adapter_dir\": str(adapters_dir),\n",
    "    \"latest_checkpoint\": latest_ckpt.name if latest_ckpt else None,\n",
    "    \"model_path\": model_path,\n",
    "    \"training_samples\": training_samples,\n",
    "    \"validation_samples\": validation_samples,\n",
    "    \"duration\": str(duration),\n",
    "    \"final_step\": final_step,\n",
    "    \"train_loss\": train_loss,\n",
    "    \"val_loss\": val_loss,\n",
    "    \"started_at\": started_at,\n",
    "    \"ended_at\": end_time.isoformat(timespec=\"seconds\"),\n",
    "    \"hyperparameters\": {\n",
    "        \"schedule\": f\"{lr_val} (constant)\" if lr_val else \"constant\",\n",
    "        \"learning_rate\": lr_val,\n",
    "        \"iterations\": iterations,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"max_seq_length\": max_len,\n",
    "        \"lora_r\": 16,\n",
    "        \"lora_alpha\": 32,\n",
    "        \"lora_dropout\": 0.1,\n",
    "        \"grad_checkpoint\": True,\n",
    "        \"seed\": seed_val,\n",
    "    },\n",
    "}\n",
    "\n",
    "manifest_path = adapters_dir / \"training_info.json\"\n",
    "with open(manifest_path, \"w\") as f:\n",
    "    json.dump(run_manifest, f, indent=2)\n",
    "\n",
    "print(f\"Metadata saved: {manifest_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check: Inference with Fine-Tuned Model\n",
    "\n",
    "Before running the full evaluation, a quick sanity check is performed to confirm that the fine-tuned model loads correctly and produces a structured response. The model and adapter are loaded using the `load()` function, and a single prompt describing a fictional conflict event is passed to the model via `generate_safe()`.\n",
    "\n",
    "The prompt describes an airstrike on a hospital in Kharkiv. The model's output is printed to allow manual inspection of whether it extracts key fields (e.g., `event_date`, `location`, `actor_1`, `fatalities`) in valid JSON format. This step helps detect any adapter issues or formatting errors before running the full test suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned model (with promoted adapters) for a quick check\n",
      "\n",
      "Model output:\n",
      "\n",
      "{\"event_date\": \"15 March 2025\", \"country\": \"ukraine\", \"location\": [\"Kharkiv\"], \"event_type\": \"remote_violence\", \"actor_1\": \"Russian military forces\", \"actor_2\": \"unknown\", \"fatalities\": \"unknown\", \"civilian_casualties\": 3, \"property_damage\": \"yes - hospitals\", \"weapons_mentioned\": \"unknown\", \"casualty_type\": \"injured\", \"attack_method\": \"airstrike\", \"disorder_type\": \"interstate_conflict\", \"infrastructure_disruption\": [\"medical_services\"]}\n"
     ]
    }
   ],
   "source": [
    "def generate_safe(model, tok, prompt, max_tokens=512, temperature=0.05):\n",
    "    sig = inspect.signature(generate)\n",
    "    kwargs = {\"model\": model, \"tokenizer\": tok, \"prompt\": prompt}\n",
    "    if \"max_tokens\" in sig.parameters: kwargs[\"max_tokens\"] = max_tokens\n",
    "    if \"max_new_tokens\" in sig.parameters: kwargs[\"max_new_tokens\"] = max_tokens\n",
    "    if \"temperature\" in sig.parameters: kwargs[\"temperature\"] = temperature\n",
    "    return generate(**kwargs)\n",
    "\n",
    "try:\n",
    "    print(\"Loading fine-tuned model (with promoted adapters) for a quick check\")\n",
    "    model_v, tok_v = load(config.MODEL_PATH, adapter_path=str(adapters_dir))\n",
    "\n",
    "    prompt = (\n",
    "    \"[INST] Extract relevant information from this conflict event report:\\n\\n\"\n",
    "    \"On 15 March 2025, Russian forces conducted an airstrike on a hospital in Kharkiv, Ukraine. \"\n",
    "    \"3 civilians were wounded. The medical building was destroyed. [/INST]\"\n",
    ")\n",
    "    out = generate_safe(model_v, tok_v, prompt=prompt, max_tokens=512, temperature=0.05)\n",
    "    print(\"\\nModel output:\\n\")\n",
    "    print(out.strip())\n",
    "except Exception as e:\n",
    "    print(f\"Sanity check failed: {e}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation for structured field extraction\n",
    " \n",
    "This cell serves as a compact audit of a field extraction model on ACLED event notes. It prompts the model for a JSON response, validates and scores the output against a fixed set of fields, prints a clear report, returns a structured results dictionary, and saves the outcome for later review.\n",
    "\n",
    "**Data**   \n",
    "* Sample up to `num_samples` rows with a fixed seed.  \n",
    "* Use the `notes` column as input to `generate_safe(model, tokenizer, ...)`.\n",
    "\n",
    "**Target fields**  \n",
    "`event_date`, `country`, `location`, `event_type`, `actor_1`, `actor_2`, `fatalities`, `civilian_casualties`, `casualty_type`, `weapons_mentioned`, `attack_method`, `property_damage`, `disorder_type`, `infrastructure_disruption`.\n",
    "\n",
    "**Validity and scoring**  \n",
    "* Locate a JSON object in the model output and parse it.  \n",
    "* Count a field as correct only when the value is meaningful.  \n",
    "  * Strings must be non empty and not placeholders such as unknown or n slash a.  \n",
    "  * Lists must be non empty with at least one meaningful element.  \n",
    "  * Numbers must be positive, except that zero is accepted for fatalities and civilian casualties.\n",
    "\n",
    "**Report contents**  \n",
    "* Rate of valid JSON responses and average field completion.  \n",
    "* Field and category breakdowns with status labels PASS, CAUTION, FAIL.  \n",
    "* One success example and one error example when available.  \n",
    "* A multi criteria assessment that produces an overall verdict.\n",
    "\n",
    "**Criteria**  \n",
    "* Valid JSON at least eighty percent.  \n",
    "* Average field completion at least sixty percent.  \n",
    "* Core Info category at least 70%.  \n",
    "* Error rate at most thirty percent.  \n",
    "The verdict is PASS when at least three criteria are met, PARTIAL when exactly two are met, otherwise FAIL.\n",
    "\n",
    "**Returned data**  \n",
    "A dictionary containing the verdict, valid_json_rate, avg_field_success, category scores, per field scores, counts of criteria met, and up to three success and three error examples.\n",
    "\n",
    "**Saved output**  \n",
    "The script adds a timestamp and model metadata, then writes `evaluation_results.json` to the adapter directory for reliable record keeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model Evaluation\n",
      "Loaded 400 test samples\n",
      "\n",
      "Testing model on 400 samples\n",
      "\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "Valid JSON responses: 400/400 (100.0%)\n",
      "Average field completion: 62.0%\n",
      "\n",
      "Field Extraction Success Rates:\n",
      "Field                     Success    Rate     Status\n",
      "-------------------------------------------------------\n",
      "\n",
      "Core Info:\n",
      "  event_date              400/400    100.0%   PASS\n",
      "  country                 400/400    100.0%   PASS\n",
      "  location                387/400     96.8%   PASS\n",
      "  event_type              400/400    100.0%   PASS\n",
      "\n",
      "Actors:\n",
      "  actor_1                 388/400     97.0%   PASS\n",
      "  actor_2                  57/400     14.2%   FAIL\n",
      "\n",
      "Casualties:\n",
      "  fatalities               55/400     13.8%   FAIL\n",
      "  civilian_casualties       6/400      1.5%   FAIL\n",
      "  casualty_type            84/400     21.0%   FAIL\n",
      "\n",
      "Weapons/Methods:\n",
      "  weapons_mentioned       184/400     46.0%   CAUTION\n",
      "  attack_method           310/400     77.5%   PASS\n",
      "\n",
      "Impact:\n",
      "  property_damage         400/400    100.0%   PASS\n",
      "  disorder_type           378/400     94.5%   PASS\n",
      "  infrastructure_disruption  22/400      5.5%   FAIL\n",
      "\n",
      "Category Performance:\n",
      "  Core Info             99.2% PASS\n",
      "  Actors                55.6% CAUTION\n",
      "  Casualties            12.1% FAIL\n",
      "  Weapons/Methods       61.8% CAUTION\n",
      "  Impact                66.7% CAUTION\n",
      "\n",
      "Success Example (found 11/14 fields):\n",
      "   Text: On 10 November 2024, Russian forces shelled Ukrainian positions near Kopanky, Kharkiv. According to ...\n",
      "   Fields: ['event_date', 'country', 'location', 'event_type', 'actor_1', 'actor_2', 'fatalities', 'casualty_type', 'attack_method', 'property_damage', 'disorder_type']\n",
      "\n",
      "Error Example:\n",
      "   Text: On 15 December 2024, Russian forces clashed with Ukrainian forces near Verkhnokamianske, Donetsk. Ca...\n",
      "   Found: 9/14 fields\n",
      "\n",
      "OVERALL ASSESSMENT:\n",
      "  JSON Format: PASS (≥80% valid responses)\n",
      "  Field Extraction: PASS (≥60% average)\n",
      "  Core Information: PASS (≥70% for basic fields)\n",
      "  Error Rate: FAIL (82.5% > 30%)\n",
      "\n",
      "Criteria met: 3/4\n",
      " MODEL EVALUATION: PASS - Ready for deployment!\n",
      "\n",
      "Results saved to: ACLED_llama_fine_tuned/adapters_optimised/evaluation_results.json\n"
     ]
    }
   ],
   "source": [
    "def model_evaluation(model, tokenizer, num_samples=400):\n",
    "    \"\"\"Evaluation to test field extraction with detailed analysis\"\"\"\n",
    "    \n",
    "    try:\n",
    "        test_df = pd.read_csv(\"ACLED_data_export/ACLED_test_dataset.csv\")\n",
    "        print(f\"Loaded {len(test_df)} test samples\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Test dataset not found at ACLED_data_export/ACLED_test_dataset.csv\")\n",
    "        return None\n",
    "    \n",
    "    test_sample = test_df.sample(n=min(num_samples, len(test_df)), random_state=42)\n",
    "    \n",
    "    # Target fields: the model is expected to extract\n",
    "    target_fields = [\n",
    "        'event_date', 'country', 'location', 'event_type', 'actor_1', 'actor_2',\n",
    "        'fatalities', 'civilian_casualties', 'casualty_type', 'weapons_mentioned',\n",
    "        'attack_method', 'property_damage', 'disorder_type', 'infrastructure_disruption'\n",
    "    ]\n",
    "    \n",
    "    # Track detailed results\n",
    "    field_success = defaultdict(int)\n",
    "    field_total = defaultdict(int)\n",
    "    valid_json_count = 0\n",
    "    total_tests = len(test_sample)\n",
    "    error_examples = []\n",
    "    success_examples = []\n",
    "    \n",
    "    print(f\"\\nTesting model on {total_tests} samples\")\n",
    "    \n",
    "    for idx, row in test_sample.iterrows():\n",
    "        text = row['notes']\n",
    "        country = row.get('country', 'unknown')\n",
    "        \n",
    "        # Utilise the same prompt\n",
    "        prompt = f\"[INST] Extract relevant information from this conflict event report:\\n\\n{text} [/INST]\"\n",
    "        \n",
    "        try:\n",
    "            response = generate_safe(model, tokenizer, prompt, max_tokens=512, temperature=0.05)\n",
    "            \n",
    "            # Try to parse JSON\n",
    "            json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "            if json_match:\n",
    "                try:\n",
    "                    extracted = json.loads(json_match.group())\n",
    "                    valid_json_count += 1\n",
    "                    \n",
    "                    # Track field completeness\n",
    "                    fields_found = 0\n",
    "                    field_details = {}\n",
    "                    \n",
    "                    for field in target_fields:\n",
    "                        field_total[field] += 1\n",
    "                        \n",
    "                        if field in extracted:\n",
    "                            value = extracted[field]\n",
    "                            # More nuanced success criteria\n",
    "                            if is_meaningful_value(value, field):\n",
    "                                field_success[field] += 1\n",
    "                                fields_found += 1\n",
    "                                field_details[field] = value\n",
    "                    \n",
    "                    # Store example\n",
    "                    example = {\n",
    "                        'text_preview': text[:100] + \"...\",\n",
    "                        'country': country,\n",
    "                        'fields_found': fields_found,\n",
    "                        'total_fields': len(target_fields),\n",
    "                        'completion_rate': fields_found / len(target_fields),\n",
    "                        'extracted_fields': field_details\n",
    "                    }\n",
    "                    \n",
    "                    if fields_found >= len(target_fields) * 0.7:\n",
    "                        success_examples.append(example)\n",
    "                    else:\n",
    "                        error_examples.append(example)\n",
    "                        \n",
    "                except json.JSONDecodeError as e:\n",
    "                    error_examples.append({\n",
    "                        'text_preview': text[:100] + \"...\",\n",
    "                        'error': f\"JSON parse error: {str(e)[:50]}\",\n",
    "                        'response_preview': response[:100] + \"...\"\n",
    "                    })\n",
    "            else:\n",
    "                error_examples.append({\n",
    "                    'text_preview': text[:100] + \"...\",\n",
    "                    'error': \"No JSON found in response\",\n",
    "                    'response_preview': response[:100] + \"...\"\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_examples.append({\n",
    "                'text_preview': text[:100] + \"...\",\n",
    "                'error': f\"Generation error: {str(e)}\"\n",
    "            })\n",
    "    \n",
    "    # The official results\n",
    "    json_success_rate = (valid_json_count / total_tests) * 100\n",
    "    avg_field_success = sum(field_success.values()) / (len(target_fields) * total_tests) * 100\n",
    "    \n",
    "    print(f\"\\nEVALUATION RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Valid JSON responses: {valid_json_count}/{total_tests} ({json_success_rate:.1f}%)\")\n",
    "    print(f\"Average field completion: {avg_field_success:.1f}%\")\n",
    "    \n",
    "    # Field-by-field breakdown with categories\n",
    "    print(f\"\\nField Extraction Success Rates:\")\n",
    "    print(f\"{'Field':<25} {'Success':<10} {'Rate':<8} {'Status'}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    # Group fields by category for better analysis\n",
    "    field_categories = {\n",
    "        'Core Info': ['event_date', 'country', 'location', 'event_type'],\n",
    "        'Actors': ['actor_1', 'actor_2'],\n",
    "        'Casualties': ['fatalities', 'civilian_casualties', 'casualty_type'],\n",
    "        'Weapons/Methods': ['weapons_mentioned', 'attack_method'],\n",
    "        'Impact': ['property_damage', 'disorder_type', 'infrastructure_disruption']\n",
    "    }\n",
    "    \n",
    "    category_scores = {}\n",
    "    \n",
    "    for category, fields in field_categories.items():\n",
    "        category_success = sum(field_success[field] for field in fields)\n",
    "        category_total = sum(field_total[field] for field in fields)\n",
    "        category_rate = (category_success / category_total * 100) if category_total > 0 else 0\n",
    "        category_scores[category] = category_rate\n",
    "        \n",
    "        print(f\"\\n{category}:\")\n",
    "        for field in fields:\n",
    "            success_rate = (field_success[field] / field_total[field] * 100) if field_total[field] > 0 else 0\n",
    "            status = \"PASS\" if success_rate >= 70 else \"CAUTION\" if success_rate >= 40 else \"FAIL\"\n",
    "            print(f\"  {field:<23} {field_success[field]:>3}/{field_total[field]:<3}    {success_rate:>5.1f}%   {status}\")\n",
    "    \n",
    "    print(f\"\\nCategory Performance:\")\n",
    "    for category, score in category_scores.items():\n",
    "        status = \"PASS\" if score >= 70 else \"CAUTION\" if score >= 40 else \"FAIL\"\n",
    "        print(f\"  {category:<20} {score:>5.1f}% {status}\")\n",
    "    \n",
    "    if success_examples:\n",
    "        print(f\"\\nSuccess Example (found {success_examples[0]['fields_found']}/{success_examples[0]['total_fields']} fields):\")\n",
    "        print(f\"   Text: {success_examples[0]['text_preview']}\")\n",
    "        print(f\"   Fields: {list(success_examples[0]['extracted_fields'].keys())}\")\n",
    "    \n",
    "    if error_examples:\n",
    "        print(f\"\\nError Example:\")\n",
    "        print(f\"   Text: {error_examples[0]['text_preview']}\")\n",
    "        if 'error' in error_examples[0]:\n",
    "            print(f\"   Issue: {error_examples[0]['error']}\")\n",
    "        if 'fields_found' in error_examples[0]:\n",
    "            print(f\"   Found: {error_examples[0]['fields_found']}/{error_examples[0]['total_fields']} fields\")\n",
    "    \n",
    "    print(f\"\\nOVERALL ASSESSMENT:\")\n",
    "    \n",
    "    # Multi-criteria evaluation\n",
    "    criteria_met = 0\n",
    "    total_criteria = 4\n",
    "    \n",
    "    if json_success_rate >= 80:\n",
    "        print(\"  JSON Format: PASS (≥80% valid responses)\")\n",
    "        criteria_met += 1\n",
    "    else:\n",
    "        print(f\"  JSON Format: FAIL ({json_success_rate:.1f}% < 80%)\")\n",
    "    \n",
    "    if avg_field_success >= 60:\n",
    "        print(\"  Field Extraction: PASS (≥60% average)\")\n",
    "        criteria_met += 1\n",
    "    else:\n",
    "        print(f\"  Field Extraction: FAIL ({avg_field_success:.1f}% < 60%)\")\n",
    "    \n",
    "    core_performance = category_scores.get('Core Info', 0)\n",
    "    if core_performance >= 70:\n",
    "        print(\"  Core Information: PASS (≥70% for basic fields)\")\n",
    "        criteria_met += 1\n",
    "    else:\n",
    "        print(f\"  Core Information: FAIL ({core_performance:.1f}% < 70%)\")\n",
    "    \n",
    "    if len(error_examples) <= total_tests * 0.3:\n",
    "        print(\"  Error Rate: PASS (≤30% failures)\")\n",
    "        criteria_met += 1\n",
    "    else:\n",
    "        print(f\"  Error Rate: FAIL ({len(error_examples)/total_tests*100:.1f}% > 30%)\")\n",
    "    \n",
    "    print(f\"\\nCriteria met: {criteria_met}/{total_criteria}\")\n",
    "    \n",
    "    if criteria_met >= 3:\n",
    "        print(\" MODEL EVALUATION: PASS - Ready for deployment!\")\n",
    "        verdict = \"PASS\"\n",
    "    elif criteria_met >= 2:\n",
    "        print(\" MODEL EVALUATION: PARTIAL - Functional but needs improvement\")\n",
    "        verdict = \"PARTIAL\"\n",
    "    else:\n",
    "        print(\" MODEL EVALUATION: FAIL - Requires significant improvement\")\n",
    "        verdict = \"FAIL\"\n",
    "    \n",
    "    return {\n",
    "        'verdict': verdict,\n",
    "        'valid_json_rate': json_success_rate / 100,\n",
    "        'avg_field_success': avg_field_success / 100,\n",
    "        'category_scores': {k: v/100 for k, v in category_scores.items()},\n",
    "        'field_success_rates': {field: field_success[field] / field_total[field] for field in target_fields if field_total[field] > 0},\n",
    "        'criteria_met': criteria_met,\n",
    "        'total_criteria': total_criteria,\n",
    "        'examples': {\n",
    "            'success': success_examples[:3],\n",
    "            'errors': error_examples[:3]\n",
    "        }\n",
    "    }\n",
    "\n",
    "def is_meaningful_value(value, field_name):\n",
    "    \"\"\"Check if a field value is meaningful (not empty/unknown)\"\"\"\n",
    "    if value is None:\n",
    "        return False\n",
    "    \n",
    "    # Handle different data types\n",
    "    if isinstance(value, str):\n",
    "        if value.lower().strip() in ['', 'unknown', 'n/a', 'none', 'null']:\n",
    "            return False\n",
    "        return len(value.strip()) > 0\n",
    "    \n",
    "    elif isinstance(value, list):\n",
    "        return len(value) > 0 and any(is_meaningful_value(item, field_name) for item in value)\n",
    "    \n",
    "    elif isinstance(value, (int, float)):\n",
    "        # For numeric fields, 0 might be meaningful\n",
    "        if field_name in ['fatalities', 'civilian_casualties']:\n",
    "            return True  # 0 casualties is meaningful information\n",
    "        return value > 0\n",
    "    \n",
    "    else:\n",
    "        return True  # Other types assumed meaningful if present\n",
    "\n",
    "print(\"Running Model Evaluation\")\n",
    "results = model_evaluation(model_v, tok_v, num_samples=400)\n",
    "\n",
    "if results:\n",
    "    \n",
    "    results['timestamp'] = datetime.now().isoformat()\n",
    "    results['model_info'] = {\n",
    "        'base_model': config.MODEL_PATH,\n",
    "        'adapter_path': str(adapters_dir),\n",
    "        'samples_tested': 400\n",
    "    }\n",
    "    \n",
    "    results_file = Path(adapters_dir) / \"evaluation_results.json\"\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"\\nResults saved to: {results_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
